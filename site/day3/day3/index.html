
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A template site, replace this with the actual description">
      
      
        <meta name="author" content="ACM BPDC">
      
      
        <link rel="canonical" href="https://riyanbhargava.github.io/acm-ml-workshop/day3/day3/">
      
      
        <link rel="prev" href="../../day2/day2/">
      
      
        <link rel="next" href="../../day4/day4/">
      
      
      <link rel="icon" href="../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Deep Learning & NLP - ML Workshop</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#day-3-deep-learning-nlp" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ML Workshop" class="md-header__button md-logo" aria-label="ML Workshop" data-md-component="logo">
      
  <img src="../../assets/ACM_Logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ML Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep Learning & NLP
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/RiyanBhargava/acm-ml-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    RiyanBhargava/acm-ml-workshop
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../day1/day1/" class="md-tabs__link">
        
  
  
    
  
  Data Cleaning and Feature Engineering

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../day2/day2/" class="md-tabs__link">
        
  
  
    
  
  Model Training and Analysis

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Deep Learning & NLP

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../day4/day4/" class="md-tabs__link">
        
  
  
    
  
  DL with CNN and Image Detection

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ML Workshop" class="md-nav__button md-logo" aria-label="ML Workshop" data-md-component="logo">
      
  <img src="../../assets/ACM_Logo.png" alt="logo">

    </a>
    ML Workshop
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RiyanBhargava/acm-ml-workshop" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    RiyanBhargava/acm-ml-workshop
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../day1/day1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Cleaning and Feature Engineering
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../day2/day2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Training and Analysis
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Deep Learning & NLP
    
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../day4/day4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DL with CNN and Image Detection
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="day-3-deep-learning-nlp">DAY 3: DEEP LEARNING &amp; NLP<a class="headerlink" href="#day-3-deep-learning-nlp" title="Permanent link">&para;</a></h1>
<hr />
<h1 id="1-overview"><strong>1. Overview</strong><a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h1>
<p>In this workshop you'll learn:</p>
<ul>
<li><strong>Deep Learning Basics:</strong> Covers the fundamentals, starting from a single <strong>neuron</strong>, building up to <strong>neural networks</strong>, and explaining the "learning" process of <strong>gradient descent</strong> and <strong>backpropagation</strong>.</li>
<li><strong>What is NLP? :</strong> Introduces Natural Language Processing and its evolution from old <strong>rules-based</strong> systems to modern <strong>Deep Learning</strong> models.</li>
<li><strong>Turning Words into Numbers:</strong> Explains the critical step of <strong>vectorization</strong>, contrasting older methods like <strong>Bag-of-Words (BoW)</strong>, <strong>TF-IDF</strong>, and <strong>One-Hot Encoding</strong> with modern <strong>Word Embeddings</strong> (Word2Vec, GloVe, fastText) that capture meaning.</li>
<li><strong>Understanding Sequence &amp; Memory:</strong> Describes why word order matters and how <strong>RNNs</strong> (Recurrent Neural Networks) and their powerful upgrade, <strong>LSTMs</strong> (Long Short-Term Memory), were created to process sequences.</li>
<li><strong>The Modern Revolution (Transformers):</strong> Details the breakthrough <strong>Attention Mechanism</strong> and the two dominant models it created: <strong>BERT</strong> (for understanding context) and <strong>GPT</strong> (for generating text).</li>
<li><strong>Challenges &amp; Applications:</strong> Briefly touches on why human language is so hard for AI (like sarcasm and bias) and where NLP is used in the real world (e.g., finance, healthcare).</li>
</ul>
<hr />
<h1 id="2-workshop-resources"><strong>2. Workshop Resources</strong><a class="headerlink" href="#2-workshop-resources" title="Permanent link">&para;</a></h1>
<h3 id="make-a-copy-and-run-the-cells"><strong>(Make a copy and run the cells) :</strong><a class="headerlink" href="#make-a-copy-and-run-the-cells" title="Permanent link">&para;</a></h3>
<h3 id="colab-notebook"><strong>üìì Colab Notebook:</strong><a class="headerlink" href="#colab-notebook" title="Permanent link">&para;</a></h3>
<p><a href="https://colab.research.google.com/drive/1RGcpQuLJz-I7EYQPfaEYDR01m6IqEMfG?usp=sharing">Open in Google Colab</a> </p>
<h3 id="dataset"><strong>üìä Dataset:</strong><a class="headerlink" href="#dataset" title="Permanent link">&para;</a></h3>
<p><a href="../../files/day3/harry_potter_corpus.txt">harry_potter_corpus.txt</a></p>
<hr />
<h1 id="3-introduction-to-deep-learning-how-computers-learn">3. Introduction to Deep Learning (How Computers "Learn")<a class="headerlink" href="#3-introduction-to-deep-learning-how-computers-learn" title="Permanent link">&para;</a></h1>
<p>Welcome! Before we teach a computer to read, we must first understand how a computer "learns" at all. The main idea is <strong>Deep Learning (DL)</strong>.</p>
<p>Imagine you want to teach a computer to recognize your handwriting. How would it do that? This is the core problem Deep Learning solves.</p>
<p>DL is a method inspired by the human brain. It's not that we're building a real brain, but we're borrowing the key idea: <strong>a network of simple, interconnected units called neurons.</strong></p>
<h2 id="31-why-neural-networks-matter-and-their-applications">3.1. Why Neural Networks Matter and Their Applications<a class="headerlink" href="#31-why-neural-networks-matter-and-their-applications" title="Permanent link">&para;</a></h2>
<p>Neural networks are central to modern AI because they <strong>learn useful internal representations directly from data</strong>, allowing them to capture complex, nonlinear structures that classical models miss. This core capability allows them to power a vast array of real-world AI systems across numerous domains.</p>
<p>Prominent applications include:</p>
<ul>
<li><strong>Computer Vision:</strong> Convolutional Neural Networks (CNNs) are used for image recognition, medical imaging analysis, and powering autonomous vehicles.</li>
<li><strong>Natural Language Processing:</strong> Transformers are the basis for machine translation, advanced chatbots, and text summarization.</li>
<li><strong>Speech Recognition:</strong> Recurrent Neural Networks (RNNs) and other deep nets are used for transcription services and voice assistants.</li>
<li><strong>Forecasting and Time Series:</strong> They are applied to demand prediction, financial modeling, and weather forecasting.</li>
<li><strong>Reinforcement Learning:</strong> Neural networks act as function approximators in game-playing agents, such as DeepMind's AlphaGo.</li>
<li><strong>Pattern Recognition:</strong> They are highly effective at identifying fraud, detecting anomalies, and classifying documents.</li>
</ul>
<h2 id="32-why-deep-learning-over-traditional-machine-learning"><strong>3.2. Why Deep Learning over Traditional Machine Learning?</strong><a class="headerlink" href="#32-why-deep-learning-over-traditional-machine-learning" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Automatic Feature Engineering:</strong> This is the biggest advantage. Traditional ML (like Support Vector Machines or Random Forests) relies on <em>manual feature engineering</em>. A data scientist must spend significant time selecting and creating features (e.g., "word count" or "average pixel brightness"). Deep Learning models learn the best features <em>automatically</em> from the raw data.</li>
<li><strong>Performance with Scale:</strong> Traditional ML models plateau in performance as you give them more data. Deep Learning models <em>continue to improve</em> as the volume of data increases.</li>
<li><strong>Handling Unstructured Data:</strong> DL excels at complex, unstructured data like text, images, and audio, where traditional ML struggles.</li>
</ol>
<p>While that framework is very powerful and versatile, it‚Äôs comes at the expense of <em>interpretability.</em> There‚Äôs often little, if any, intuitive explanation‚Äîbeyond a raw mathematical one‚Äîfor how the values of individual model parameters learned by a neural network reflect real-world characteristics of data. For that reason, deep learning models are often referred to as ‚Äúblack boxes,‚Äù especially when compared to traditional types of machine learning models.</p>
<h2 id="33-the-building-block-the-artificial-neuron">3.3. The Building Block: The Artificial Neuron<a class="headerlink" href="#33-the-building-block-the-artificial-neuron" title="Permanent link">&para;</a></h2>
<p>Think of a single <strong>neuron</strong> as a tiny, simple decision-maker. It gets some inputs and decides how strongly to "fire" an output.</p>
<p>Here's its job, step-by-step:</p>
<ol>
<li><strong>It Receives Inputs (X):</strong> These are just numbers. For an image, this could be the brightness value (0-255) of a few pixels.</li>
<li><strong>It Has Weights (W):</strong> Each input has a <strong>weight</strong>. This is the <em>most important concept</em>. A weight is just a number that represents <strong>importance</strong>. A high weight means "pay a lot of attention to this input!" A low weight means "this input doesn't matter much."</li>
<li><strong>It Has a Bias (b):</strong> A <strong>bias</strong> is an extra "nudge." It's a number that helps the neuron decide how easy or hard it is to fire. (e.g., "Don't fire unless you are <em>really</em> sure").</li>
<li><strong>It Calculates an Output (Y):</strong> The neuron multiplies each input by its weight, adds them all up, adds the bias, and then passes this total through an <strong>Activation Function</strong>. This function just squashes the number (e.g., to be between 0 and 1) to make it a clean, final output signal.</li>
</ol>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image.png" /></p>
<h2 id="34-building-a-deep-brain-the-neural-network">3.4. Building a "Deep" Brain: The Neural Network<a class="headerlink" href="#34-building-a-deep-brain-the-neural-network" title="Permanent link">&para;</a></h2>
<p>A "deep" network is just many layers of these neurons stacked together. This is where the magic happens!</p>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%201.png" /></p>
<p>For example: Recognizing a handwritten digit</p>
<ol>
<li><strong>Input Layer:</strong> This layer just "receives" the raw data (e.g., all 784 pixels of a handwritten digit). It doesn't make any decisions.</li>
<li><strong>Hidden Layers:</strong> This is the <em>real</em> "brain" of the network. The term "deep" comes from having <em>multiple</em> hidden layers. They perform <strong>automatic feature learning</strong>:<ul>
<li><strong>Layer 1</strong> might learn to find simple edges and lines.</li>
<li><strong>Layer 2</strong> might combine those edges to find loops and curves.</li>
<li><strong>Layer 3</strong> might combine those loops to recognize a full "8" or "9".</li>
</ul>
</li>
<li><strong>Output Layer:</strong> This layer gives the final answer (e.g., 10 neurons, one for each digit 0-9, where the "9" neuron fires the strongest).</li>
</ol>
<p><img alt="Screenshot 2025-11-15 at 9.16.03 PM.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/Screenshot_2025-11-15_at_9.16.03_PM.png" /></p>
<h2 id="35-how-does-it-learn-the-training-process">3.5. How Does it Learn? (The Training Process)<a class="headerlink" href="#35-how-does-it-learn-the-training-process" title="Permanent link">&para;</a></h2>
<p>The power of a neural network is its ability to find the optimal <strong>weights</strong> and <strong>biases</strong> that map inputs to correct outputs. It achieves this by iteratively "learning from its mistakes" through a process driven by <strong>Backpropagation</strong> and <strong>Gradient Descent</strong>.</p>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%202.png" /></p>
<p>This learning process is a four-step cycle:</p>
<p><strong>i. The Forward Pass (The Guess)</strong>
First, the network makes a guess. Inputs (like an image of a "7") are fed <em>forward</em> through the network's layers. At each layer, the data is multiplied by the current weights, a bias is added, and it passes through a nonlinear activation function. This produces the network's initial, likely random, prediction (e.g., it guesses "3").</p>
<p><strong>ii. The Loss Calculation (The Mistake)</strong>
Next, the network measures <em>how wrong</em> its guess was. A <strong>Loss Function</strong> (or Cost Function) compares the network's prediction to the true label. This calculation results in a single number, the "loss" or "mistake score," which quantifies the error. A high score means a bad guess; the goal is to get this score as low as possible.</p>
<p><strong>iii. The Backward Pass (Assigning Blame)</strong>
This is the core of the learning mechanism, enabled by <strong>Backpropagation</strong> (short for "backward propagation of error").</p>
<ul>
<li><strong>Calculates Contribution:</strong> Starting from the final loss score, the algorithm works <em>backward</em> through the network, layer by layer.</li>
<li><strong>Uses Calculus:</strong> Using the chain rule of calculus, it calculates the "gradient"‚Äîa derivative that precisely measures how much each individual weight and bias in the entire network contributed to the final error.</li>
<li><strong>Finds Direction:</strong> This gradient "blames" the parameters. It tells the network not only <em>who</em> was responsible for the mistake but also <em>which direction</em> to nudge each parameter to fix it.</li>
</ul>
<p><strong>iv. The Weight Update (The Correction)</strong></p>
<p>Finally, the network applies the correction using <strong>Gradient Descent</strong>.</p>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%203.png" /></p>
<p><strong>A. Gradient Descent ‚Äî What It Actually Does:</strong></p>
<ul>
<li>Think of all possible weight combinations as a giant, hilly landscape.</li>
<li>The <strong>height</strong> of the landscape at any point is the <strong>loss</strong>.</li>
<li>The goal: <strong>reach the lowest valley</strong> ‚Äî the point where loss is minimum.</li>
</ul>
<p><strong>B. Local Minima vs. Global Minimum:</strong></p>
<ul>
<li>A <strong>global minimum</strong> is the <em>absolute lowest</em> point in the entire landscape.</li>
<li>A <strong>local minimum</strong> is a <em>small valley</em> that is lower than its surroundings but not the lowest overall.</li>
<li>Gradient Descent follows the slope downward, so if the landscape is complex, it may get stuck in a <strong>local minimum</strong> instead of reaching the <strong>global minimum</strong>.</li>
</ul>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%204.png" /></p>
<p><strong>C. Normal GD vs. Stochastic GD:</strong></p>
<p><strong>Stochastic Gradient Descent (SGD)</strong> is a fast and noisy version of <strong>Gradient Descent</strong> used to train machine learning models‚Äîespecially neural networks, using <strong>ONE</strong> <strong>training example</strong> at a time, instead of the entire dataset.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th><strong>Normal / Batch Gradient Descent</strong></th>
<th><strong>Stochastic Gradient Descent (SGD)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data Used per Update</strong></td>
<td>Entire dataset</td>
<td>One data point (or small batch)</td>
</tr>
<tr>
<td><strong>Speed</strong></td>
<td>Very slow</td>
<td>Much faster</td>
</tr>
<tr>
<td><strong>Computational Cost</strong></td>
<td>Very high</td>
<td>Much cheaper</td>
</tr>
<tr>
<td><strong>Accuracy of Gradient</strong></td>
<td>Very accurate</td>
<td>Noisy updates</td>
</tr>
<tr>
<td><strong>Effect on Optimization</strong></td>
<td>Can get stuck in local minima</td>
<td>Noise helps escape local minima and find better solutions</td>
</tr>
</tbody>
</table>
<p><strong>Why SGD is used primarily?</strong></p>
<p>Because computing gradients for millions of samples every step is too expensive. SGD drastically reduces the number of computations and speeds up learning, making training modern neural networks feasible.</p>
<hr />
<p><strong>v. The Training Loop:</strong></p>
<ul>
<li>This entire four-step cycle is repeated many times, showing the network thousands of data examples. Each full pass through the training dataset is called an <strong>epoch</strong>.
With each epoch, the weights and biases are nudged closer to their optimal values, the "mistake score" descends into the "valley," and the network's predictions become incrementally more accurate.</li>
</ul>
<p>However, despite practitioners' effort to train high performing models, neural networks still face challenges similar to other machine learning models‚Äîmost significantly, overfitting. When a neural network becomes overly complex with too many parameters, the model will overfit to the training data and predict poorly. Overfitting is a common problem in all kinds of neural networks, and paying close attention to bias-variance tradeoff is paramount to creating high-performing neural network models.  </p>
<h2 id="36-types-of-neural-networks"><strong>3.6. Types of neural networks</strong><a class="headerlink" href="#36-types-of-neural-networks" title="Permanent link">&para;</a></h2>
<p>While multilayer perceptrons are the foundation, neural networks have evolved into specialized architectures suited for different domains:</p>
<ul>
<li><strong>Convolutional neural networks (CNNs or convnets)</strong>: Designed for grid-like data such as images. CNNs excel at image recognition, computer vision and facial recognition thanks to convolutional filters that detect spatial hierarchies of features.</li>
<li><strong>Recurrent neural networks (RNNs)</strong>: Incorporate feedback loops that allow information to persist across time steps. RNNs are well-suited for speech recognition, time series forecasting and sequential data.</li>
<li><strong>Transformers</strong>: A modern architecture that replaced RNNs for many sequence tasks. Transformers leverage attention mechanisms to capture dependencies in natural language processing (NLP) and power state-of-the-art models like GPT.</li>
</ul>
<p>These variations highlight the versatility of neural networks. Regardless of architecture, all rely on the same principles: artificial neurons, nonlinear activations and optimization algorithms.</p>
<h2 id="37-applying-the-machine-to-language"><strong>3.7. Applying the Machine to Language</strong><a class="headerlink" href="#37-applying-the-machine-to-language" title="Permanent link">&para;</a></h2>
<p>Now we apply our "learning machine" to the messy, complex problem of human language.</p>
<p>Understanding Natural Language Processing(NLP)At its core, all modern NLP follows a three-step process:</p>
<ol>
<li><strong>Step 1: Text to Numbers (Embedding):</strong> We must convert raw text ("The quick brown fox...") into a numerical format (vectors) that a machine can understand. This is the most critical step.</li>
<li><strong>Step 2: Process the Numbers (The Model):</strong> The numerical vectors are fed into a deep learning model (like an RNN or a Transformer). This "brain" processes the numbers to "understand" the patterns, context, and relationships.</li>
<li><strong>Step 3: Numbers to Output (The Task):</strong> The model's final numerical output is converted into a human-usable result. This could be:<ul>
<li>A single label (e.g., "Positive Sentiment").</li>
<li>A new sequence of text (e.g., a translation).</li>
<li>A specific word (e.g., an "autocomplete" suggestion).</li>
</ul>
</li>
</ol>
<p>Before deep learning, this process was much more manual.</p>
<hr />
<h1 id="4-the-evolution-of-nlp-three-main-approaches">4. The Evolution of NLP: Three Main Approaches<a class="headerlink" href="#4-the-evolution-of-nlp-three-main-approaches" title="Permanent link">&para;</a></h1>
<p>To understand language, NLP models have evolved over time. They started with strict, simple rules and grew into the powerful, flexible "learning" systems we have today.</p>
<p>You can think of this evolution in three main stages.</p>
<h2 id="41-before-we-begin-two-core-ideas">4.1. Before We Begin: Two Core Ideas<a class="headerlink" href="#41-before-we-begin-two-core-ideas" title="Permanent link">&para;</a></h2>
<p>All NLP, from the simplest to the most complex, relies on two basic ways of analyzing language:</p>
<ol>
<li><strong>Syntactical Analysis (Grammar):</strong> This is the "rules" part. It focuses on the <strong>structure and grammar</strong> of a sentence. It checks if the word order is correct according to the rules of the language.<ul>
<li><strong>Example:</strong> "The cat sat on the mat" is <strong>syntactically correct</strong>.</li>
<li><strong>Example:</strong> "Sat the on mat cat" is <strong>syntactically incorrect</strong>.</li>
</ul>
</li>
<li><strong>Semantical Analysis (Meaning):</strong> This is the "meaning" part. Once it knows the grammar is correct, this step tries to figure out the <strong>meaning and intent</strong> of the sentence.<ul>
<li><strong>Example:</strong> "The cat sat on the mat" and "The mat was sat on by the cat" have different <em>syntax</em> (structure) but the same <em>semantics</em> (meaning).</li>
</ul>
</li>
</ol>
<p>Now, let's look at how the models evolved.</p>
<h2 id="42-approach-a-rules-based-nlp-the-if-then-approach">4.2. Approach A: Rules-Based NLP (The "If-Then" Approach)<a class="headerlink" href="#42-approach-a-rules-based-nlp-the-if-then-approach" title="Permanent link">&para;</a></h2>
<p>This was the earliest approach to NLP. It's based on <strong>manually programmed, "if-then" rules</strong>.</p>
<ul>
<li><strong>How it Worked:</strong> A programmer had to sit down and write explicit rules for the computer to follow.<ul>
<li><code>IF</code> the user says "hello," <code>THEN</code> respond with "Hi, how can I help you?"</li>
<li><code>IF</code> the user says "What are your hours?" <code>THEN</code> respond with "We are open 9 AM to 5 PM."</li>
</ul>
</li>
<li><strong>The Problem:</strong> This approach is extremely <strong>limited and not scalable</strong>.<ul>
<li>It has no "learning" or AI capabilities.</li>
<li>It breaks easily. If a user asks, "When are you guys open?" instead of "What are your hours?", the system would fail because it doesn't have a specific rule for that exact phrase.</li>
</ul>
</li>
<li><strong>Example:</strong> Early automated phone menus (like Moviefone) that only understood specific commands.</li>
</ul>
<h2 id="43-approach-b-statistical-nlp-the-probability-approach">4.3. Approach B: Statistical NLP (The "Probability" Approach)<a class="headerlink" href="#43-approach-b-statistical-nlp-the-probability-approach" title="Permanent link">&para;</a></h2>
<p>This was the next big step, which introduced <strong>machine learning</strong>. Instead of relying on hard-coded rules, this approach "learns" from a large amount of text.</p>
<ul>
<li><strong>How it Worked:</strong> The model analyzes data and assigns a <strong>statistical likelihood (a probability)</strong> to different word combinations.<ul>
<li>For example, it learns that after the words "New York," the word "City" is <em>highly probable</em>, while the word "banana" is <em>very improbable</em>.</li>
</ul>
</li>
<li><strong>The Big Breakthrough: Vector Representation.</strong> This approach introduced the essential technique of mapping words to <strong>numbers (called "vectors")</strong>. This allowed, for the first time, computers to perform mathematical and statistical calculations on words.</li>
<li><strong>Examples:</strong> Older spellcheckers (which suggest the <em>most likely</em> correct word) and T9 texting on old phones (which predicted the <em>most likely</em> word you were typing).</li>
</ul>
<blockquote>
<p>A Quick Note on Training:
These models needed "labeled data"‚Äîdata that a human had already manually annotated (e.t., "This is a noun," "This is a verb"). This was slow and expensive.
A key breakthrough called Self-Supervised Learning (SSL) allowed models to learn from unlabeled raw text, which is much faster and cheaper and a key reason why modern Deep Learning is so powerful.
</p>
</blockquote>
<h2 id="44-approach-c-deep-learning-nlp-the-modern-approach">4.4. Approach C: Deep Learning NLP (The "Modern" Approach)<a class="headerlink" href="#44-approach-c-deep-learning-nlp-the-modern-approach" title="Permanent link">&para;</a></h2>
<p>This is the dominant, state-of-the-art approach used today. It's an evolution of the statistical method but uses powerful, multi-layered <strong>neural networks</strong> to learn from <em>massive</em> volumes of unstructured, raw data.</p>
<p>These models are incredibly accurate because they can understand complex context and nuance. Several types of deep learning models are important:</p>
<ul>
<li><strong>Sequence-to-Sequence (Seq2Seq) Models:</strong><ul>
<li><strong>What they do:</strong> They are designed to transform an input sequence (like a sentence) into a <em>different</em> output sequence.</li>
<li><strong>Best for:</strong> Machine Translation. (e.g., converting a German sentence into an English one).</li>
</ul>
</li>
<li><strong>Transformer Models:</strong><ul>
<li><strong>What they do:</strong> This is the <em>biggest breakthrough</em> in modern NLP. Transformers use a mechanism called <strong>"self-attention"</strong> to look at all the words in a sentence at once and calculate how <em>important</em> each word is to all the other words, no matter how far apart.</li>
<li><strong>Example:</strong> Google's <strong>BERT</strong> model, which powers its search engine, is a famous transformer.</li>
</ul>
</li>
<li><strong>Autoregressive Models:</strong><ul>
<li><strong>What they do:</strong> This is a type of transformer model that is expertly trained to do one thing: <strong>predict the next word in a sequence</strong>. By doing this over and over, it can generate entire paragraphs of human-like text.</li>
<li><strong>Examples:</strong> <strong>GPT</strong> (which powers ChatGPT), Llama, and Claude.</li>
</ul>
</li>
<li><strong>Foundation Models:</strong><ul>
<li><strong>What they do:</strong> These are <em>huge</em>, pre-trained "base" models (like <strong>IBM's Granite</strong> or OpenAI's GPT-4) that have a very broad, general understanding of language. They can then be quickly adapted for many specific tasks, from content generation to data extraction.</li>
</ul>
</li>
</ul>
<hr />
<h1 id="5-how-nlp-works-the-4-step-pipeline"><strong>5. How NLP Works: The 4-Step Pipeline</strong><a class="headerlink" href="#5-how-nlp-works-the-4-step-pipeline" title="Permanent link">&para;</a></h1>
<p>A computer can't just "read" a sentence. To get from raw human language to a useful insight, it follows a strict, step-by-step "assembly line."</p>
<h2 id="51-step-1-text-preprocessing-the-cleaning-step"><strong>5.1. Step 1: Text Preprocessing (The "Cleaning" Step)</strong><a class="headerlink" href="#51-step-1-text-preprocessing-the-cleaning-step" title="Permanent link">&para;</a></h2>
<p>First, we clean up the raw text and turn it into a standardized format. This is the "prep work" in a kitchen‚Äîgetting your ingredients (the words) ready before you start cooking (the analysis).</p>
<ul>
<li><strong>Tokenization:</strong> Splitting a long string of text into smaller pieces, or "tokens."<ul>
<li><em>Example:</em> "The cat sat" becomes <code>["The", "cat", "sat"]</code></li>
</ul>
</li>
<li><strong>Lowercasing:</strong> Converting all characters to lowercase.<ul>
<li><em>Example:</em> "Apple" and "apple" both become <code>"apple"</code>.</li>
</ul>
</li>
<li><strong>Stop Word Removal:</strong> Removing common "filler" words (like "is," "the," "a," "on") that add little unique meaning.</li>
<li><strong>Stemming &amp; Lemmatization:</strong> Reducing words to their "root" form (e.g., "running," "ran," and "runs" all become "run").</li>
<li><strong>Text Cleaning:</strong> Removing punctuation, special characters (@, #), numbers, etc.</li>
</ul>
<h2 id="52-step-2-feature-extraction-the-converting-step"><strong>5.2. Step 2: Feature Extraction (The "Converting" Step)</strong><a class="headerlink" href="#52-step-2-feature-extraction-the-converting-step" title="Permanent link">&para;</a></h2>
<p>This is a critical step. <strong>Computers do not understand words; they only understand numbers.</strong> Feature extraction converts the clean text tokens into a numerical representation (a "vector") that a machine can actually analyze.</p>
<h3 id="521-the-old-way-statistical-counts">5.2.1. The "Old Way" (Statistical Counts)<a class="headerlink" href="#521-the-old-way-statistical-counts" title="Permanent link">&para;</a></h3>
<p>Before we had powerful neural networks, we relied on <strong>statistics and word counts</strong>. These models were clever but lacked any <em>real</em> understanding.</p>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%205.png" /></p>
<h3 id="i-bag-of-words-bow"><strong>i. Bag-of-Words (BoW):</strong><a class="headerlink" href="#i-bag-of-words-bow" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>How it Works:</strong></p>
<p>Treats a sentence as a collection (‚Äúbag‚Äù) of words. Ignores grammar and word order.</p>
</li>
<li>
<p><strong>Example Sentence:</strong></p>
<p><strong>‚ÄúThe cat sat on the mat‚Äù</strong></p>
</li>
<li>
<p><strong>Vocabulary (example):</strong></p>
<p><code>["the", "cat", "sat", "on", "mat"]</code></p>
</li>
<li>
<p><strong>BoW Vector:</strong></p>
<p>Count how many times each word appears:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>the</th>
<th>cat</th>
<th>sat</th>
<th>on</th>
<th>mat</th>
</tr>
</thead>
<tbody>
<tr>
<td>Count</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><strong>BoW Vector:</strong> ‚Üí <strong>[2, 1, 1, 1, 1]</strong></p>
</li>
<li>
<p><strong>Limitation:</strong></p>
<p>‚ÄúThe cat chased the dog‚Äù and ‚ÄúThe dog chased the cat‚Äù would look almost identical.</p>
</li>
</ul>
<hr />
<h3 id="ii-tf-idf-term-frequency-inverse-document-frequency"><strong>ii. TF-IDF (Term Frequency-Inverse Document Frequency):</strong><a class="headerlink" href="#ii-tf-idf-term-frequency-inverse-document-frequency" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>How it Works:</strong></p>
<p>Gives high importance to words that are <em>frequent in this document</em> but <em>rare across all documents</em>.</p>
</li>
<li>
<p><strong>Example Sentence:</strong></p>
<p><strong>‚ÄúThe cat sat on the mat‚Äù</strong></p>
</li>
<li>
<p><strong>Vocabulary:</strong> same as before.</p>
</li>
<li>
<p><strong>TF-IDF Vector (example values):</strong></p>
<table>
<thead>
<tr>
<th>Word</th>
<th>the</th>
<th>cat</th>
<th>sat</th>
<th>on</th>
<th>mat</th>
</tr>
</thead>
<tbody>
<tr>
<td>TF-IDF</td>
<td>0.0</td>
<td>0.52</td>
<td>0.64</td>
<td>0.48</td>
<td>0.64</td>
</tr>
</tbody>
</table>
<p>‚Üí <strong>TF-IDF Vector:</strong> <strong>[0.0, 0.52, 0.64, 0.48, 0.64]</strong></p>
<p>(‚Äúthe‚Äù gets <strong>0.0</strong> because it appears everywhere in the dataset, so IDF ‚âà 0)</p>
</li>
<li>
<p><strong>Limitation:</strong></p>
<p>Still no understanding of meaning (e.g., ‚Äúcat‚Äù ‚â† ‚Äúkitten‚Äù to TF-IDF).</p>
</li>
</ul>
<h3 id="iii-one-hot-encoding"><strong>iii. One-Hot Encoding</strong><a class="headerlink" href="#iii-one-hot-encoding" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Idea:</strong></p>
<p>Every word gets a giant vector full of zeros except <strong>one 1</strong> at its index in the global vocabulary.</p>
</li>
<li>
<p><strong>Vocabulary Example (size = 5):</strong></p>
<p><code>["the", "cat", "sat", "on", "mat"]</code></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Word</th>
<th>Vector</th>
</tr>
</thead>
<tbody>
<tr>
<td>the</td>
<td>[1, 0, 0, 0, 0]</td>
</tr>
<tr>
<td>cat</td>
<td>[0, 1, 0, 0, 0]</td>
</tr>
<tr>
<td>sat</td>
<td>[0, 0, 1, 0, 0]</td>
</tr>
<tr>
<td>on</td>
<td>[0, 0, 0, 1, 0]</td>
</tr>
<tr>
<td>mat</td>
<td>[0, 0, 0, 0, 1]</td>
</tr>
<tr>
<td>- <strong>Sentence Representation:</strong></td>
<td></td>
</tr>
</tbody>
</table>
<pre><code>Usually stored as 5 separate one-hot vectors (one per word), e.g.:

**‚ÄúThe cat sat on the mat‚Äù ‚Üí**

[1,0,0,0,0]
[0,1,0,0,0]
[0,0,1,0,0]
[0,0,0,1,0]
[1,0,0,0,0]
[0,0,0,0,1]
</code></pre>
<ul>
<li><strong>Problems:</strong><ul>
<li>If vocab = 50,000 ‚Üí each word is a 50,000-length vector</li>
<li>No semantic meaning at all</li>
<li>‚Äúcat‚Äù, ‚Äúdog‚Äù, and ‚Äúcar‚Äù all look equally unrelated</li>
</ul>
</li>
</ul>
<h3 id="522-the-modern-way-contextual-embeddings"><strong>5.2.2. The "Modern Way" (Contextual Embeddings)</strong><a class="headerlink" href="#522-the-modern-way-contextual-embeddings" title="Permanent link">&para;</a></h3>
<p>Instead of <em>counting</em>, modern NLP systems <em>learn</em> the <strong>meaning</strong> of words by training neural networks on billions of sentences.</p>
<hr />
<h3 id="i-word2vec-word-vector"><strong>i. Word2Vec (Word ‚Üí Vector)</strong><a class="headerlink" href="#i-word2vec-word-vector" title="Permanent link">&para;</a></h3>
<p><strong>How it works :</strong></p>
<p>We train a tiny neural network.</p>
<p>Its task is <strong>fake</strong>:</p>
<blockquote>
<p>‚ÄúGiven a word, predict the words around it.‚Äù
</p>
</blockquote>
<p>Example window size = <strong>2</strong></p>
<p>Sentence: <strong>‚Äúthe cat sat on the mat‚Äù</strong></p>
<p>For each center word, the model tries to predict nearby words:</p>
<table>
<thead>
<tr>
<th>Center</th>
<th>Words predicted (window = 2)</th>
</tr>
</thead>
<tbody>
<tr>
<td>cat</td>
<td>the, sat</td>
</tr>
<tr>
<td>sat</td>
<td>cat, on</td>
</tr>
<tr>
<td>on</td>
<td>sat, the</td>
</tr>
<tr>
<td>mat</td>
<td>the</td>
</tr>
</tbody>
</table>
<p>Each training step:</p>
<p><strong>Input = one word ‚Üí Output = probabilities of surrounding words</strong></p>
<hr />
<p><strong>The Weight Matrix (What We Steal) :</strong></p>
<p>Inside Word2Vec is a <strong>big matrix of weights.</strong></p>
<p>Suppose vocabulary size = 10,000</p>
<p>Embedding dimension = 300</p>
<p>Matrix shape: <strong>10,000 √ó 300</strong></p>
<pre><code>        dim1 dim2 dim3 ... dim300
word1   0.12 0.88 0.01     0.33
word2   0.55 0.02 0.19     0.44
word3   0.90 0.10 0.77     0.12
 ...
</code></pre>
<p>Each <strong>row</strong> is a word‚Äôs embedding ‚Äî a 300-dimensional vector.</p>
<hr />
<h3 id="what-is-a-300-dimensional-vector"><strong>What is a 300-dimensional vector?</strong><a class="headerlink" href="#what-is-a-300-dimensional-vector" title="Permanent link">&para;</a></h3>
<p>Think of it like a <strong>profile</strong> of a word, described by 300 ‚Äúfeatures‚Äù the model learns by itself.</p>
<p>Example (tiny version using only 4 dims):</p>
<p>Vector(‚Äúcat‚Äù) =</p>
<pre><code>[0.8,  0.1,  0.3,  0.9]
</code></pre>
<p>Vector(‚Äúdog‚Äù) =</p>
<pre><code>[0.79, 0.12, 0.31, 0.91]
</code></pre>
<p>The numbers are <strong>not human-interpretable</strong>.</p>
<p>But the <strong>patterns</strong> let the model recognize similarity (cat ‚âà dog).</p>
<p>A 300-dimensional vector is just a longer version:</p>
<pre><code>[0.23, -0.18, 0.04, 1.22, ..., 0.09]  ‚Üê 300 numbers
</code></pre>
<p>Higher dimension ‚Üí more information about meaning.</p>
<hr />
<h3 id="detailed-example"><strong>Detailed Example:</strong><a class="headerlink" href="#detailed-example" title="Permanent link">&para;</a></h3>
<p><strong>Sentence: the cat sat on the mat</strong></p>
<p><strong>Window size = 2 (predict 2 words left &amp; right)</strong></p>
<p>Below is every training pair Word2Vec creates:</p>
<p><strong>1. Center word: ‚Äúthe‚Äù</strong></p>
<p>Neighbors: <em>cat</em></p>
<p>Training: the ‚Üí cat</p>
<p><strong>2. Center word: ‚Äúcat‚Äù</strong></p>
<p>Neighbors: the, sat</p>
<p>Training: cat ‚Üí (the, sat)</p>
<p><strong>3. Center word: ‚Äúsat‚Äù</strong></p>
<p>Neighbors: cat, on</p>
<p>Training: sat ‚Üí (cat, on)</p>
<p><strong>4. Center word: ‚Äúon‚Äù</strong></p>
<p>Neighbors: sat, the</p>
<p>Training: on ‚Üí (sat, the)</p>
<p><strong>5. Center word: ‚Äúthe‚Äù</strong></p>
<p>Neighbors: on, mat</p>
<p>Training: the ‚Üí (on, mat)</p>
<p><strong>6. Center word: ‚Äúmat‚Äù</strong></p>
<p>Neighbors: the</p>
<p>Training: mat ‚Üí the</p>
<p>This is done across <strong>millions</strong> of sentences.</p>
<p>The neural network slowly learns which words occur in similar contexts.</p>
<p>After training, we <strong>throw away the network</strong> and <strong>keep the weight matrix</strong>.</p>
<p>That matrix becomes:</p>
<ul>
<li>a dictionary</li>
<li>where every word = a learned vector</li>
<li>that captures meaning</li>
</ul>
<p>This is the famous Word2Vec trick.</p>
<h3 id="ii-glove-global-co-occurrence-matrix"><strong>ii. GloVe (Global Co-occurrence Matrix)</strong><a class="headerlink" href="#ii-glove-global-co-occurrence-matrix" title="Permanent link">&para;</a></h3>
<p>GloVe does <strong>NOT</strong> predict windows.</p>
<p>It builds one giant table counting <strong>how often words appear near each other</strong>.</p>
<p>Using the SAME sentence:</p>
<p>We count how many times each pair appears within a window of 2:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>the</th>
<th>cat</th>
<th>sat</th>
<th>on</th>
<th>mat</th>
</tr>
</thead>
<tbody>
<tr>
<td>the</td>
<td>‚Äî</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>cat</td>
<td>1</td>
<td>‚Äî</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>sat</td>
<td>0</td>
<td>1</td>
<td>‚Äî</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>on</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>‚Äî</td>
<td>1</td>
</tr>
<tr>
<td>mat</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>‚Äî</td>
</tr>
</tbody>
</table>
<p>Some examples from above:</p>
<ul>
<li>‚Äúcat‚Äù and ‚Äúthe‚Äù appear together once</li>
<li>‚Äúcat‚Äù and ‚Äúsat‚Äù appear together once</li>
<li>‚Äúon‚Äù and ‚Äúmat‚Äù appear together once</li>
<li>‚Äúsat‚Äù and ‚Äúmat‚Äù never appear together</li>
</ul>
<p>GloVe then <strong>factorizes</strong> this co-occurrence matrix into two smaller matrices (like compressing it).</p>
<p>The result of the factorization is:</p>
<pre><code>the ‚Üí 300D vector
cat ‚Üí 300D vector
sat ‚Üí 300D vector
on  ‚Üí 300D vector
mat ‚Üí 300D vector
</code></pre>
<p>Same output format as Word2Vec ‚Äî but learned differently.</p>
<hr />
<h3 id="iii-fasttext-breaks-words-into-character-pieces"><strong>iii. fastText (Breaks words into character pieces)</strong><a class="headerlink" href="#iii-fasttext-breaks-words-into-character-pieces" title="Permanent link">&para;</a></h3>
<p>fastText uses the SAME sentence, but it <strong>doesn‚Äôt learn vectors for whole words directly</strong>.</p>
<p>Using:</p>
<p><strong>‚Äúthe cat sat on the mat‚Äù</strong></p>
<p>Instead of learning a vector for ‚Äúcat‚Äù, fastText breaks it into character n-grams:</p>
<p>For n=3 (trigrams):</p>
<pre><code>&lt;ca, cat, at&gt;   ‚Üí for &quot;cat&quot;
&lt;sa, sat, at&gt;   ‚Üí for &quot;sat&quot;
&lt;ma, mat, at&gt;   ‚Üí for &quot;mat&quot;
&lt;th, the, he&gt;   ‚Üí for &quot;the&quot;
&lt;on&gt;            ‚Üí for &quot;on&quot;
</code></pre>
<p>Then fastText learns vectors for all these pieces.</p>
<p>Example (simplified):</p>
<pre><code>Vector(&quot;cat&quot;) = Vector(&quot;&lt;ca&quot;) + Vector(&quot;cat&quot;) + Vector(&quot;at&quot;)
Vector(&quot;mat&quot;) = Vector(&quot;&lt;ma&quot;) + Vector(&quot;mat&quot;) + Vector(&quot;at&quot;)
</code></pre>
<p>Since ‚Äúcat‚Äù and ‚Äúmat‚Äù share the subword ‚Äúat‚Äù, their vectors become similar.</p>
<p>That‚Äôs why fastText can handle:</p>
<ul>
<li>misspellings</li>
<li>new words</li>
<li>rare words</li>
<li>morphological changes (‚Äúsitting‚Äù, ‚Äúsits‚Äù, ‚Äúsat‚Äù)</li>
</ul>
<p>Even if the full word wasn't seen during training.</p>
<hr />
<h2 id="53-step-3-text-analysis-the-understanding-step"><strong>5.3. Step 3: Text Analysis (The "Understanding" Step)</strong><a class="headerlink" href="#53-step-3-text-analysis-the-understanding-step" title="Permanent link">&para;</a></h2>
<p>Now that our text is in a clean, numerical format, the real work can begin. This step involves feeding the numerical data into a <strong>model architecture</strong> (the "brain") to interpret and extract meaningful information.</p>
<h3 id="531-traditional-analysis-tasks">5.3.1. Traditional Analysis Tasks<a class="headerlink" href="#531-traditional-analysis-tasks" title="Permanent link">&para;</a></h3>
<p>This is <em>what</em> we want the model to do:</p>
<ul>
<li><strong>Part-of-Speech (POS) Tagging:</strong> Identifying nouns, verbs, adjectives, etc.</li>
<li><strong>Named Entity Recognition (NER):</strong> Finding people, places, and organizations.</li>
<li><strong>Sentiment Analysis:</strong> Determining if the tone is positive or negative.</li>
<li><strong>Topic Modeling:</strong> Finding the main themes in a document.</li>
</ul>
<h3 id="532-modern-model-architectures-the-brain"><strong>5.3.2. Modern Model Architectures (The "Brain")</strong><a class="headerlink" href="#532-modern-model-architectures-the-brain" title="Permanent link">&para;</a></h3>
<p>A standard ANN has no memory. If you give it:</p>
<pre><code>Input 1: ‚Äúhow‚Äù
Input 2: ‚Äúare‚Äù
</code></pre>
<p>It completely forgets ‚Äúhow‚Äù when it receives ‚Äúare‚Äù.</p>
<p>But language is <strong>sequential</strong>. Meaning depends on <em>order</em>.</p>
<p>So we need models that can ‚Äúremember‚Äù previous inputs.</p>
<hr />
<h3 id="i-recurrent-neural-networks-rnns"><strong>i. Recurrent Neural Networks (RNNs)</strong><a class="headerlink" href="#i-recurrent-neural-networks-rnns" title="Permanent link">&para;</a></h3>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%206.png" /></p>
<p>An RNN adds a <strong>loop</strong> so that information can be passed from one step to the next.</p>
<pre><code>Input x‚ÇÅ ‚Üí (RNN) ‚Üí h‚ÇÅ ‚Üí output
Input x‚ÇÇ ‚Üí (RNN) ‚Üí h‚ÇÇ ‚Üí output
Input x‚ÇÉ ‚Üí (RNN) ‚Üí h‚ÇÉ ‚Üí output
</code></pre>
<p>Where:</p>
<ul>
<li><strong>x‚Çú = word embedding at time t</strong></li>
<li><strong>h‚Çú = hidden state (RNN‚Äôs memory)</strong></li>
</ul>
<p>The hidden state is updated as:</p>
<pre><code>h‚Çú = tanh(Wx * x‚Çú + Wh * h‚Çú‚Çã‚ÇÅ)
</code></pre>
<h3 id="example-encode-a-4-word-sentence-step-by-step"><strong>Example: Encode a 4-word sentence step-by-step</strong><a class="headerlink" href="#example-encode-a-4-word-sentence-step-by-step" title="Permanent link">&para;</a></h3>
<p>Sentence: <strong>‚ÄúI love deep learning‚Äù</strong></p>
<p>Assume each word is turned into a 4-dimensional vector (tiny example to understand the process).</p>
<p>Let the embeddings be:</p>
<pre><code>I          ‚Üí [1, 0, 0, 0]
love       ‚Üí [0, 1, 0, 0]
deep       ‚Üí [0, 0, 1, 0]
learning   ‚Üí [0, 0, 0, 1]
</code></pre>
<p>Start with <strong>h‚ÇÄ = [0,0,0,0]</strong> (zero memory).</p>
<hr />
<p><strong>Step 1: word = ‚ÄúI‚Äù</strong></p>
<pre><code>x‚ÇÅ = [1, 0, 0, 0]
h‚ÇÅ = tanh(Wx*x‚ÇÅ + Wh*h‚ÇÄ)
</code></pre>
<p>Hidden state might become something like:</p>
<pre><code>h‚ÇÅ = [0.6, -0.1, 0.2, 0.0]
</code></pre>
<hr />
<p><strong>Step 2: word = ‚Äúlove‚Äù</strong></p>
<pre><code>x‚ÇÇ = [0, 1, 0, 0]
h‚ÇÇ = tanh(Wx*x‚ÇÇ + Wh*h‚ÇÅ)
</code></pre>
<p>Now memory includes both <strong>I</strong> + <strong>love</strong>:</p>
<pre><code>h‚ÇÇ = [0.40, 0.55, -0.10, 0.20]
</code></pre>
<hr />
<p><strong>Step 3: word = ‚Äúdeep‚Äù</strong></p>
<pre><code>h‚ÇÉ = tanh(Wx*x‚ÇÉ + Wh*h‚ÇÇ)
</code></pre>
<p>Memory grows again:</p>
<pre><code>h‚ÇÉ = [0.10, 0.62, 0.33, 0.45]
</code></pre>
<hr />
<p><strong>Step 4: word = ‚Äúlearning‚Äù</strong></p>
<pre><code>h‚ÇÑ = tanh(Wx*x‚ÇÑ + Wh*h‚ÇÉ)
</code></pre>
<p>Final encoding of the entire sentence:</p>
<pre><code>h‚ÇÑ = [0.21, 0.70, 0.55, 0.63]
</code></pre>
<p><strong>This final <code>h‚ÇÑ</code> is the vector representation of the whole sentence.</strong></p>
<p>This is how RNNs represent sequences.</p>
<hr />
<p><strong>Why RNNs Struggle:</strong> They only have <strong>one state (<code>h‚Çú</code>)</strong>. Over many steps, early information fades ‚Üí <strong>vanishing gradient problem</strong>.</p>
<hr />
<h3 id="ii-long-short-term-memory-lstms"><strong>ii. Long Short-Term Memory (LSTMs)</strong><a class="headerlink" href="#ii-long-short-term-memory-lstms" title="Permanent link">&para;</a></h3>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%207.png" /></p>
<p>LSTMs fix this by adding <strong>two memory paths</strong>:</p>
<ol>
<li><strong>Hidden state (h‚Çú)</strong> ‚Üí short-term memory</li>
<li><strong>Cell state (c‚Çú)</strong> ‚Üí long-term memory (highway through time)</li>
</ol>
<p>Each step uses <strong>three gates</strong>:</p>
<ul>
<li><strong>Forget gate</strong> ‚Üí remove useless old info</li>
<li><strong>Input gate</strong> ‚Üí add important new info</li>
<li><strong>Output gate</strong> ‚Üí decide what to reveal as the hidden state</li>
</ul>
<hr />
<h3 id="example-same-sentence-encoded-with-lstm"><strong>Example: Same sentence encoded with LSTM</strong><a class="headerlink" href="#example-same-sentence-encoded-with-lstm" title="Permanent link">&para;</a></h3>
<p>Sentence: <strong>‚ÄúI love deep learning‚Äù</strong></p>
<p>Each step outputs <strong>two vectors</strong>:</p>
<ul>
<li>hidden state <strong>h‚Çú</strong></li>
<li>cell state <strong>c‚Çú</strong></li>
</ul>
<p>Start with:</p>
<pre><code>h‚ÇÄ = [0,0,0,0]
c‚ÇÄ = [0,0,0,0]
</code></pre>
<hr />
<p><strong>Step 1: ‚ÄúI‚Äù</strong></p>
<p>The gates decide what to store.</p>
<p>Example (illustration):</p>
<pre><code>c‚ÇÅ = [0.9, -0.1, 0.1, 0.0]
h‚ÇÅ = [0.7, -0.05, 0.15, 0.0]
</code></pre>
<hr />
<p><strong>Step 2: ‚Äúlove‚Äù</strong></p>
<p>Forget gate removes irrelevant parts of <code>c‚ÇÅ</code>.</p>
<p>Input gate adds new info.</p>
<pre><code>c‚ÇÇ = [0.85, 0.40, 0.25, 0.10]
h‚ÇÇ = [0.62, 0.55, 0.20, 0.15]
</code></pre>
<hr />
<p><strong>Step 3: ‚Äúdeep‚Äù</strong></p>
<pre><code>c‚ÇÉ = [0.80, 0.57, 0.60, 0.33]
h‚ÇÉ = [0.55, 0.60, 0.45, 0.40]
</code></pre>
<hr />
<p><strong>Step 4: ‚Äúlearning‚Äù</strong></p>
<p>Final states:</p>
<pre><code>c‚ÇÑ = [0.90, 0.70, 0.85, 0.75]
h‚ÇÑ = [0.60, 0.75, 0.65, 0.58]
</code></pre>
<hr />
<p>RNN: Only <strong>h‚ÇÑ</strong> contains the meaning of the whole sentence.</p>
<p>LSTM: Both <strong>h‚ÇÑ</strong> and <strong>c‚ÇÑ</strong> represent the final sentence meaning.</p>
<ul>
<li><code>h‚ÇÑ</code>: what the model is ‚Äúfocusing on‚Äù at the last word</li>
<li><code>c‚ÇÑ</code>: deep long-term memory preserved across the sentence</li>
</ul>
<p>You can think of LSTM like:</p>
<pre><code>h‚Çú = short-term note
c‚Çú = long-term diary
</code></pre>
<p>This is why LSTMs understand <strong>long sentences</strong> much better than basic RNNs.</p>
<hr />
<h3 id="iii-the-modern-revolution-the-transformer"><strong>iii. The Modern Revolution (The Transformer)</strong><a class="headerlink" href="#iii-the-modern-revolution-the-transformer" title="Permanent link">&para;</a></h3>
<p>Even LSTMs struggle with very long sentences, and their sequential nature (processing one word at a time) makes them slow to train. The <strong>Transformer</strong> architecture solved this.</p>
<h3 id="a-encoder-decoder-models"><strong>a) Encoder-Decoder Models</strong><a class="headerlink" href="#a-encoder-decoder-models" title="Permanent link">&para;</a></h3>
<p>This architecture is key to tasks like machine translation.</p>
<ol>
<li><strong>Encoder:</strong> An "encoder" (which could be an RNN) reads the entire input sentence (e.g., "How are you?") and compresses its full meaning into a single vector (a "context vector").</li>
<li><strong>Decoder:</strong> A "decoder" (another RNN) takes that <em>one</em> vector and "decodes" it into the output sentence (e.g., "¬øC√≥mo est√°s?").</li>
<li><strong>The Problem:</strong> This single context vector is a <strong>bottleneck</strong>. It's hard to cram the entire meaning of a 50-word sentence into one vector.</li>
</ol>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%208.png" /></p>
<h3 id="b-the-breakthrough-the-attention-mechanism"><strong>b) The Breakthrough: The Attention Mechanism</strong><a class="headerlink" href="#b-the-breakthrough-the-attention-mechanism" title="Permanent link">&para;</a></h3>
<p><strong>Attention</strong> solved the bottleneck. Instead of forcing the decoder to rely on <em>one</em> vector, it allows the decoder to "look back" at <em>all</em> the encoder's outputs from the <em>entire</em> input sentence at every step.</p>
<p>It learns to "pay attention" to the specific input words that are most relevant for generating the <em>current</em> output word. This was a massive leap in performance.</p>
<ul>
<li><strong>Advantage:</strong> It's <strong>highly parallelizable</strong> (much faster to train) and can capture <em>extremely</em> long-range dependencies, making it the new state-of-the-art.</li>
</ul>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%209.png" /></p>
<h3 id="iv-modern-models-bert-gpt"><strong>iv. Modern Models: BERT &amp; GPT</strong><a class="headerlink" href="#iv-modern-models-bert-gpt" title="Permanent link">&para;</a></h3>
<p>These are the two most famous models built on the Transformer architecture.</p>
<h3 id="a-bert-bidirectional-encoder-representations-from-transformers"><strong>a) BERT (Bidirectional Encoder Representations from Transformers)</strong><a class="headerlink" href="#a-bert-bidirectional-encoder-representations-from-transformers" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>What it is:</strong> An <strong>Encoder-only</strong> Transformer.</li>
<li><strong>How it Learns:</strong> It's trained by taking a sentence, "masking" (hiding) 15% of the words, and then trying to predict those hidden words.</li>
<li><strong>Key Feature:</strong> It's <strong>bidirectional</strong>. To predict a masked word, it looks at <em>both</em> the words that come <em>before</em> it and the words that come <em>after</em> it.</li>
<li><strong>Best For:</strong> <strong>Understanding</strong> tasks. It builds a deep understanding of context, making it perfect for sentiment analysis, question answering, and text classification.</li>
</ul>
<h3 id="b-gpt-generative-pre-trained-transformer"><strong>b) GPT (Generative Pre-trained Transformer)</strong><a class="headerlink" href="#b-gpt-generative-pre-trained-transformer" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>What it is:</strong> A <strong>Decoder-only</strong> Transformer.</li>
<li><strong>How it Learns:</strong> It's trained as a "language model," meaning it simply tries to predict the <em>very next word</em> in a sentence, given all the words that came before it.</li>
<li><strong>Key Feature:</strong> It's <strong>auto-regressive</strong> (one-way). It only looks <em>backward</em> (at the words that came before).</li>
<li><strong>Best For:</strong> <strong>Generation</strong> tasks. Because it's trained to "predict the next word," it is exceptional at writing essays, holding conversations, summarizing text, and generating creative content.</li>
</ul>
<p><img alt="image.png" src="../../assets/DAY%203%20DEEP%20LEARNING%20%26%20NLP/image%2010.png" /></p>
<hr />
<h2 id="54-step-4-model-training-the-learning-step"><strong>5.4. Step 4: Model Training (The "Learning" Step)</strong><a class="headerlink" href="#54-step-4-model-training-the-learning-step" title="Permanent link">&para;</a></h2>
<p>This step is the <em>process</em> that "teaches" the model architectures from Step 3.</p>
<p>This is where the model "learns" by looking for patterns and relationships within the data.</p>
<ol>
<li><strong>Feed Data:</strong> The model (e.g., BERT) is fed the numerical data from Step 2.</li>
<li><strong>Make Prediction:</strong> It makes a prediction (e.g., "I think this movie review is positive").</li>
<li><strong>Check Answer:</strong> It checks its prediction against the right answer (the "label").</li>
<li><strong>Measure Error:</strong> It measures how "wrong" it was (this is called the "loss").</li>
<li><strong>Adjust:</strong> It slightly adjusts its internal parameters (weights) to be "less wrong" next time.</li>
</ol>
<p>This process is repeated millions or even billions of times. Once "trained," this model can be saved and used in Step 3 to make predictions on new, unseen data.</p>
<hr />
<h1 id="6-why-is-nlp-so-hard"><strong>6. Why is NLP So Hard?</strong><a class="headerlink" href="#6-why-is-nlp-so-hard" title="Permanent link">&para;</a></h1>
<p>Human language is incredibly complex and messy. Even the best NLP models struggle with the same things humans do. These "ambiguities" are the biggest challenge.</p>
<ul>
<li><strong>Biased Training Data:</strong> If the data used to train a model is biased (e.g., pulled from biased parts of the web), the model's answers will also be biased. This is a major risk, especially in sensitive fields like healthcare or HR.</li>
<li><strong>Misinterpretation ("Garbage In, Garbage Out"):</strong> A model can easily get confused by messy, real-world language, including:<ul>
<li>Slang, idioms, or fragments</li>
<li>Mumbled words or strong dialects</li>
<li>Bad grammar or misspellings</li>
<li>Homonyms (e.g., "bear" the animal vs. "bear" the burden)</li>
</ul>
</li>
<li><strong>Tone of Voice &amp; Sarcasm:</strong> The <em>way</em> something is said can change its meaning completely. Models struggle to detect sarcasm or exaggeration, as they often only "read" the words, not the intent.</li>
<li><strong>New and Evolving Language:</strong> New words are invented all the time ("rizz," "skibidi"), and grammar rules evolve. Models can't keep up unless they are constantly retrained.</li>
</ul>
<hr />
<h1 id="7-where-is-nlp-used"><strong>7. Where is NLP Used?</strong><a class="headerlink" href="#7-where-is-nlp-used" title="Permanent link">&para;</a></h1>
<p>You can find NLP applications in almost every major industry.</p>
<ul>
<li><strong>Finance:</strong> NLP models instantly read financial reports, news articles, and social media to help make split-second trading decisions.</li>
<li><strong>Healthcare:</strong> NLP analyzes millions of medical records and research papers at once, helping doctors detect diseases earlier or find new insights.</li>
<li><strong>Insurance:</strong> Models analyze insurance claims to spot patterns (like potential fraud) and help automate the claims process.</li>
<li><strong>Legal:</strong> Instead of lawyers manually reading millions of documents for a case, NLP can automate "legal discovery" by scanning and finding all relevant information.</li>
</ul>
<p>A computer can't just "read" a sentence. To get from raw human language to a useful insight, it follows a strict "assembly line" process.</p>
<hr />
<h1 id="8-practical-implementation-next-word-prediction-using-pre-trained-model"><strong>8. Practical Implementation: Next-Word Prediction using Pre-Trained Model</strong><a class="headerlink" href="#8-practical-implementation-next-word-prediction-using-pre-trained-model" title="Permanent link">&para;</a></h1>
<h2 id="fine-tuning-bert-on-harry-potter-corpus">Fine-Tuning BERT on Harry Potter Corpus<a class="headerlink" href="#fine-tuning-bert-on-harry-potter-corpus" title="Permanent link">&para;</a></h2>
<p><strong>Open the Colab Link, Make a Copy and Upload the dataset on Colab</strong></p>
<p><strong>üìì Colab Notebook:</strong></p>
<p><a href="https://colab.research.google.com/drive/1RGcpQuLJz-I7EYQPfaEYDR01m6IqEMfG?usp=sharing">Open in Google Colab</a> </p>
<p><strong>üìä Dataset:</strong>  </p>
<p><a href="../../files/day3/harry_potter_corpus.txt">harry_potter_corpus.txt</a></p>
<hr />
<h1 id="9-summary">9. Summary<a class="headerlink" href="#9-summary" title="Permanent link">&para;</a></h1>
<ul>
<li>Covered <strong>Deep Learning basics</strong>, including artificial neurons, neural networks, and how models learn using <strong>forward pass, loss, backpropagation, and gradient descent</strong>.</li>
<li>Explored <strong>why deep learning outperforms traditional ML</strong>, handling unstructured data and learning features automatically.</li>
<li>Introduced <strong>NLP</strong>, its evolution from <strong>rules-based</strong> to <strong>statistical</strong> to <strong>deep learning approaches</strong>.</li>
<li>Learned <strong>text preprocessing</strong>, feature extraction, and vectorization methods: <strong>BoW, TF-IDF, One-Hot, Word2Vec, GloVe, fastText</strong>.</li>
<li>Studied <strong>sequence models</strong>: RNNs, LSTMs, and the <strong>Transformer architecture</strong> with <strong>attention mechanism</strong>.</li>
<li>Covered modern NLP models: <strong>BERT for understanding</strong> and <strong>GPT for text generation</strong>, and their real-world applications.</li>
<li>Discussed <strong>challenges in NLP</strong>, like ambiguity, sarcasm, bias, evolving language, and applications in finance, healthcare, insurance, legal, and more.</li>
</ul>
<h2 id="see-you-next-week">See you next week! üöÄ<a class="headerlink" href="#see-you-next-week" title="Permanent link">&para;</a></h2>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 - ACM BITS Pilani Dubai Campus
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://instagram.com/acmbpdc" target="_blank" rel="noopener" title="instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M224.3 141a115 115 0 1 0-.6 230 115 115 0 1 0 .6-230m-.6 40.4a74.6 74.6 0 1 1 .6 149.2 74.6 74.6 0 1 1-.6-149.2m93.4-45.1a26.8 26.8 0 1 1 53.6 0 26.8 26.8 0 1 1-53.6 0m129.7 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8M399 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/acmbpdc" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://discord.gg/DYQdxquYwP" target="_blank" rel="noopener" title="discord.gg" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "header.autohide"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>