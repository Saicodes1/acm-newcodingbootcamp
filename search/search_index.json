{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACM Machine Learning Workshop","text":""},{"location":"#unlock-the-power-of-data-with-machine-learning","title":"Unlock the Power of Data with Machine Learning","text":"<p>Welcome to the ACM Machine Learning Workshop, where curiosity meets computation. This workshop is your gateway into the fascinating world of Machine Learning (ML) \u2014 a field that empowers computers to learn from data and make intelligent decisions without being explicitly programmed.</p>"},{"location":"#about-this-workshop","title":"About This Workshop","text":"<p>This 4-day workshop is designed to blend theory with hands-on practice. You'll explore essential ML concepts, clean and prepare real datasets, build models, and even experiment with deep learning and natural language processing.</p>"},{"location":"#workshop-outline","title":"Workshop Outline","text":"Day Topic Highlights Day 1 Data Cleaning &amp; Feature Engineering Handle missing data, create features, prepare datasets Day 2 Model Training &amp; Analysis Train, test, and evaluate key ML models"},{"location":"#1-what-is-machine-learning","title":"1. What is Machine Learning?","text":"<p>Machine Learning is a branch of Artificial Intelligence (AI) that enables systems to identify patterns, make predictions, and improve automatically through experience. Unlike traditional programming where we explicitly code rules, ML allows computers to discover rules and patterns from data.</p>"},{"location":"#11-how-does-machine-learning-work","title":"1.1 How Does Machine Learning Work?","text":"<p>Think of ML like teaching a child to recognize animals. Instead of giving them a rulebook with detailed descriptions, you show them many pictures of cats and dogs. Over time, they learn to identify the patterns that distinguish cats from dogs \u2014 pointy ears, whiskers, size, behavior, etc.</p> <p>Similarly, ML systems: 1. Learn from data (training phase) 2. Identify patterns automatically 3. Make predictions on new, unseen data 4. Improve over time with more experience</p> <p>Data --&gt; ML Algorithm --&gt; Model --&gt; Predictions</p>"},{"location":"#12-real-world-applications-use-cases","title":"1.2 Real-World Applications &amp; Use Cases","text":"<p>Machine Learning is transforming industries and solving complex problems across diverse domains:</p>"},{"location":"#121-entertainment-media","title":"1.2.1 Entertainment &amp; Media","text":"<ul> <li>Netflix &amp; Spotify: Personalized recommendations based on viewing/listening history</li> <li>YouTube: Content suggestions and automatic video categorization</li> <li>Gaming: AI opponents that adapt to player behavior</li> </ul>"},{"location":"#122-healthcare-life-sciences","title":"1.2.2 Healthcare &amp; Life Sciences","text":"<ul> <li>Disease Diagnosis: Early detection of cancer, diabetes, and heart conditions from medical images</li> <li>Drug Discovery: Predicting molecular behavior to accelerate pharmaceutical research</li> <li>Patient Monitoring: Predicting patient deterioration in ICUs</li> <li>Personalized Treatment: Recommending treatments based on genetic profiles</li> </ul>"},{"location":"#123-finance-banking","title":"1.2.3 Finance &amp; Banking","text":"<ul> <li>Fraud Detection: Identifying suspicious transactions in real-time</li> <li>Credit Scoring: Assessing loan eligibility and risk</li> <li>Algorithmic Trading: Predicting stock market trends</li> <li>Customer Segmentation: Personalizing banking services</li> </ul>"},{"location":"#124-transportation-autonomous-vehicles","title":"1.2.4 Transportation &amp; Autonomous Vehicles","text":"<ul> <li>Self-Driving Cars: Tesla, Waymo using computer vision and sensor fusion</li> <li>Route Optimization: Uber, Google Maps predicting traffic and suggesting routes</li> <li>Predictive Maintenance: Anticipating vehicle component failures</li> </ul>"},{"location":"#125-manufacturing-industry","title":"1.2.5 Manufacturing &amp; Industry","text":"<ul> <li>Quality Control: Detecting defects in production lines</li> <li>Predictive Maintenance: Preventing equipment failures</li> <li>Supply Chain Optimization: Forecasting demand and managing inventory</li> <li>Energy Optimization: Reducing power consumption</li> </ul>"},{"location":"#13-what-kind-of-problems-can-ml-solve","title":"1.3 What Kind of Problems Can ML Solve?","text":"<p>Machine Learning excels at solving problems that fall into these categories:</p>"},{"location":"#131-classification-problems","title":"1.3.1 Classification Problems","text":"<p>Assigning data into predefined categories</p> <ul> <li>Is this email spam or not spam?</li> <li>Is this tumor malignant or benign?</li> <li>Which animal is in this photo?</li> <li>Will this customer churn or stay?</li> </ul> <p>Suggested Image: Classification visualization (e.g., scatter plot with different colored regions)</p>"},{"location":"#132-regression-problems","title":"1.3.2 Regression Problems","text":"<p>Predicting continuous numerical values</p> <ul> <li>What will be the house price?</li> <li>How much will sales be next month?</li> <li>What temperature will it be tomorrow?</li> <li>What's the expected lifetime value of a customer?</li> </ul>"},{"location":"#133-clustering-problems","title":"1.3.3 Clustering Problems","text":"<p>Finding natural groupings in data</p> <ul> <li>Customer segmentation for targeted marketing</li> <li>Document organization and topic modeling</li> <li>Genetic sequence analysis</li> <li>Social network community detection</li> </ul> <p>Suggested Image: Cluster visualization showing data points grouped into different clusters</p>"},{"location":"#134-anomaly-detection","title":"1.3.4 Anomaly Detection","text":"<p>Identifying unusual patterns</p> <ul> <li>Credit card fraud detection</li> <li>Network intrusion detection</li> <li>Manufacturing defect identification</li> <li>System health monitoring</li> </ul>"},{"location":"#135-recommendation-systems","title":"1.3.5 Recommendation Systems","text":"<p>Suggesting relevant items</p> <ul> <li>Movie/music recommendations</li> <li>Product suggestions</li> <li>Content personalization</li> <li>Friend suggestions on social media</li> </ul>"},{"location":"#136-natural-language-processing","title":"1.3.6 Natural Language Processing","text":"<p>Understanding and generating human language</p> <ul> <li>Sentiment analysis (is this review positive or negative?)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Chatbots and virtual assistants</li> </ul> <p>Suggested Image: Word cloud or sentiment analysis visualization</p>"},{"location":"#137-computer-vision","title":"1.3.7 Computer Vision","text":"<p>Understanding visual information</p> <ul> <li>Face recognition</li> <li>Object detection and tracking</li> <li>Medical image analysis</li> <li>Autonomous navigation</li> </ul> <p>Suggested Image: Object detection example (image with bounding boxes and labels)</p>"},{"location":"#14-when-not-to-use-machine-learning","title":"1.4 When NOT to Use Machine Learning","text":"<p>While ML is powerful, it's not always the right solution:</p> <ul> <li>When simple rules suffice: If you can solve it with if-else statements, you probably don't need ML</li> <li>When data is scarce: ML needs substantial data to learn patterns</li> <li>When interpretability is critical: Some ML models are \"black boxes\" (though this is improving)</li> <li>When the problem is well-understood: Traditional algorithms may be more efficient</li> </ul>"},{"location":"#15-the-ml-workflow","title":"1.5 The ML Workflow","text":"<p>Every ML project follows a similar journey:</p> <ol> <li>Problem Definition: What are you trying to predict or understand?</li> <li>Data Collection: Gather relevant data</li> <li>Data Preparation: Clean, transform, and engineer features (Day 1)</li> <li>Model Selection: Choose appropriate algorithms</li> <li>Training: Let the model learn from data (will be taught on Day 2)</li> <li>Evaluation: Measure performance on test data (will be taught on Day 2)</li> <li>Deployment: Put the model into production (will be taught on Day 4)</li> <li>Monitoring: Track performance and retrain as needed</li> </ol>"},{"location":"#2-why-learn-machine-learning","title":"2. Why Learn Machine Learning?","text":"<p>Machine Learning has become one of the most in-demand skills across industries. By understanding ML, you can:</p> <ul> <li>Turn data into actionable insights.</li> <li>Build intelligent systems that adapt and improve.</li> <li>Contribute to innovations in AI, automation, and analytics.</li> </ul>"},{"location":"#3-resources-next-steps","title":"3. Resources &amp; Next Steps","text":"<ul> <li>Recommended reading and tutorials will be shared during the sessions.</li> <li>After the workshop, try applying what you've learned to a small end-to-end project.</li> <li>Questions? Feel free to ask during the sessions or reach out via the repository.</li> </ul> <p>We hope you enjoy the workshop and gain valuable insights into the world of machine learning!</p>"},{"location":"day1/day1/","title":"Day 1: Data Cleaning and Feature Engineering","text":""},{"location":"day1/day1/#workshop-resources","title":"\ud83d\udce5 Workshop Resources","text":"<p>For today's hands-on session, you'll need the following materials:</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: Download day1_materials.zip - Contains the Bangalore house prices dataset</p> <p>Important: Make a copy of the Colab notebook (File \u2192 Save a copy in Drive) before running it. We'll be using these resources throughout today's session.</p>"},{"location":"day1/day1/#1-introduction-to-real-estate-price-prediction","title":"1. Introduction to Real Estate Price Prediction","text":"<p>Welcome to Day 1 of our Machine Learning workshop! Today, we'll embark on an exciting journey to build a real estate price prediction model using data from Bangalore, India. Before we dive into coding, let's understand the fundamental concepts that make machine learning projects successful.</p>"},{"location":"day1/day1/#2-understanding-the-problem","title":"2. Understanding the Problem","text":"<p>Imagine you're a real estate agent or a home buyer trying to determine the fair price of a property. What factors would you consider? The location, size of the house, number of bedrooms, bathrooms, and many other features play crucial roles. Our goal is to teach a computer to understand these patterns and predict prices automatically.</p>"},{"location":"day1/day1/#3-the-machine-learning-pipeline","title":"3. The Machine Learning Pipeline","text":"<p>Every successful machine learning project follows a structured approach:</p> <ol> <li>Data Collection: Gathering relevant information</li> <li>Data Cleaning: Removing errors and inconsistencies</li> <li>Feature Engineering: Creating meaningful variables from raw data</li> <li>Exploratory Data Analysis: Understanding patterns in data</li> <li>Model Building: Training algorithms (we'll cover this in Day 2)</li> <li>Model Evaluation: Testing how well our model performs</li> </ol> <p>Today, we'll focus on the first four crucial steps - the foundation of any ML project.</p>"},{"location":"day1/day1/#4-data-cleaning-the-foundation-of-quality-models","title":"4. Data Cleaning: The Foundation of Quality Models","text":""},{"location":"day1/day1/#41-why-is-data-cleaning-important","title":"4.1 Why is Data Cleaning Important?","text":"<p>Think of data cleaning like preparing ingredients before cooking. You wouldn't use rotten vegetables or unwashed produce in a meal, right? Similarly, dirty data leads to poor predictions. Real-world data is messy - it has missing values, inconsistencies, duplicates, and errors that can mislead our model.</p>"},{"location":"day1/day1/#42-understanding-missing-values","title":"4.2 Understanding Missing Values","text":"<p>Example Scenario: Imagine a dataset of house listings where some entries don't have information about the number of bathrooms or the location. What should we do?</p> <p>Two Common Approaches:</p> <ol> <li>Deletion: Remove rows with missing data (when dataset is large)</li> <li>Imputation: Fill missing values with mean, median, or mode (when data is scarce)</li> </ol> <p>When to delete vs. impute? If you have 13,000 rows and only 1,000 have missing values, deletion is safe. But if 8,000 rows have missing values, you might want to impute to preserve information.</p> <p>Example DataFrame - Before:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 NaN 1100 60 Electronic City 4 NaN 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 NaN 950 55 Indiranagar 4 3.0 1800 110 <p>After Deletion:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Koramangala 3 3.0 1450 95 Indiranagar 4 3.0 1800 110 <p>Result: Lost 3 rows of data</p> <p>After Imputation (with median = 2.5):</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 2.5 1100 60 Electronic City 4 2.5 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 2.5 950 55 Indiranagar 4 3.0 1800 110 <p>Result: Preserved all 6 rows of data</p>"},{"location":"day1/day1/#43-dealing-with-irrelevant-features","title":"4.3 Dealing with Irrelevant Features","text":"<p>Not every piece of information is useful. Consider these columns in a house price dataset:</p> <ul> <li>Society Name: The specific housing society</li> </ul> <p>Question: Do these strongly influence price predictions? Often, the answer is no. Removing irrelevant features:</p> <ul> <li>Simplifies the model</li> <li>Reduces computational cost</li> <li>Prevents overfitting</li> <li>Improves model performance</li> </ul>"},{"location":"day1/day1/#44-standardizing-data-formats","title":"4.4 Standardizing Data Formats","text":"<p>The Problem: Your dataset has a \"size\" column with values like:</p> <ul> <li>\"2 BHK\"</li> <li>\"3 Bedroom\"</li> <li>\"4 BHK\"</li> </ul> <p>The Solution: Extract just the numeric part (2, 3, 4) to create a consistent \"bhk\" (Bedroom, Hall, Kitchen) column that machines can understand.</p> <p>Example - Before:</p> Location Size Bath Price Whitefield 2 BHK 2 60 Marathahalli 3 Bedroom 2 85 HSR Layout 4 BHK 3 120 Koramangala 2 Bedroom 1 55 Indiranagar 3 BHK 2 95 Electronic City 1 RK 1 35 <p>After Standardization:</p> Location Size BHK Bath Price Whitefield 2 BHK 2 2 60 Marathahalli 3 Bedroom 3 2 85 HSR Layout 4 BHK 4 3 120 Koramangala 2 Bedroom 2 1 55 Indiranagar 3 BHK 3 2 95 Electronic City 1 RK 1 1 35 <p>Now we have a clean numeric BHK column that machines can process!</p>"},{"location":"day1/day1/#45-handling-range-values","title":"4.5 Handling Range Values","text":"<p>Example: A property's size is listed as \"1133 - 1384 sq ft\" instead of a single number.</p> <p>Solution: Convert ranges to their average. For \"1133 - 1384\", we'd use (1133 + 1384) / 2 = 1258.5 sq ft.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Whitefield 3 1200 85 Marathahalli 2 1133 - 1384 60 HSR Layout 2 950 - 1100 55 Koramangala 4 2200 - 2450 120 Indiranagar 3 1500 95 Electronic City 3 1350 - 1550 80 <p>After Range Conversion:</p> Location BHK Total_sqft Price Whitefield 3 1200.0 85 Marathahalli 2 1258.5 60 HSR Layout 2 1025.0 55 Koramangala 4 2325.0 120 Indiranagar 3 1500.0 95 Electronic City 3 1450.0 80 <p>All range values are now converted to single numeric values (averages)</p>"},{"location":"day1/day1/#46-cleaning-inconsistent-units","title":"4.6 Cleaning Inconsistent Units","text":"<p>Sometimes you'll find values like:</p> <ul> <li>\"2500 sq ft\"</li> <li>\"34.46 Sq. Meter\"</li> <li>\"4125 Perch\"</li> </ul> <p>These mixed units make comparison impossible. The best approach is to convert everything to a standard unit or exclude entries that can't be converted reliably.</p>"},{"location":"day1/day1/#5-feature-engineering-creating-meaningful-variables","title":"5. Feature Engineering: Creating Meaningful Variables","text":"<p>Feature engineering is the art of creating new, more informative variables from existing data. It's often the difference between a mediocre and an excellent model.</p>"},{"location":"day1/day1/#51-creating-price-per-square-foot","title":"5.1 Creating Price Per Square Foot","text":"<p>Why? Absolute price doesn't tell the whole story. A 3000 sq ft house costing \u20b960 lakhs might be a better deal than a 1000 sq ft house at \u20b930 lakhs.</p> <p>Calculation: Price per sq ft = (Price \u00d7 100,000) / Total Square Feet</p> <p>This normalized metric helps us compare properties of different sizes on equal footing.</p> <p>Example - Before:</p> Location BHK Total_sqft Price (Lakhs) Whitefield 2 1000 30 Marathahalli 3 3000 60 Koramangala 3 1500 50 HSR Layout 2 900 35 Indiranagar 4 2000 80 Electronic City 3 1200 45 <p>After Feature Engineering:</p> Location BHK Total_sqft Price (Lakhs) Price_per_sqft Whitefield 2 1000 30 3,000 Marathahalli 3 3000 60 2,000 Koramangala 3 1500 50 3,333 HSR Layout 2 900 35 3,889 Indiranagar 4 2000 80 4,000 Electronic City 3 1200 45 3,750 <p>Insights from Price per sqft: - Despite Marathahalli being expensive (\u20b960L), it has the lowest price per sqft (\u20b92,000) - HSR Layout offers better value at \u20b93,889 per sqft - Indiranagar is the most expensive at \u20b94,000 per sqft</p>"},{"location":"day1/day1/#52-grouping-rare-categories","title":"5.2 Grouping Rare Categories","text":"<p>The Problem: Your dataset has 1,293 unique locations, but 1,052 of them appear fewer than 10 times.</p> <p>The Solution: Group infrequent categories into an \"other\" category. Why?</p> <ol> <li>Statistical Significance: Locations with only 1-2 properties don't provide enough data for reliable patterns</li> <li>Model Simplicity: Fewer categories mean fewer variables to process</li> <li>Generalization: Helps the model focus on common patterns rather than rare exceptions</li> </ol> <p>Real-world analogy: If you're learning to recognize cars, you'd focus on common brands like Toyota, Honda, and Ford before worrying about rare vintage models.</p> <p>Example - Before:</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Yelahanka 8 60 Devanahalli 5 45 Bagalur 3 40 Attibele 2 35 Hoskote 1 30 <p>After Grouping (threshold &lt; 10):</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Other 19 42 <p>Result: Reduced from 11 unique locations to 7, grouping 5 rare locations with insufficient data</p>"},{"location":"day1/day1/#6-outlier-detection-and-removal","title":"6. Outlier Detection and Removal","text":"<p>Outliers are extreme values that don't fit the general pattern. They can severely distort your model's understanding of the data.</p>"},{"location":"day1/day1/#61-what-are-outliers","title":"6.1 What are Outliers?","text":"<p>Example 1: A 6-bedroom house with only 1,020 square feet total. That's roughly 170 sq ft per room - smaller than most bathrooms! This is clearly an error or exceptional case.</p> <p>Example 2: A property listed at \u20b912,000,000 per square foot when most properties in that area are \u20b95,000-10,000 per sq ft.</p> <p>Example DataFrame:</p> Location BHK Bath Total_sqft Price Price_per_sqft Sqft_per_room Whitefield 2 2 1200 65 5,417 600 Marathahalli 3 2 1500 90 6,000 500 Koramangala 6 2 1020 80 7,843 170 \u2190 Outlier! HSR Layout 3 2 1400 16,800 12,000,000 467 \u2190 Extreme! Indiranagar 4 3 2000 115 5,750 500 Electronic City 2 1 1100 62 5,636 550 Whitefield 8 3 1200 95 7,917 150 \u2190 Outlier! <p>Problems Identified: - Row 3: 6 BHK in only 1020 sqft = 170 sqft per room (impossible!) - Row 4: \u20b912 million per sqft (data entry error, probably meant \u20b912,000) - Row 7: 8 BHK in 1200 sqft = 150 sqft per room (unrealistic)</p>"},{"location":"day1/day1/#62-why-remove-outliers","title":"6.2 Why Remove Outliers?","text":"<p>Imagine teaching someone about typical house prices by showing them:</p> <ul> <li>99 normal houses (\u20b930-80 lakhs)</li> <li>1 ultra-luxury mansion (\u20b9500 lakhs)</li> </ul> <p>They might develop a skewed understanding. Similarly, outliers can mislead machine learning models.</p>"},{"location":"day1/day1/#63-domain-based-outlier-removal","title":"6.3 Domain-Based Outlier Removal","text":"<p>Rule of Thumb: In urban Indian housing, a reasonable minimum is about 300 square feet per bedroom.</p> <p>Logic: </p> <ul> <li>1 BHK should have at least 300 sq ft</li> <li>2 BHK should have at least 600 sq ft</li> <li>3 BHK should have at least 900 sq ft</li> </ul> <p>Properties below these thresholds are likely data entry errors or exceptional cases we should exclude.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid Koramangala 6 1020 80 170 \u2717 Remove HSR Layout 4 1800 110 450 \u2713 Valid Indiranagar 4 800 70 200 \u2717 Remove Electronic City 2 950 55 475 \u2713 Valid Yelahanka 5 1200 75 240 \u2717 Remove Hebbal 3 1350 85 450 \u2713 Valid <p>After Domain-Based Removal (minimum 300 sqft/room):</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid HSR Layout 4 1800 110 450 \u2713 Valid Electronic City 2 950 55 475 \u2713 Valid Hebbal 3 1350 85 450 \u2713 Valid <p>Result: Removed 3 properties with unrealistic sqft per room ratios</p>"},{"location":"day1/day1/#64-outlier-removal-using-box-plots-and-iqr","title":"6.4 Outlier Removal using Box Plots and IQR","text":""},{"location":"day1/day1/#box-plot-visualization","title":"Box Plot Visualization","text":"<p>A box plot (or whisker plot) is a graphical representation that helps visualize the spread and skewness of numerical data. It displays:</p> <ul> <li>Median (Q2) \u2014 The midpoint of the dataset, dividing it into two equal halves.</li> <li>First Quartile (Q1) \u2014 The 25th percentile \u2014 25% of the data lies below this value.</li> <li>Third Quartile (Q3) \u2014 The 75th percentile \u2014 75% of the data lies below this value.</li> <li>Interquartile Range (IQR) \u2014 The range between Q3 and Q1, calculated as IQR = Q3 - Q1.</li> <li>Whiskers: Extend from Q1 and Q3 to show variability outside the upper and lower quartiles.</li> <li>Outliers: Points plotted beyond the whiskers that indicate unusually high or low values.</li> </ul> <p>A box plot makes it easy to spot outliers visually, as they appear as isolated points away from the main data cluster.</p> <p>Steps:</p> <ol> <li> <p>Compute Q1 and Q3 \u2014 Find the 25th and 75th percentiles of the data.</p> <p>Suppose you have house prices (in lakhs):</p> <p>[10, 48, 55, 120, 125, 185, 600]</p> <p>Q1 (First Quartile / 25th Percentile)</p> <p>The value below which 25% of the data falls.</p> <p>Here, Q1 \u2248 48 \u2192 one-fourth of the data is below \u20b948 L.</p> <p>Q3 (Third Quartile / 75th Percentile)</p> <p>The value below which 75% of the data falls.</p> <p>Here, Q3 \u2248 185 \u2192 most data (three-fourths) is below \u20b9185 L.</p> </li> <li> <p>Calculate IQR</p> <p><code>\ud835\udc3c\ud835\udc44\ud835\udc45=\ud835\udc443\u2212\ud835\udc441</code></p> </li> <li> <p>Determine cutoff limits</p> <p><code>Lower bound = Q1 - 1.5 \u00d7 IQR</code></p> <p><code>Upper bound = Q3 + 1.5 \u00d7 IQR</code></p> </li> <li> <p>Identify and remove outliers \u2014 Any value less than the lower bound or greater than the upper bound is considered an outlier.</p> </li> </ol> <p></p> <p></p>"},{"location":"day1/day1/#7-data-normalization-and-standardization","title":"7. Data Normalization and Standardization","text":"<p>Before feeding data to machine learning models, we often need to scale our features to ensure they're on similar ranges.</p>"},{"location":"day1/day1/#71-why-scale-features","title":"7.1 Why Scale Features?","text":"<p>The Problem: Features with larger ranges can dominate the learning process.</p> <p>Example - Unscaled Data:</p> Location BHK Bath Total_sqft Price Whitefield 2 2 1200 60 Marathahalli 3 3 2500 120 Koramangala 2 1 800 40 HSR Layout 4 3 3000 150 Indiranagar 3 2 1800 95 Electronic City 2 2 1000 50 <p>Feature Ranges:</p> <ul> <li><code>Total_sqft</code>: 800 to 3000 (range = 2200)</li> <li><code>Bath</code>: 1 to 3 (range = 2)</li> <li><code>BHK</code>: 2 to 4 (range = 2)</li> </ul> <p>Notice how <code>Total_sqft</code> has a much larger range! Without scaling, models might give it disproportionate importance simply because of its larger numeric values.</p>"},{"location":"day1/day1/#72-standardization-z-score-normalization","title":"7.2 Standardization (Z-score Normalization)","text":"<p>Transforms data to have mean = 0 and standard deviation = 1.</p> <p>Formula: z = (x - \u03bc) / \u03c3</p> <p>When to use: When your data follows a normal distribution or when using algorithms like SVM, Linear Regression, or Logistic Regression.</p> <p>Example - After Standardization:</p> Location Total_sqft Standardized_sqft Bath Standardized_bath Whitefield 1200 -0.27 2 0.00 Marathahalli 2500 1.46 3 1.22 Koramangala 800 -1.18 1 -1.22 HSR Layout 3000 2.19 3 1.22 Indiranagar 1800 0.64 2 0.00 Electronic City 1000 -0.64 2 0.00 <p>Result:</p> <ul> <li>Mean \u2248 0 for both features</li> <li>Standard deviation = 1</li> <li>Values can be negative or positive</li> <li>Preserves outliers' relationships</li> </ul>"},{"location":"day1/day1/#73-normalization-min-max-scaling","title":"7.3 Normalization (Min-Max Scaling)","text":"<p>Scales data to a fixed range, typically [0, 1].</p> <p>Formula: x_scaled = (x - x_min) / (x_max - x_min)</p> <p>When to use: When you need a bounded range, especially for neural networks or when data doesn't follow normal distribution.</p> <p>Example - After Normalization:</p> Location Total_sqft Normalized_sqft Bath Normalized_bath Whitefield 1200 0.18 2 0.50 Marathahalli 2500 0.77 3 1.00 Koramangala 800 0.00 1 0.00 HSR Layout 3000 1.00 3 1.00 Indiranagar 1800 0.45 2 0.50 Electronic City 1000 0.09 2 0.50 <p>Result:</p> <ul> <li>All values between 0 and 1</li> <li>Minimum value becomes 0</li> <li>Maximum value becomes 1</li> <li>Preserves the relative distances between values</li> </ul> <p>Key Difference: </p> <ul> <li>Standardization: Values can be negative or &gt; 1; preserves outliers better</li> <li>Normalization: Always between [0,1]; more affected by outliers</li> </ul>"},{"location":"day1/day1/#8-preparing-data-for-machine-learning","title":"8. Preparing Data for Machine Learning","text":""},{"location":"day1/day1/#81-encoding-categorical-variables","title":"8.1 Encoding Categorical Variables","text":"<p>Machine learning algorithms work with numbers, not text. We need to convert categorical variables like \"location\" into numerical format.</p>"},{"location":"day1/day1/#811-one-hot-encoding","title":"8.1.1 One-Hot Encoding","text":"<p>Creates separate binary (0 or 1) columns for each category.</p> <p>Example - Before One-Hot Encoding:</p> Location BHK Bath Total_sqft Price Rajaji Nagar 2 2 1200 50 Hebbal 3 2 1500 75 Koramangala 2 1 1000 60 Rajaji Nagar 3 3 1400 70 Whitefield 4 3 2000 95 Hebbal 2 2 1100 55 <p>After One-Hot Encoding:</p> BHK Bath Total_sqft Price Rajaji_Nagar Hebbal Koramangala Whitefield 2 2 1200 50 1 0 0 0 3 2 1500 75 0 1 0 0 2 1 1000 60 0 0 1 0 3 3 1400 70 1 0 0 0 4 3 2000 95 0 0 0 1 2 2 1100 55 0 1 0 0 <p>Each location now has its own binary column. A \"1\" indicates the property is in that location.</p> <p>Advantages:</p> <ul> <li>Works well with algorithms that assume linear relationships</li> <li>No ordinal relationship assumed between categories</li> <li>Widely supported</li> </ul> <p>Disadvantages:</p> <ul> <li>Creates many columns for high-cardinality features</li> <li>Increases memory usage and computation time</li> <li>Can lead to sparse matrices</li> </ul> <p>Note: We don't create a column for \"other\" because if all location columns are 0, the model knows it's \"other.\"</p>"},{"location":"day1/day1/#812-label-encoding","title":"8.1.2 Label Encoding","text":"<p>Assigns a unique integer to each category.</p> <p>Example:</p> Location BHK Price Label_Encoded Rajaji Nagar 2 50 0 Hebbal 3 75 1 Koramangala 2 60 2 Rajaji Nagar 3 70 0 Whitefield 4 95 3 Hebbal 2 55 1 <p>Each unique location gets a single integer. Memory efficient but implies order.</p> <p>Advantages:</p> <ul> <li>Memory efficient (single column)</li> <li>Simple and fast</li> </ul> <p>Disadvantages:</p> <ul> <li>Implies ordinal relationship (Koramangala &gt; Hebbal &gt; Rajaji Nagar)</li> <li>Can mislead algorithms like Linear Regression</li> <li>Best for: Ordinal data (e.g., Low, Medium, High) or tree-based models</li> </ul>"},{"location":"day1/day1/#813-targetmean-encoding","title":"8.1.3 Target/Mean Encoding","text":"<p>Replaces categories with the mean of the target variable for that category.</p> <p>Example - Original Data:</p> Location BHK Price Rajaji Nagar 2 50 Hebbal 3 75 Koramangala 2 60 Rajaji Nagar 3 70 Hebbal 2 80 Koramangala 4 65 <p>After Target Encoding (using mean price per location):</p> Location BHK Price Location_Encoded Rajaji Nagar 2 50 60.0 Hebbal 3 75 77.5 Koramangala 2 60 62.5 Rajaji Nagar 3 70 60.0 Hebbal 2 80 77.5 Koramangala 4 65 62.5 <p>Each location is replaced by its average price (Rajaji Nagar = 60, Hebbal = 77.5, Koramangala = 62.5)</p> <p>Advantages:</p> <ul> <li>Captures relationship between category and target</li> <li>Single column (memory efficient)</li> </ul> <p>Disadvantages:</p> <ul> <li>Risk of data leakage</li> <li>Can overfit</li> <li>Best for: High-cardinality features in tree-based models</li> </ul>"},{"location":"day1/day1/#814-frequency-encoding","title":"8.1.4 Frequency Encoding","text":"<p>Replaces categories with their frequency count or percentage.</p> <p>Example:</p> Location BHK Price Frequency_Count Frequency_Pct Rajaji Nagar 2 50 250 0.25 Hebbal 3 75 180 0.18 Koramangala 2 60 220 0.22 Rajaji Nagar 3 70 250 0.25 Whitefield 4 95 200 0.20 Hebbal 2 55 180 0.18 <p>Popular locations get higher frequency values (Rajaji Nagar appears 250 times = 25% of dataset)</p> <p>Best for: When frequency itself is predictive (e.g., popular locations might be more expensive)</p> <p>Comparison Summary:</p> Method Columns Created Memory Preserves Info Risk of Overfitting Best Use Case One-Hot Many (n-1) High Yes Low Linear models, nominal data Label 1 Low Partial Low Tree models, ordinal data Target 1 Low Yes High Tree models, high cardinality Frequency 1 Low Partial Medium When frequency matters <p>For our project, we'll use One-Hot Encoding as it works well with linear models and doesn't assume any ordinal relationship between locations.</p>"},{"location":"day1/day1/#82-separating-features-and-target","title":"8.2 Separating Features and Target","text":"<p>Features (X): The input variables we use to make predictions - Total square feet - Number of bathrooms - Number of bedrooms (BHK) - Location (one-hot encoded)</p> <p>Target (y): What we're trying to predict - Price</p> <p>This separation is crucial because we train the model to find patterns between X and y.</p>"},{"location":"day1/day1/#9-data-visualization-seeing-patterns","title":"9. Data Visualization: Seeing Patterns","text":"<p>Visualization helps us understand our data intuitively.</p> <p></p>"},{"location":"day1/day1/#91-histogram-of-price-per-square-foot","title":"9.1 Histogram of Price Per Square Foot","text":"<p>A histogram shows the distribution of values: - X-axis: Price ranges (e.g., \u20b93,000-4,000, \u20b94,000-5,000) - Y-axis: How many properties fall in each range</p> <p>What to look for: - Where most properties are concentrated - Whether the distribution is normal (bell-shaped) - Presence of extreme values</p> <p></p>"},{"location":"day1/day1/#92-scatter-plots-for-outlier-detection","title":"9.2 Scatter Plots for Outlier Detection","text":"<p>Purpose: Compare 2 BHK vs. 3 BHK properties in the same location.</p> <p>Axes: - X-axis: Total square feet - Y-axis: Price</p> <p>What we expect:  - 3 BHK properties (green) should generally be above 2 BHK properties (blue) for the same square footage - Both should show an upward trend (more sq ft = higher price)</p> <p>Red Flags: - Blue dots (2 BHK) above green crosses (3 BHK) at the same square footage - Properties that don't follow the general upward trend</p> <p></p>"},{"location":"day1/day1/#10-summary-of-day-1-concepts","title":"10. Summary of Day 1 Concepts","text":"<p>Today we've learned that successful machine learning requires careful preparation:</p> <ol> <li>Clean your data: Remove inconsistencies, handle missing values, standardize formats</li> <li>Engineer features: Create meaningful variables like price per sq ft</li> <li>Remove outliers: Eliminate extreme values using domain knowledge and statistical methods</li> <li>Prepare for algorithms: Convert categories to numbers, separate features from targets</li> </ol>"},{"location":"day1/day1/#11-now-lets-begin-to-code","title":"11. Now let's begin to Code!","text":"<p>Open the Colab Link, Make a Copy and Upload the dataset on Colab</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: day1.zip - Contains the Bangalore house prices dataset</p> <p>See you next week! \ud83d\ude80</p>"},{"location":"day2/day2/","title":"Day 2: Model Training and Analysis","text":""},{"location":"day2/day2/#complete-documentation-guide","title":"Complete Documentation Guide","text":""},{"location":"day2/day2/#1-overview","title":"1. Overview","text":"<p>This workshop introduces fundamental machine learning concepts with practical implementation. You'll learn how to: - Split data for training and testing - Implement multiple regression and classification models - Evaluate model performance - Compare and select the best model</p> <p>Dataset: Real estate pricing data </p>"},{"location":"day2/day2/#workshop-resources-make-a-copy-and-run-the-cells","title":"Workshop Resources (Make a copy and run the cells) :","text":"<ol> <li> <p>Model Training(Regression) : ML_Workshop_Day2_Regression</p> </li> <li> <p>Preprocessing of Classification dataset : Drugclassification_Preprocessing</p> </li> <li> <p>Model Training(Classification) : ML_Worshop_Day2_Classification</p> </li> <li> <p>Dataset Files : Datasets</p> </li> </ol>"},{"location":"day2/day2/#2-dataset-information","title":"2. Dataset Information","text":""},{"location":"day2/day2/#preprocessed-dataset","title":"Preprocessed Dataset","text":"<ul> <li>Total Records: 10,835 properties</li> <li>Features: 246 (after preprocessing)</li> <li>Target Variable: <code>price</code> (continuous numerical value) - that we want to predict</li> </ul>"},{"location":"day2/day2/#feature-preparation","title":"Feature Preparation","text":"<ul> <li>The first step we do after pre processing our dataset, is split the features and target</li> <li>Target : what we are predicting</li> <li>Features : the properties that we use to predict certain value</li> </ul> <pre><code># Separating features and target\n# Features: drop 'price' (target), keep all others\nfeature_cols = [col for col in df1.columns if col != 'price']\nX = df1[feature_cols]\ny = df1['price']\n\n\n# Dataset shape\nFeatures: (10835, 244)\nTarget: (10835,)\n</code></pre>"},{"location":"day2/day2/#3-overfitting-and-underfitting","title":"3. Overfitting and Underfitting","text":"<p>When building machine learning models, the goal is to capture the true underlying patterns in data so the model can generalize to new, unseen examples.  </p> <p>However, models can sometimes go wrong in two common ways:</p> <ol> <li>Overfitting</li> <li>Underfitting </li> </ol> <p></p> <p>Striking the right balance between underfitting and overfitting is key to building robust machine learning models.</p>"},{"location":"day2/day2/#31-overfitting","title":"3.1. Overfitting","text":"<p>Overfitting happens when a model learns too much from the training data, including details that don\u2019t matter (like noise or outliers).</p> <p>Example : - Imagine fitting a very complicated curve to a set of points. The curve will go through every point, but it won\u2019t represent the actual pattern. - As a result, the model works great on training data but fails when tested on new data.</p> <p></p> <p>Reasons for Overfitting:</p> <ol> <li>High variance and low bias.</li> <li>The model is too complex.</li> <li>The size of the training data.</li> </ol>"},{"location":"day2/day2/#32-underfitting","title":"3.2. Underfitting","text":"<p>Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what\u2019s going on in the data.</p> <p>Example: - Imagine drawing a straight line to fit points that actually follow a curve. The line misses most of the pattern. - In this case, the model doesn\u2019t work well on either the training or testing data.</p> <p></p> <p>Reasons for Underfitting:</p> <ol> <li>The model is too simple, So it may be not capable to represent the complexities in the data.</li> <li>The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.</li> <li>The size of the training dataset used is not enough.</li> <li>Features are not scaled.</li> </ol>"},{"location":"day2/day2/#4-train-test-split","title":"4. Train-Test Split","text":""},{"location":"day2/day2/#what-is-train-test-split","title":"What is Train-Test Split?","text":"<p>Train-test split divides your dataset into two parts:</p> <p></p> <p>Training Set (80%): Used to teach the model - Model learns patterns from this data - Used for fitting/training algorithms</p> <p>Testing Set (20%): Used to evaluate the model - Model has never seen this data - Tests how well model generalizes to new data</p>"},{"location":"day2/day2/#implementation","title":"Implementation","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42)\n\n#test_size -&gt; indicates that 20% is for testing and 80% is for training\n\n#random_state -&gt; reproducibility - ensures same kind of split occurs everytime\n</code></pre>"},{"location":"day2/day2/#41-why-split-data","title":"4.1. Why Split Data?","text":"<ul> <li>Prevents Overfitting: Model doesn't memorize training data</li> <li>Tests Generalization: Evaluates performance on unseen data</li> <li>Realistic Performance: Simulates real-world predictions</li> </ul>"},{"location":"day2/day2/#5-types-of-machine-learning-problems","title":"5. Types of Machine Learning Problems","text":"<p>In supervised learning, problems are usually divided into two types :   </p> <ul> <li>Regression Problem</li> <li>Classification Problem</li> </ul>"},{"location":"day2/day2/#51-regression-problem","title":"5.1. Regression Problem","text":"<ul> <li>Goal : To predict a continuous numeric value.</li> <li> <p>Regression models try to find relationships between input variables (features) and a continuous output.</p> </li> <li> <p>Examples:</p> <ul> <li>Predicting house prices \ud83c\udfe0</li> <li>Estimating temperature \ud83c\udf21\ufe0f</li> <li>Forecasting stock prices \ud83d\udcc8  </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Linear Regression    </li> <li>Decision Tree Regressor</li> <li>Random Forest Regressor</li> <li>K - Nearest Neighbors Regressor</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>Mean Absolute Error (MAE)</li> <li>R\u00b2 Score</li> </ol> </li> </ul>"},{"location":"day2/day2/#52-classification-problem","title":"5.2. Classification Problem","text":"<ul> <li>Goal : To predict a discrete label or category.</li> <li> <p>Classification models learn to separate data into different classes.</p> </li> <li> <p>Examples:</p> <ul> <li>Email spam detection \u2709\ufe0f</li> <li>Disease diagnosis (positive/negative) \ud83e\uddec</li> <li>Image recognition (cat vs. dog) \ud83d\udc31\ud83d\udc36 </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Logistic Regression</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier</li> <li>k-Nearest Neighbors (KNN)</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Accuracy</li> <li>Precision &amp; Recall</li> <li>F1 Score</li> <li>Confusion Matrix</li> </ol> </li> </ul>"},{"location":"day2/day2/#6-regression-models","title":"6. Regression Models","text":"<p>Regression predicts continuous numerical values (e.g., house prices, temperature, sales).</p>"},{"location":"day2/day2/#61-linear-regression","title":"6.1. Linear Regression","text":"<p>How it works: Finds the best straight line through your data points.</p> <p>Mathematical Formula: </p> <pre><code>\u0177 = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\n\nWhere:\n\u0177 = predicted value\n\u03b2\u2080 = intercept (bias)\n\u03b2\u2081, \u03b2\u2082, ..., \u03b2\u2099 = coefficients (weights)\nx\u2081, x\u2082, ..., x\u2099 = feature values\n</code></pre> <p>Simple form: <code>y = mx + b</code> </p> <ul> <li>Simple and interpretable</li> <li>Assumes linear relationship between features and target</li> </ul> <p></p> <p>Strengths:  </p> <ul> <li>Fast to train</li> <li>Easy to interpret</li> <li>Works well with linear relationships</li> </ul> <p>Weaknesses:  </p> <ul> <li>Cannot capture complex non-linear patterns</li> <li>Sensitive to outliers - basically those values that are much out of range when compared to normal values</li> </ul> <p>Use cases : </p> <ul> <li>Predicting house prices based on area, location, etc.  </li> <li>Estimating sales revenue from advertising spend.  </li> <li>Forecasting demand or performance metrics. </li> </ul>"},{"location":"day2/day2/#62-decision-tree-regressor","title":"6.2. Decision Tree Regressor","text":"<p>How it works: Creates a tree of yes/no questions to make predictions.</p> <p>Example:</p> <pre><code>Is size &gt; 2000 sq ft?\n  \u251c\u2500 Yes \u2192 Is location = downtown?\n  \u2502         \u251c\u2500 Yes \u2192 Predict $500k\n  \u2502         \u2514\u2500 No \u2192 Predict $350k\n  \u2514\u2500 No \u2192 Predict $250k\n</code></pre> <p></p> <p>Mathematical Formula :</p> <pre><code>Prediction at leaf node = (1/n) \u03a3\u1d62\u208c\u2081\u207f y\u1d62\n\nWhere:\nn = number of samples in the leaf\ny\u1d62 = actual values in the leaf\n(Takes the mean of training samples that reach that leaf)\n\nSplit criterion (MSE):\nMSE = (1/n) \u03a3\u1d62\u208c\u2081\u207f (y\u1d62 - \u0177)\u00b2\n</code></pre> <p>Strengths:  </p> <ul> <li>Handles non-linear relationships</li> <li>Easy to visualize and understand</li> <li>No feature scaling needed</li> </ul> <p>Weaknesses:  </p> <ul> <li>Can overfit easily</li> <li>Sensitive to small data changes</li> <li>May create overly complex trees</li> </ul> <p>Use cases : </p> <ul> <li>Predicting sales based on season, location, and marketing.  </li> <li>Modeling complex, non-linear data patterns.  </li> </ul>"},{"location":"day2/day2/#63-random-forest-regressor","title":"6.3. Random Forest Regressor","text":"<p>How it works: Creates many decision trees and averages their predictions.</p> <p>Think of it as: A committee of experts voting on the answer - Each tree sees slightly different data - Final prediction = average of all trees - Reduces overfitting compared to single tree</p> <p></p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/T) \u03a3\u209c\u208c\u2081\u1d40 h\u209c(x)\n\nWhere:\nT = number of trees in the forest\nh\u209c(x) = prediction from tree t\n\u0177 = final prediction (average of all trees)\n</code></pre> <p>Strengths:  </p> <ul> <li>More accurate than single decision tree</li> <li>Handles complex relationships</li> <li>Reduces overfitting</li> <li>Shows feature importance</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slower to train</li> <li>Less interpretable</li> <li>Requires more memory</li> </ul> <p>Use Cases : </p> <ul> <li>Predicting house prices, insurance claim amounts.  </li> <li>Forecasting demand or energy consumption. </li> </ul>"},{"location":"day2/day2/#64-k-nearest-neighbors-knn-regressor","title":"6.4. K-Nearest Neighbors (KNN) Regressor","text":"<p>How it works: Predicts based on the K closest training examples.</p> <p>Example (K=5): - Find 5 nearest houses to your property - Average their prices - That's your prediction</p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/K) \u03a3\u1d62\u208c\u2081\u1d37 y\u1d62\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = value of i-th nearest neighbor\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Simple to understand</li> <li>No training phase (lazy learning)</li> <li>Naturally handles non-linear patterns</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slow predictions on large datasets</li> <li>Needs feature scaling</li> <li>Sensitive to irrelevant features</li> </ul> <p>Use Cases : </p> <ul> <li>Estimating house rent based on nearby similar properties.  </li> <li>Predicting temperature using data from nearby weather stations.</li> </ul>"},{"location":"day2/day2/#7-classification-models","title":"7. Classification Models","text":"<p>Classification predicts categories/classes (e.g., spam/not spam, disease/healthy, high/medium/low price).</p>"},{"location":"day2/day2/#71-logistic-regression","title":"7.1. Logistic Regression","text":"<p>How it works: Despite the name, it's for classification! Predicts probability of belonging to a class.</p> <p>Example: Predicting if house is \"expensive\" or \"affordable\"</p> <pre><code>Probability = 1 / (1 + e^(-score))\nIf probability &gt; 0.5 \u2192 Expensive\nIf probability \u2264 0.5 \u2192 Affordable\n</code></pre> <p>Mathematical Formula :</p> <pre><code>P(y=1|x) = 1 / (1 + e^(-z))\n\nWhere:\nz = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\nP(y=1|x) = probability of class 1\ne = Euler's number (\u22482.718)\n\nDecision: If P(y=1|x) &gt; 0.5 \u2192 Class 1\n          If P(y=1|x) \u2264 0.5 \u2192 Class 0\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Fast and efficient</li> <li>Provides probability scores</li> <li>Easy to interpret</li> </ul> <p>Weaknesses:  </p> <ul> <li>Assumes linear decision boundary</li> <li>Not effective for complex relationships</li> </ul> <p>When to use: Binary classification with linearly separable data</p>"},{"location":"day2/day2/#72-decision-tree-classifier","title":"7.2. Decision Tree Classifier","text":"<p>How it works: Same tree structure as regression, but predicts categories.</p> <p>Example:</p> <pre><code>Is size &gt; 2000 sq ft?\n  \u251c\u2500 Yes \u2192 Is location = downtown?\n  \u2502         \u251c\u2500 Yes \u2192 Class: Luxury\n  \u2502         \u2514\u2500 No \u2192 Class: Standard\n  \u2514\u2500 No \u2192 Class: Budget\n</code></pre> <p>Mathematical Formula :</p> <pre><code>Gini Impurity = 1 - \u03a3\u1d62\u208c\u2081\u1d9c p\u1d62\u00b2\n\nWhere:\nc = number of classes\np\u1d62 = proportion of class i in node\n\nEntropy (alternative):\nH = -\u03a3\u1d62\u208c\u2081\u1d9c p\u1d62 log\u2082(p\u1d62)\n\n(Tree splits to minimize impurity)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Handles non-linear boundaries</li> <li>Interpretable</li> <li>Works with categorical data</li> </ul> <p>Weaknesses:  </p> <ul> <li>Overfits easily</li> <li>Unstable with small data changes</li> </ul> <p>When to use: When you need interpretability and have categorical data</p>"},{"location":"day2/day2/#73-random-forest-classifier","title":"7.3. Random Forest Classifier","text":"<p>How it works: Ensemble of decision trees voting on the class.</p> <p>Voting Example (5 trees):  </p> <ul> <li>Tree 1: Luxury</li> <li>Tree 2: Standard</li> <li>Tree 3: Luxury</li> <li>Tree 4: Luxury</li> <li>Tree 5: Standard</li> <li>Final Prediction: Luxury (majority vote: 3/5)</li> </ul> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{h\u2081(x), h\u2082(x), ..., h\u209c(x)}\n\nWhere:\nT = number of trees\nh\u209c(x) = prediction from tree t\nmode = most frequent class (majority vote)\n\nFor probabilities:\nP(class=c|x) = (1/T) \u03a3\u209c\u208c\u2081\u1d40 I(h\u209c(x) = c)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>High accuracy</li> <li>Reduces overfitting</li> <li>Shows feature importance</li> <li>Handles imbalanced data well</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slower than single tree</li> <li>Less interpretable</li> <li>More memory intensive</li> </ul> <p>When to use: When accuracy is priority and you have sufficient data</p>"},{"location":"day2/day2/#74-k-nearest-neighbors-knn-classifier","title":"7.4. K-Nearest Neighbors (KNN) Classifier","text":"<p>How it works: Assigns class based on K nearest neighbors' majority vote.</p> <p>Example (K=5):  </p> <ul> <li>Find 5 nearest houses</li> <li>3 are \"Luxury\", 2 are \"Standard\"</li> <li>Predict: \"Luxury\" (majority)</li> </ul> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{y\u2081, y\u2082, ..., y\u2096}\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = class of i-th nearest neighbor\nmode = most frequent class\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Simple and intuitive</li> <li>No training needed</li> <li>Naturally handles multi-class</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slow for large datasets</li> <li>Sensitive to feature scaling</li> <li>Curse of dimensionality</li> </ul> <p>When to use: Small to medium datasets with good feature engineering</p>"},{"location":"day2/day2/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"day2/day2/#section-81-regression-metrics","title":"Section 8.1 : Regression Metrics","text":""},{"location":"day2/day2/#811-mean-squared-error-mse","title":"8.1.1. Mean Squared Error (MSE)","text":"<p>Formula: Average of squared differences between predictions and actual values <code>MSE = (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Heavily penalizes large errors</li> <li>Units are squared (e.g., dollars\u00b2)</li> </ul> <p>Example:   </p> <ul> <li>Actual: $300k, Predicted: $310k \u2192 Error\u00b2: (10k)\u00b2 = 100M</li> <li>Actual: $300k, Predicted: $320k \u2192 Error\u00b2: (20k)\u00b2 = 400M</li> <li>MSE = (100M + 400M) / 2 = 250M</li> </ul> <p>Common Use Cases :</p> <ul> <li> <p>Training neural networks: Used as a loss function because it's differentiable and penalizes large errors  </p> </li> <li> <p>Quality control: When large deviations are particularly costly or dangerous  </p> </li> <li> <p>Financial forecasting: Where overestimating or underestimating by large amounts has severe consequences  </p> </li> </ul>"},{"location":"day2/day2/#812-root-mean-squared-error-rmse","title":"8.1.2. Root Mean Squared Error (RMSE)","text":"<p>Formula: Square root of MSE <code>RMSE = \u221a[ (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 ]</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Same units as target (dollars, not dollars\u00b2)</li> <li>More interpretable than MSE</li> </ul> <p>Example:  </p> <ul> <li>From above, MSE = 250M  </li> <li>RMSE = \u221a250M \u2248 15.8k</li> </ul> <p>Common Use Cases :</p> <ul> <li> <p>Real estate price prediction: Easy to interpret (\"model is off by $15.8k on average\")  </p> </li> <li> <p>Weather forecasting: Temperature predictions where errors need to be in degrees, not degrees\u00b2  </p> </li> <li> <p>Sales forecasting: When stakeholders need to understand prediction error in actual sales units</p> </li> </ul>"},{"location":"day2/day2/#813-mean-absolute-error-mae","title":"8.1.3. Mean Absolute Error (MAE)","text":"<p>Formula: Average of absolute differences <code>MAE = (1/n) * \u03a3 |y\u1d62 - \u0177\u1d62|</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Less sensitive to outliers than RMSE</li> <li>Direct average error</li> </ul> <p>Example: </p> <ul> <li>Actual: $300k, Predicted: $310k \u2192 |Error| = 10k  </li> <li>Actual: $300k, Predicted: $320k \u2192 |Error| = 20k  </li> <li>MAE = (10k + 20k) / 2 = 15k</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Energy consumption forecasting: Where extreme values (holidays, events) shouldn't dominate the metric  </p> </li> <li> <p>Customer lifetime value prediction: When a few high-value customers shouldn't distort performance  </p> </li> <li> <p>Budget planning: Where you need realistic average deviations for resource allocation</p> </li> </ul>"},{"location":"day2/day2/#814-r2-score-r-squared","title":"8.1.4. R\u00b2 Score (R-Squared)","text":"<p>Formula: 1 - (Sum of Squared Residuals / Total Sum of Squares) <code>R\u00b2 = 1 - [ \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 / \u03a3 (y\u1d62 - \u0233)\u00b2 ]</code></p> <p>Interpretation:  </p> <ul> <li>Range: -\u221e to 1.0</li> <li>1.0 = Perfect predictions</li> <li>0.0 = Model no better than predicting mean</li> <li>&lt; 0 = Model worse than predicting mean</li> </ul> <p>Example: </p> <ul> <li>Total variance (\u03a3(y\u1d62 - \u0233)\u00b2) = 1000M  </li> <li>Residual variance (\u03a3(y\u1d62 - \u0177\u1d62)\u00b2) = 250M  </li> <li>R\u00b2 = 1 - (250 / 1000) = 0.75 \u2192 Model explains 75% of variance</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Scientific research: Reporting how well your model explains the phenomenon  </p> </li> <li> <p>Marketing analytics: Understanding how much of sales variance is explained by campaigns vs. other factors  </p> </li> <li> <p>Academic/reporting contexts: When stakeholders need a single, intuitive performance metric</p> </li> </ul>"},{"location":"day2/day2/#82-classification-metrics","title":"8.2. Classification Metrics","text":"<p>Let us assume we have:</p> Actual Predicted Positive (1) Positive (1) Negative (0) Positive (1) Positive (1) Negative (0) Negative (0) Negative (0) <p>So: TP = 1, TN = 1, FP = 1, FN = 1</p> <p> </p>"},{"location":"day2/day2/#821-accuracy","title":"8.2.1. Accuracy","text":"<p>Formula: (Correct Predictions) / (Total Predictions) <code>Accuracy = (TP + TN) / (TP + TN + FP + FN)</code></p> <p>Example: Accuracy = (1 + 1) / (1 + 1 + 1 + 1) = 0.5 \u2192 50% accuracy</p> <p>Limitation: Misleading with imbalanced classes</p> <p>Common Use Cases : </p> <ul> <li> <p>Quality assessment: Product defect detection when defects and non-defects are roughly equal  </p> </li> <li> <p>Initial model evaluation: Quick baseline metric before diving into detailed analysis  </p> </li> <li> <p>Avoid for: Fraud detection, disease diagnosis, or any imbalanced dataset (accuracy paradox)</p> </li> </ul>"},{"location":"day2/day2/#822-confusion-matrix","title":"8.2.2. Confusion Matrix","text":"<p>Compares predictions vs actual:</p> <pre><code>                Predicted\n              No    Yes\nActual  No   [TN]  [FP]\n        Yes  [FN]  [TP]\n</code></pre> <ul> <li>TP: True Positives (correctly predicted Yes)</li> <li>TN: True Negatives (correctly predicted No)</li> <li>FP: False Positives (predicted Yes, actually No)</li> <li>FN: False Negatives (predicted No, actually Yes)</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Medical diagnosis: Understanding both false alarms (FP) and missed cases (FN)  </p> </li> <li> <p>Spam filtering: Seeing how many legitimate emails are caught (FP) vs. spam that gets through (FN)</p> </li> </ul>"},{"location":"day2/day2/#823-precision","title":"8.2.3. Precision","text":"<p>Formula:  <code>TP / (TP + FP)</code></p> <p>Meaning: \"Of all positive predictions, how many were correct?\"</p> <p>Example:  = 1 / (1 + 1) = 0.5 \u2192 50% of predicted positives are correct</p> <p>Common Use Cases : </p> <ul> <li> <p>Product recommendations: Only recommend products you're confident users will like  </p> </li> <li> <p>Marketing campaign targeting: When contacting customers has a cost, ensure targets are relevant </p> </li> <li> <p>Legal document review: When reviewing flagged documents is expensive, minimize false flags</p> </li> </ul>"},{"location":"day2/day2/#824-recall-sensitivity","title":"8.2.4. Recall (Sensitivity)","text":"<p>Formula:  <code>TP / (TP + FN)</code></p> <p>Meaning: \"Of all actual positives, how many did we catch?\"</p> <p>Example : = 1 / (1 + 1) = 0.5 \u2192 50% of actual positives identified</p> <p>Common Use Cases : </p> <ul> <li> <p>Fraud detection: Better to flag suspicious transactions for review than miss actual fraud  </p> </li> <li> <p>Security systems: Airport security, intrusion detection where missing threats is catastrophic</p> </li> </ul>"},{"location":"day2/day2/#825-f1-score","title":"8.2.5. F1-Score","text":"<p>Formula:  <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code></p> <p>Meaning: Harmonic mean of precision and recall</p> <p>Example : F1 = 2 * (0.5 * 0.5) / (0.5 + 0.5) = 0.5</p> <p>When to use: Balances precision and recall, especially with imbalanced data</p> <p>Common Use Cases : </p> <ul> <li> <p>Medical diagnosis with cost considerations: Balancing false alarms with missed diagnoses  </p> </li> <li> <p>Model comparison: Single metric for comparing models when both precision and recall matter</p> </li> </ul>"},{"location":"day2/day2/#9-model-comparison-regression-results","title":"9. Model Comparison (Regression Results)","text":""},{"location":"day2/day2/#91-regression-performance-ranking","title":"9.1. Regression Performance Ranking","text":"Rank Model R\u00b2 Score RMSE MAE \ud83e\udd47 1 Linear Regression 0.7904 30.79 9.95 \ud83e\udd48 2 Random Forest 0.7375 34.45 1.77 \ud83e\udd49 3 KNN 0.6585 39.29 1.91 4 Decision Tree 0.6268 41.08 3.14"},{"location":"day2/day2/#92-key-observations","title":"9.2. Key Observations","text":"<p>Linear Regression wins because:  </p> <ul> <li>\u2705 Highest R\u00b2 score (79.04%)  </li> <li>\u2705 Lowest RMSE (best average error)  </li> <li>\u2705 Fast training and prediction  </li> <li>\u2705 Easy to interpret  </li> </ul> <p>Interesting finding: Despite lower MAE, Random Forest has better overall performance metrics than simpler models.</p>"},{"location":"day2/day2/#10-model-comparison-classification-results","title":"10. Model Comparison (Classification Results)","text":""},{"location":"day2/day2/#101-classification-performance-ranking","title":"10.1. Classification Performance Ranking","text":"Model Accuracy Precision Recall F1 Score Logistic Regression 0.9231 0.9479 0.9231 0.9271 Decision Tree 1.0000 1.0000 1.0000 1.0000 Random Forest 1.0000 1.0000 1.0000 1.0000 KNN 0.6410 0.6282 0.6410 0.6197"},{"location":"day2/day2/#102-confusion-matrix-for-all-models","title":"10.2. Confusion Matrix for all models","text":""},{"location":"day2/day2/#103-key-observations","title":"10.3. Key Observations","text":"<p>\ud83c\udfc6 BEST CLASSIFICATION MODELS</p> Model Accuracy Precision Recall F1 Score Decision Tree 1.0 1.0 1.0 1.0 Random Forest 1.0 1.0 1.0 1.0 <p>Decision Tree &amp; Random Forest win because: </p> <ul> <li>\u2705 Perfect test accuracy on this dataset</li> <li>\u2705 Can capture complex, non-linear relationships</li> <li>\u2705 Handle both categorical and numerical features naturally</li> <li>\u2705 Robust and flexible for small datasets</li> </ul>"},{"location":"day2/day2/#11-general-ml-best-practices","title":"11. General ML Best Practices","text":"<ul> <li> <p>Always split your data</p> <ul> <li>Train-test split prevents overfitting</li> <li>Use cross-validation for robust evaluation</li> </ul> </li> <li> <p>Try multiple models</p> <ul> <li>Different models work better for different data</li> <li>No \"one size fits all\" solution</li> </ul> </li> <li> <p>Understand your metrics</p> <ul> <li>R\u00b2 for overall model fit</li> <li>RMSE for average prediction error</li> <li>MAE for median error magnitude</li> </ul> </li> <li> <p>Consider the business context</p> <ul> <li>Is $31k error acceptable for your use case?</li> <li>Sometimes a simple, interpretable model is better than a complex one</li> </ul> </li> </ul>"},{"location":"day2/day2/#12-summary","title":"12. Summary","text":"<p>You've learned:</p> <ul> <li>\u2705 Train-test split methodology</li> <li>\u2705 5 regression algorithms</li> <li>\u2705 6 classification algorithms </li> <li>\u2705 Multiple evaluation metrics</li> <li>\u2705 Model comparison techniques</li> </ul> <p>Remember:  The best model isn't always the most complex one.    Choose based on:  </p> <ul> <li>Performance on test data</li> <li>Interpretability needs</li> <li>Computational resources</li> <li>Business requirements</li> </ul> <p>Created for ML Workshop Day 2 | Happy Learning! \ud83c\udf93</p>"},{"location":"day3/day3/","title":"DAY 3: DEEP LEARNING &amp; NLP","text":""},{"location":"day3/day3/#1-overview","title":"1. Overview","text":"<p>In this workshop you'll learn:</p> <ul> <li>Deep Learning Basics: Covers the fundamentals, starting from a single neuron, building up to neural networks, and explaining the \"learning\" process of gradient descent and backpropagation.</li> <li>What is NLP? : Introduces Natural Language Processing and its evolution from old rules-based systems to modern Deep Learning models.</li> <li>Turning Words into Numbers: Explains the critical step of vectorization, contrasting older methods like Bag-of-Words (BoW), TF-IDF, and One-Hot Encoding with modern Word Embeddings (Word2Vec, GloVe, fastText) that capture meaning.</li> <li>Understanding Sequence &amp; Memory: Describes why word order matters and how RNNs (Recurrent Neural Networks) and their powerful upgrade, LSTMs (Long Short-Term Memory), were created to process sequences.</li> <li>The Modern Revolution (Transformers): Details the breakthrough Attention Mechanism and the two dominant models it created: BERT (for understanding context) and GPT (for generating text).</li> <li>Challenges &amp; Applications: Briefly touches on why human language is so hard for AI (like sarcasm and bias) and where NLP is used in the real world (e.g., finance, healthcare).</li> </ul>"},{"location":"day3/day3/#2-workshop-resources","title":"2. Workshop Resources","text":""},{"location":"day3/day3/#make-a-copy-and-run-the-cells","title":"(Make a copy and run the cells) :","text":""},{"location":"day3/day3/#colab-notebook","title":"\ud83d\udcd3 Colab Notebook:","text":"<p>Open in Google Colab </p>"},{"location":"day3/day3/#dataset","title":"\ud83d\udcca Dataset:","text":"<p>harry_potter_corpus.txt</p>"},{"location":"day3/day3/#3-introduction-to-deep-learning-how-computers-learn","title":"3. Introduction to Deep Learning (How Computers \"Learn\")","text":"<p>Welcome! Before we teach a computer to read, we must first understand how a computer \"learns\" at all. The main idea is Deep Learning (DL).</p> <p>Imagine you want to teach a computer to recognize your handwriting. How would it do that? This is the core problem Deep Learning solves.</p> <p>DL is a method inspired by the human brain. It's not that we're building a real brain, but we're borrowing the key idea: a network of simple, interconnected units called neurons.</p>"},{"location":"day3/day3/#31-why-neural-networks-matter-and-their-applications","title":"3.1. Why Neural Networks Matter and Their Applications","text":"<p>Neural networks are central to modern AI because they learn useful internal representations directly from data, allowing them to capture complex, nonlinear structures that classical models miss. This core capability allows them to power a vast array of real-world AI systems across numerous domains.</p> <p>Prominent applications include:</p> <ul> <li>Computer Vision: Convolutional Neural Networks (CNNs) are used for image recognition, medical imaging analysis, and powering autonomous vehicles.</li> <li>Natural Language Processing: Transformers are the basis for machine translation, advanced chatbots, and text summarization.</li> <li>Speech Recognition: Recurrent Neural Networks (RNNs) and other deep nets are used for transcription services and voice assistants.</li> <li>Forecasting and Time Series: They are applied to demand prediction, financial modeling, and weather forecasting.</li> <li>Reinforcement Learning: Neural networks act as function approximators in game-playing agents, such as DeepMind's AlphaGo.</li> <li>Pattern Recognition: They are highly effective at identifying fraud, detecting anomalies, and classifying documents.</li> </ul>"},{"location":"day3/day3/#32-why-deep-learning-over-traditional-machine-learning","title":"3.2. Why Deep Learning over Traditional Machine Learning?","text":"<ol> <li>Automatic Feature Engineering: This is the biggest advantage. Traditional ML (like Support Vector Machines or Random Forests) relies on manual feature engineering. A data scientist must spend significant time selecting and creating features (e.g., \"word count\" or \"average pixel brightness\"). Deep Learning models learn the best features automatically from the raw data.</li> <li>Performance with Scale: Traditional ML models plateau in performance as you give them more data. Deep Learning models continue to improve as the volume of data increases.</li> <li>Handling Unstructured Data: DL excels at complex, unstructured data like text, images, and audio, where traditional ML struggles.</li> </ol> <p>While that framework is very powerful and versatile, it\u2019s comes at the expense of interpretability. There\u2019s often little, if any, intuitive explanation\u2014beyond a raw mathematical one\u2014for how the values of individual model parameters learned by a neural network reflect real-world characteristics of data. For that reason, deep learning models are often referred to as \u201cblack boxes,\u201d especially when compared to traditional types of machine learning models.</p>"},{"location":"day3/day3/#33-the-building-block-the-artificial-neuron","title":"3.3. The Building Block: The Artificial Neuron","text":"<p>Think of a single neuron as a tiny, simple decision-maker. It gets some inputs and decides how strongly to \"fire\" an output.</p> <p>Here's its job, step-by-step:</p> <ol> <li>It Receives Inputs (X): These are just numbers. For an image, this could be the brightness value (0-255) of a few pixels.</li> <li>It Has Weights (W): Each input has a weight. This is the most important concept. A weight is just a number that represents importance. A high weight means \"pay a lot of attention to this input!\" A low weight means \"this input doesn't matter much.\"</li> <li>It Has a Bias (b): A bias is an extra \"nudge.\" It's a number that helps the neuron decide how easy or hard it is to fire. (e.g., \"Don't fire unless you are really sure\").</li> <li>It Calculates an Output (Y): The neuron multiplies each input by its weight, adds them all up, adds the bias, and then passes this total through an Activation Function. This function just squashes the number (e.g., to be between 0 and 1) to make it a clean, final output signal.</li> </ol> <p></p>"},{"location":"day3/day3/#34-building-a-deep-brain-the-neural-network","title":"3.4. Building a \"Deep\" Brain: The Neural Network","text":"<p>A \"deep\" network is just many layers of these neurons stacked together. This is where the magic happens!</p> <p></p> <p>For example: Recognizing a handwritten digit</p> <ol> <li>Input Layer: This layer just \"receives\" the raw data (e.g., all 784 pixels of a handwritten digit). It doesn't make any decisions.</li> <li>Hidden Layers: This is the real \"brain\" of the network. The term \"deep\" comes from having multiple hidden layers. They perform automatic feature learning:<ul> <li>Layer 1 might learn to find simple edges and lines.</li> <li>Layer 2 might combine those edges to find loops and curves.</li> <li>Layer 3 might combine those loops to recognize a full \"8\" or \"9\".</li> </ul> </li> <li>Output Layer: This layer gives the final answer (e.g., 10 neurons, one for each digit 0-9, where the \"9\" neuron fires the strongest).</li> </ol> <p></p>"},{"location":"day3/day3/#35-how-does-it-learn-the-training-process","title":"3.5. How Does it Learn? (The Training Process)","text":"<p>The power of a neural network is its ability to find the optimal weights and biases that map inputs to correct outputs. It achieves this by iteratively \"learning from its mistakes\" through a process driven by Backpropagation and Gradient Descent.</p> <p></p> <p>This learning process is a four-step cycle:</p> <p>i. The Forward Pass (The Guess) First, the network makes a guess. Inputs (like an image of a \"7\") are fed forward through the network's layers. At each layer, the data is multiplied by the current weights, a bias is added, and it passes through a nonlinear activation function. This produces the network's initial, likely random, prediction (e.g., it guesses \"3\").</p> <p>ii. The Loss Calculation (The Mistake) Next, the network measures how wrong its guess was. A Loss Function (or Cost Function) compares the network's prediction to the true label. This calculation results in a single number, the \"loss\" or \"mistake score,\" which quantifies the error. A high score means a bad guess; the goal is to get this score as low as possible.</p> <p>iii. The Backward Pass (Assigning Blame) This is the core of the learning mechanism, enabled by Backpropagation (short for \"backward propagation of error\").</p> <ul> <li>Calculates Contribution: Starting from the final loss score, the algorithm works backward through the network, layer by layer.</li> <li>Uses Calculus: Using the chain rule of calculus, it calculates the \"gradient\"\u2014a derivative that precisely measures how much each individual weight and bias in the entire network contributed to the final error.</li> <li>Finds Direction: This gradient \"blames\" the parameters. It tells the network not only who was responsible for the mistake but also which direction to nudge each parameter to fix it.</li> </ul> <p>iv. The Weight Update (The Correction)</p> <p>Finally, the network applies the correction using Gradient Descent.</p> <p></p> <p>A. Gradient Descent \u2014 What It Actually Does:</p> <ul> <li>Think of all possible weight combinations as a giant, hilly landscape.</li> <li>The height of the landscape at any point is the loss.</li> <li>The goal: reach the lowest valley \u2014 the point where loss is minimum.</li> </ul> <p>B. Local Minima vs. Global Minimum:</p> <ul> <li>A global minimum is the absolute lowest point in the entire landscape.</li> <li>A local minimum is a small valley that is lower than its surroundings but not the lowest overall.</li> <li>Gradient Descent follows the slope downward, so if the landscape is complex, it may get stuck in a local minimum instead of reaching the global minimum.</li> </ul> <p></p> <p>C. Normal GD vs. Stochastic GD:</p> <p>Stochastic Gradient Descent (SGD) is a fast and noisy version of Gradient Descent used to train machine learning models\u2014especially neural networks, using ONE training example at a time, instead of the entire dataset.</p> Feature Normal / Batch Gradient Descent Stochastic Gradient Descent (SGD) Data Used per Update Entire dataset One data point (or small batch) Speed Very slow Much faster Computational Cost Very high Much cheaper Accuracy of Gradient Very accurate Noisy updates Effect on Optimization Can get stuck in local minima Noise helps escape local minima and find better solutions <p>Why SGD is used primarily?</p> <p>Because computing gradients for millions of samples every step is too expensive. SGD drastically reduces the number of computations and speeds up learning, making training modern neural networks feasible.</p> <p>v. The Training Loop:</p> <ul> <li>This entire four-step cycle is repeated many times, showing the network thousands of data examples. Each full pass through the training dataset is called an epoch. With each epoch, the weights and biases are nudged closer to their optimal values, the \"mistake score\" descends into the \"valley,\" and the network's predictions become incrementally more accurate.</li> </ul> <p>However, despite practitioners' effort to train high performing models, neural networks still face challenges similar to other machine learning models\u2014most significantly, overfitting. When a neural network becomes overly complex with too many parameters, the model will overfit to the training data and predict poorly. Overfitting is a common problem in all kinds of neural networks, and paying close attention to bias-variance tradeoff is paramount to creating high-performing neural network models.  </p>"},{"location":"day3/day3/#36-types-of-neural-networks","title":"3.6. Types of neural networks","text":"<p>While multilayer perceptrons are the foundation, neural networks have evolved into specialized architectures suited for different domains:</p> <ul> <li>Convolutional neural networks (CNNs or convnets): Designed for grid-like data such as images. CNNs excel at image recognition, computer vision and facial recognition thanks to convolutional filters that detect spatial hierarchies of features.</li> <li>Recurrent neural networks (RNNs): Incorporate feedback loops that allow information to persist across time steps. RNNs are well-suited for speech recognition, time series forecasting and sequential data.</li> <li>Transformers: A modern architecture that replaced RNNs for many sequence tasks. Transformers leverage attention mechanisms to capture dependencies in natural language processing (NLP) and power state-of-the-art models like GPT.</li> </ul> <p>These variations highlight the versatility of neural networks. Regardless of architecture, all rely on the same principles: artificial neurons, nonlinear activations and optimization algorithms.</p>"},{"location":"day3/day3/#37-applying-the-machine-to-language","title":"3.7. Applying the Machine to Language","text":"<p>Now we apply our \"learning machine\" to the messy, complex problem of human language.</p> <p>Understanding Natural Language Processing(NLP)At its core, all modern NLP follows a three-step process:</p> <ol> <li>Step 1: Text to Numbers (Embedding): We must convert raw text (\"The quick brown fox...\") into a numerical format (vectors) that a machine can understand. This is the most critical step.</li> <li>Step 2: Process the Numbers (The Model): The numerical vectors are fed into a deep learning model (like an RNN or a Transformer). This \"brain\" processes the numbers to \"understand\" the patterns, context, and relationships.</li> <li>Step 3: Numbers to Output (The Task): The model's final numerical output is converted into a human-usable result. This could be:<ul> <li>A single label (e.g., \"Positive Sentiment\").</li> <li>A new sequence of text (e.g., a translation).</li> <li>A specific word (e.g., an \"autocomplete\" suggestion).</li> </ul> </li> </ol> <p>Before deep learning, this process was much more manual.</p>"},{"location":"day3/day3/#4-the-evolution-of-nlp-three-main-approaches","title":"4. The Evolution of NLP: Three Main Approaches","text":"<p>To understand language, NLP models have evolved over time. They started with strict, simple rules and grew into the powerful, flexible \"learning\" systems we have today.</p> <p>You can think of this evolution in three main stages.</p>"},{"location":"day3/day3/#41-before-we-begin-two-core-ideas","title":"4.1. Before We Begin: Two Core Ideas","text":"<p>All NLP, from the simplest to the most complex, relies on two basic ways of analyzing language:</p> <ol> <li>Syntactical Analysis (Grammar): This is the \"rules\" part. It focuses on the structure and grammar of a sentence. It checks if the word order is correct according to the rules of the language.<ul> <li>Example: \"The cat sat on the mat\" is syntactically correct.</li> <li>Example: \"Sat the on mat cat\" is syntactically incorrect.</li> </ul> </li> <li>Semantical Analysis (Meaning): This is the \"meaning\" part. Once it knows the grammar is correct, this step tries to figure out the meaning and intent of the sentence.<ul> <li>Example: \"The cat sat on the mat\" and \"The mat was sat on by the cat\" have different syntax (structure) but the same semantics (meaning).</li> </ul> </li> </ol> <p>Now, let's look at how the models evolved.</p>"},{"location":"day3/day3/#42-approach-a-rules-based-nlp-the-if-then-approach","title":"4.2. Approach A: Rules-Based NLP (The \"If-Then\" Approach)","text":"<p>This was the earliest approach to NLP. It's based on manually programmed, \"if-then\" rules.</p> <ul> <li>How it Worked: A programmer had to sit down and write explicit rules for the computer to follow.<ul> <li><code>IF</code> the user says \"hello,\" <code>THEN</code> respond with \"Hi, how can I help you?\"</li> <li><code>IF</code> the user says \"What are your hours?\" <code>THEN</code> respond with \"We are open 9 AM to 5 PM.\"</li> </ul> </li> <li>The Problem: This approach is extremely limited and not scalable.<ul> <li>It has no \"learning\" or AI capabilities.</li> <li>It breaks easily. If a user asks, \"When are you guys open?\" instead of \"What are your hours?\", the system would fail because it doesn't have a specific rule for that exact phrase.</li> </ul> </li> <li>Example: Early automated phone menus (like Moviefone) that only understood specific commands.</li> </ul>"},{"location":"day3/day3/#43-approach-b-statistical-nlp-the-probability-approach","title":"4.3. Approach B: Statistical NLP (The \"Probability\" Approach)","text":"<p>This was the next big step, which introduced machine learning. Instead of relying on hard-coded rules, this approach \"learns\" from a large amount of text.</p> <ul> <li>How it Worked: The model analyzes data and assigns a statistical likelihood (a probability) to different word combinations.<ul> <li>For example, it learns that after the words \"New York,\" the word \"City\" is highly probable, while the word \"banana\" is very improbable.</li> </ul> </li> <li>The Big Breakthrough: Vector Representation. This approach introduced the essential technique of mapping words to numbers (called \"vectors\"). This allowed, for the first time, computers to perform mathematical and statistical calculations on words.</li> <li>Examples: Older spellcheckers (which suggest the most likely correct word) and T9 texting on old phones (which predicted the most likely word you were typing).</li> </ul> <p>A Quick Note on Training: These models needed \"labeled data\"\u2014data that a human had already manually annotated (e.t., \"This is a noun,\" \"This is a verb\"). This was slow and expensive. A key breakthrough called Self-Supervised Learning (SSL) allowed models to learn from unlabeled raw text, which is much faster and cheaper and a key reason why modern Deep Learning is so powerful. </p>"},{"location":"day3/day3/#44-approach-c-deep-learning-nlp-the-modern-approach","title":"4.4. Approach C: Deep Learning NLP (The \"Modern\" Approach)","text":"<p>This is the dominant, state-of-the-art approach used today. It's an evolution of the statistical method but uses powerful, multi-layered neural networks to learn from massive volumes of unstructured, raw data.</p> <p>These models are incredibly accurate because they can understand complex context and nuance. Several types of deep learning models are important:</p> <ul> <li>Sequence-to-Sequence (Seq2Seq) Models:<ul> <li>What they do: They are designed to transform an input sequence (like a sentence) into a different output sequence.</li> <li>Best for: Machine Translation. (e.g., converting a German sentence into an English one).</li> </ul> </li> <li>Transformer Models:<ul> <li>What they do: This is the biggest breakthrough in modern NLP. Transformers use a mechanism called \"self-attention\" to look at all the words in a sentence at once and calculate how important each word is to all the other words, no matter how far apart.</li> <li>Example: Google's BERT model, which powers its search engine, is a famous transformer.</li> </ul> </li> <li>Autoregressive Models:<ul> <li>What they do: This is a type of transformer model that is expertly trained to do one thing: predict the next word in a sequence. By doing this over and over, it can generate entire paragraphs of human-like text.</li> <li>Examples: GPT (which powers ChatGPT), Llama, and Claude.</li> </ul> </li> <li>Foundation Models:<ul> <li>What they do: These are huge, pre-trained \"base\" models (like IBM's Granite or OpenAI's GPT-4) that have a very broad, general understanding of language. They can then be quickly adapted for many specific tasks, from content generation to data extraction.</li> </ul> </li> </ul>"},{"location":"day3/day3/#5-how-nlp-works-the-4-step-pipeline","title":"5. How NLP Works: The 4-Step Pipeline","text":"<p>A computer can't just \"read\" a sentence. To get from raw human language to a useful insight, it follows a strict, step-by-step \"assembly line.\"</p>"},{"location":"day3/day3/#51-step-1-text-preprocessing-the-cleaning-step","title":"5.1. Step 1: Text Preprocessing (The \"Cleaning\" Step)","text":"<p>First, we clean up the raw text and turn it into a standardized format. This is the \"prep work\" in a kitchen\u2014getting your ingredients (the words) ready before you start cooking (the analysis).</p> <ul> <li>Tokenization: Splitting a long string of text into smaller pieces, or \"tokens.\"<ul> <li>Example: \"The cat sat\" becomes <code>[\"The\", \"cat\", \"sat\"]</code></li> </ul> </li> <li>Lowercasing: Converting all characters to lowercase.<ul> <li>Example: \"Apple\" and \"apple\" both become <code>\"apple\"</code>.</li> </ul> </li> <li>Stop Word Removal: Removing common \"filler\" words (like \"is,\" \"the,\" \"a,\" \"on\") that add little unique meaning.</li> <li>Stemming &amp; Lemmatization: Reducing words to their \"root\" form (e.g., \"running,\" \"ran,\" and \"runs\" all become \"run\").</li> <li>Text Cleaning: Removing punctuation, special characters (@, #), numbers, etc.</li> </ul>"},{"location":"day3/day3/#52-step-2-feature-extraction-the-converting-step","title":"5.2. Step 2: Feature Extraction (The \"Converting\" Step)","text":"<p>This is a critical step. Computers do not understand words; they only understand numbers. Feature extraction converts the clean text tokens into a numerical representation (a \"vector\") that a machine can actually analyze.</p>"},{"location":"day3/day3/#521-the-old-way-statistical-counts","title":"5.2.1. The \"Old Way\" (Statistical Counts)","text":"<p>Before we had powerful neural networks, we relied on statistics and word counts. These models were clever but lacked any real understanding.</p> <p></p>"},{"location":"day3/day3/#i-bag-of-words-bow","title":"i. Bag-of-Words (BoW):","text":"<ul> <li> <p>How it Works:</p> <p>Treats a sentence as a collection (\u201cbag\u201d) of words. Ignores grammar and word order.</p> </li> <li> <p>Example Sentence:</p> <p>\u201cThe cat sat on the mat\u201d</p> </li> <li> <p>Vocabulary (example):</p> <p><code>[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]</code></p> </li> <li> <p>BoW Vector:</p> <p>Count how many times each word appears:</p> Word the cat sat on mat Count 2 1 1 1 1 <p>BoW Vector: \u2192 [2, 1, 1, 1, 1]</p> </li> <li> <p>Limitation:</p> <p>\u201cThe cat chased the dog\u201d and \u201cThe dog chased the cat\u201d would look almost identical.</p> </li> </ul>"},{"location":"day3/day3/#ii-tf-idf-term-frequency-inverse-document-frequency","title":"ii. TF-IDF (Term Frequency-Inverse Document Frequency):","text":"<ul> <li> <p>How it Works:</p> <p>Gives high importance to words that are frequent in this document but rare across all documents.</p> </li> <li> <p>Example Sentence:</p> <p>\u201cThe cat sat on the mat\u201d</p> </li> <li> <p>Vocabulary: same as before.</p> </li> <li> <p>TF-IDF Vector (example values):</p> Word the cat sat on mat TF-IDF 0.0 0.52 0.64 0.48 0.64 <p>\u2192 TF-IDF Vector: [0.0, 0.52, 0.64, 0.48, 0.64]</p> <p>(\u201cthe\u201d gets 0.0 because it appears everywhere in the dataset, so IDF \u2248 0)</p> </li> <li> <p>Limitation:</p> <p>Still no understanding of meaning (e.g., \u201ccat\u201d \u2260 \u201ckitten\u201d to TF-IDF).</p> </li> </ul>"},{"location":"day3/day3/#iii-one-hot-encoding","title":"iii. One-Hot Encoding","text":"<ul> <li> <p>Idea:</p> <p>Every word gets a giant vector full of zeros except one 1 at its index in the global vocabulary.</p> </li> <li> <p>Vocabulary Example (size = 5):</p> <p><code>[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]</code></p> </li> </ul> Word Vector the [1, 0, 0, 0, 0] cat [0, 1, 0, 0, 0] sat [0, 0, 1, 0, 0] on [0, 0, 0, 1, 0] mat [0, 0, 0, 0, 1] - Sentence Representation: <pre><code>Usually stored as 5 separate one-hot vectors (one per word), e.g.:\n\n**\u201cThe cat sat on the mat\u201d \u2192**\n\n[1,0,0,0,0]\n[0,1,0,0,0]\n[0,0,1,0,0]\n[0,0,0,1,0]\n[1,0,0,0,0]\n[0,0,0,0,1]\n</code></pre> <ul> <li>Problems:<ul> <li>If vocab = 50,000 \u2192 each word is a 50,000-length vector</li> <li>No semantic meaning at all</li> <li>\u201ccat\u201d, \u201cdog\u201d, and \u201ccar\u201d all look equally unrelated</li> </ul> </li> </ul>"},{"location":"day3/day3/#522-the-modern-way-contextual-embeddings","title":"5.2.2. The \"Modern Way\" (Contextual Embeddings)","text":"<p>Instead of counting, modern NLP systems learn the meaning of words by training neural networks on billions of sentences.</p>"},{"location":"day3/day3/#i-word2vec-word-vector","title":"i. Word2Vec (Word \u2192 Vector)","text":"<p>How it works :</p> <p>We train a tiny neural network.</p> <p>Its task is fake:</p> <p>\u201cGiven a word, predict the words around it.\u201d </p> <p>Example window size = 2</p> <p>Sentence: \u201cthe cat sat on the mat\u201d</p> <p>For each center word, the model tries to predict nearby words:</p> Center Words predicted (window = 2) cat the, sat sat cat, on on sat, the mat the <p>Each training step:</p> <p>Input = one word \u2192 Output = probabilities of surrounding words</p> <p>The Weight Matrix (What We Steal) :</p> <p>Inside Word2Vec is a big matrix of weights.</p> <p>Suppose vocabulary size = 10,000</p> <p>Embedding dimension = 300</p> <p>Matrix shape: 10,000 \u00d7 300</p> <pre><code>        dim1 dim2 dim3 ... dim300\nword1   0.12 0.88 0.01     0.33\nword2   0.55 0.02 0.19     0.44\nword3   0.90 0.10 0.77     0.12\n ...\n</code></pre> <p>Each row is a word\u2019s embedding \u2014 a 300-dimensional vector.</p>"},{"location":"day3/day3/#what-is-a-300-dimensional-vector","title":"What is a 300-dimensional vector?","text":"<p>Think of it like a profile of a word, described by 300 \u201cfeatures\u201d the model learns by itself.</p> <p>Example (tiny version using only 4 dims):</p> <p>Vector(\u201ccat\u201d) =</p> <pre><code>[0.8,  0.1,  0.3,  0.9]\n</code></pre> <p>Vector(\u201cdog\u201d) =</p> <pre><code>[0.79, 0.12, 0.31, 0.91]\n</code></pre> <p>The numbers are not human-interpretable.</p> <p>But the patterns let the model recognize similarity (cat \u2248 dog).</p> <p>A 300-dimensional vector is just a longer version:</p> <pre><code>[0.23, -0.18, 0.04, 1.22, ..., 0.09]  \u2190 300 numbers\n</code></pre> <p>Higher dimension \u2192 more information about meaning.</p>"},{"location":"day3/day3/#detailed-example","title":"Detailed Example:","text":"<p>Sentence: the cat sat on the mat</p> <p>Window size = 2 (predict 2 words left &amp; right)</p> <p>Below is every training pair Word2Vec creates:</p> <p>1. Center word: \u201cthe\u201d</p> <p>Neighbors: cat</p> <p>Training: the \u2192 cat</p> <p>2. Center word: \u201ccat\u201d</p> <p>Neighbors: the, sat</p> <p>Training: cat \u2192 (the, sat)</p> <p>3. Center word: \u201csat\u201d</p> <p>Neighbors: cat, on</p> <p>Training: sat \u2192 (cat, on)</p> <p>4. Center word: \u201con\u201d</p> <p>Neighbors: sat, the</p> <p>Training: on \u2192 (sat, the)</p> <p>5. Center word: \u201cthe\u201d</p> <p>Neighbors: on, mat</p> <p>Training: the \u2192 (on, mat)</p> <p>6. Center word: \u201cmat\u201d</p> <p>Neighbors: the</p> <p>Training: mat \u2192 the</p> <p>This is done across millions of sentences.</p> <p>The neural network slowly learns which words occur in similar contexts.</p> <p>After training, we throw away the network and keep the weight matrix.</p> <p>That matrix becomes:</p> <ul> <li>a dictionary</li> <li>where every word = a learned vector</li> <li>that captures meaning</li> </ul> <p>This is the famous Word2Vec trick.</p>"},{"location":"day3/day3/#ii-glove-global-co-occurrence-matrix","title":"ii. GloVe (Global Co-occurrence Matrix)","text":"<p>GloVe does NOT predict windows.</p> <p>It builds one giant table counting how often words appear near each other.</p> <p>Using the SAME sentence:</p> <p>We count how many times each pair appears within a window of 2:</p> Word the cat sat on mat the \u2014 1 0 1 1 cat 1 \u2014 1 0 0 sat 0 1 \u2014 1 0 on 1 0 1 \u2014 1 mat 1 0 0 1 \u2014 <p>Some examples from above:</p> <ul> <li>\u201ccat\u201d and \u201cthe\u201d appear together once</li> <li>\u201ccat\u201d and \u201csat\u201d appear together once</li> <li>\u201con\u201d and \u201cmat\u201d appear together once</li> <li>\u201csat\u201d and \u201cmat\u201d never appear together</li> </ul> <p>GloVe then factorizes this co-occurrence matrix into two smaller matrices (like compressing it).</p> <p>The result of the factorization is:</p> <pre><code>the \u2192 300D vector\ncat \u2192 300D vector\nsat \u2192 300D vector\non  \u2192 300D vector\nmat \u2192 300D vector\n</code></pre> <p>Same output format as Word2Vec \u2014 but learned differently.</p>"},{"location":"day3/day3/#iii-fasttext-breaks-words-into-character-pieces","title":"iii. fastText (Breaks words into character pieces)","text":"<p>fastText uses the SAME sentence, but it doesn\u2019t learn vectors for whole words directly.</p> <p>Using:</p> <p>\u201cthe cat sat on the mat\u201d</p> <p>Instead of learning a vector for \u201ccat\u201d, fastText breaks it into character n-grams:</p> <p>For n=3 (trigrams):</p> <pre><code>&lt;ca, cat, at&gt;   \u2192 for \"cat\"\n&lt;sa, sat, at&gt;   \u2192 for \"sat\"\n&lt;ma, mat, at&gt;   \u2192 for \"mat\"\n&lt;th, the, he&gt;   \u2192 for \"the\"\n&lt;on&gt;            \u2192 for \"on\"\n</code></pre> <p>Then fastText learns vectors for all these pieces.</p> <p>Example (simplified):</p> <pre><code>Vector(\"cat\") = Vector(\"&lt;ca\") + Vector(\"cat\") + Vector(\"at\")\nVector(\"mat\") = Vector(\"&lt;ma\") + Vector(\"mat\") + Vector(\"at\")\n</code></pre> <p>Since \u201ccat\u201d and \u201cmat\u201d share the subword \u201cat\u201d, their vectors become similar.</p> <p>That\u2019s why fastText can handle:</p> <ul> <li>misspellings</li> <li>new words</li> <li>rare words</li> <li>morphological changes (\u201csitting\u201d, \u201csits\u201d, \u201csat\u201d)</li> </ul> <p>Even if the full word wasn't seen during training.</p>"},{"location":"day3/day3/#53-step-3-text-analysis-the-understanding-step","title":"5.3. Step 3: Text Analysis (The \"Understanding\" Step)","text":"<p>Now that our text is in a clean, numerical format, the real work can begin. This step involves feeding the numerical data into a model architecture (the \"brain\") to interpret and extract meaningful information.</p>"},{"location":"day3/day3/#531-traditional-analysis-tasks","title":"5.3.1. Traditional Analysis Tasks","text":"<p>This is what we want the model to do:</p> <ul> <li>Part-of-Speech (POS) Tagging: Identifying nouns, verbs, adjectives, etc.</li> <li>Named Entity Recognition (NER): Finding people, places, and organizations.</li> <li>Sentiment Analysis: Determining if the tone is positive or negative.</li> <li>Topic Modeling: Finding the main themes in a document.</li> </ul>"},{"location":"day3/day3/#532-modern-model-architectures-the-brain","title":"5.3.2. Modern Model Architectures (The \"Brain\")","text":"<p>A standard ANN has no memory. If you give it:</p> <pre><code>Input 1: \u201chow\u201d\nInput 2: \u201care\u201d\n</code></pre> <p>It completely forgets \u201chow\u201d when it receives \u201care\u201d.</p> <p>But language is sequential. Meaning depends on order.</p> <p>So we need models that can \u201cremember\u201d previous inputs.</p>"},{"location":"day3/day3/#i-recurrent-neural-networks-rnns","title":"i. Recurrent Neural Networks (RNNs)","text":"<p>An RNN adds a loop so that information can be passed from one step to the next.</p> <pre><code>Input x\u2081 \u2192 (RNN) \u2192 h\u2081 \u2192 output\nInput x\u2082 \u2192 (RNN) \u2192 h\u2082 \u2192 output\nInput x\u2083 \u2192 (RNN) \u2192 h\u2083 \u2192 output\n</code></pre> <p>Where:</p> <ul> <li>x\u209c = word embedding at time t</li> <li>h\u209c = hidden state (RNN\u2019s memory)</li> </ul> <p>The hidden state is updated as:</p> <pre><code>h\u209c = tanh(Wx * x\u209c + Wh * h\u209c\u208b\u2081)\n</code></pre>"},{"location":"day3/day3/#example-encode-a-4-word-sentence-step-by-step","title":"Example: Encode a 4-word sentence step-by-step","text":"<p>Sentence: \u201cI love deep learning\u201d</p> <p>Assume each word is turned into a 4-dimensional vector (tiny example to understand the process).</p> <p>Let the embeddings be:</p> <pre><code>I          \u2192 [1, 0, 0, 0]\nlove       \u2192 [0, 1, 0, 0]\ndeep       \u2192 [0, 0, 1, 0]\nlearning   \u2192 [0, 0, 0, 1]\n</code></pre> <p>Start with h\u2080 = [0,0,0,0] (zero memory).</p> <p>Step 1: word = \u201cI\u201d</p> <pre><code>x\u2081 = [1, 0, 0, 0]\nh\u2081 = tanh(Wx*x\u2081 + Wh*h\u2080)\n</code></pre> <p>Hidden state might become something like:</p> <pre><code>h\u2081 = [0.6, -0.1, 0.2, 0.0]\n</code></pre> <p>Step 2: word = \u201clove\u201d</p> <pre><code>x\u2082 = [0, 1, 0, 0]\nh\u2082 = tanh(Wx*x\u2082 + Wh*h\u2081)\n</code></pre> <p>Now memory includes both I + love:</p> <pre><code>h\u2082 = [0.40, 0.55, -0.10, 0.20]\n</code></pre> <p>Step 3: word = \u201cdeep\u201d</p> <pre><code>h\u2083 = tanh(Wx*x\u2083 + Wh*h\u2082)\n</code></pre> <p>Memory grows again:</p> <pre><code>h\u2083 = [0.10, 0.62, 0.33, 0.45]\n</code></pre> <p>Step 4: word = \u201clearning\u201d</p> <pre><code>h\u2084 = tanh(Wx*x\u2084 + Wh*h\u2083)\n</code></pre> <p>Final encoding of the entire sentence:</p> <pre><code>h\u2084 = [0.21, 0.70, 0.55, 0.63]\n</code></pre> <p>This final <code>h\u2084</code> is the vector representation of the whole sentence.</p> <p>This is how RNNs represent sequences.</p> <p>Why RNNs Struggle: They only have one state (<code>h\u209c</code>). Over many steps, early information fades \u2192 vanishing gradient problem.</p>"},{"location":"day3/day3/#ii-long-short-term-memory-lstms","title":"ii. Long Short-Term Memory (LSTMs)","text":"<p>LSTMs fix this by adding two memory paths:</p> <ol> <li>Hidden state (h\u209c) \u2192 short-term memory</li> <li>Cell state (c\u209c) \u2192 long-term memory (highway through time)</li> </ol> <p>Each step uses three gates:</p> <ul> <li>Forget gate \u2192 remove useless old info</li> <li>Input gate \u2192 add important new info</li> <li>Output gate \u2192 decide what to reveal as the hidden state</li> </ul>"},{"location":"day3/day3/#example-same-sentence-encoded-with-lstm","title":"Example: Same sentence encoded with LSTM","text":"<p>Sentence: \u201cI love deep learning\u201d</p> <p>Each step outputs two vectors:</p> <ul> <li>hidden state h\u209c</li> <li>cell state c\u209c</li> </ul> <p>Start with:</p> <pre><code>h\u2080 = [0,0,0,0]\nc\u2080 = [0,0,0,0]\n</code></pre> <p>Step 1: \u201cI\u201d</p> <p>The gates decide what to store.</p> <p>Example (illustration):</p> <pre><code>c\u2081 = [0.9, -0.1, 0.1, 0.0]\nh\u2081 = [0.7, -0.05, 0.15, 0.0]\n</code></pre> <p>Step 2: \u201clove\u201d</p> <p>Forget gate removes irrelevant parts of <code>c\u2081</code>.</p> <p>Input gate adds new info.</p> <pre><code>c\u2082 = [0.85, 0.40, 0.25, 0.10]\nh\u2082 = [0.62, 0.55, 0.20, 0.15]\n</code></pre> <p>Step 3: \u201cdeep\u201d</p> <pre><code>c\u2083 = [0.80, 0.57, 0.60, 0.33]\nh\u2083 = [0.55, 0.60, 0.45, 0.40]\n</code></pre> <p>Step 4: \u201clearning\u201d</p> <p>Final states:</p> <pre><code>c\u2084 = [0.90, 0.70, 0.85, 0.75]\nh\u2084 = [0.60, 0.75, 0.65, 0.58]\n</code></pre> <p>RNN: Only h\u2084 contains the meaning of the whole sentence.</p> <p>LSTM: Both h\u2084 and c\u2084 represent the final sentence meaning.</p> <ul> <li><code>h\u2084</code>: what the model is \u201cfocusing on\u201d at the last word</li> <li><code>c\u2084</code>: deep long-term memory preserved across the sentence</li> </ul> <p>You can think of LSTM like:</p> <pre><code>h\u209c = short-term note\nc\u209c = long-term diary\n</code></pre> <p>This is why LSTMs understand long sentences much better than basic RNNs.</p>"},{"location":"day3/day3/#iii-the-modern-revolution-the-transformer","title":"iii. The Modern Revolution (The Transformer)","text":"<p>Even LSTMs struggle with very long sentences, and their sequential nature (processing one word at a time) makes them slow to train. The Transformer architecture solved this.</p>"},{"location":"day3/day3/#a-encoder-decoder-models","title":"a) Encoder-Decoder Models","text":"<p>This architecture is key to tasks like machine translation.</p> <ol> <li>Encoder: An \"encoder\" (which could be an RNN) reads the entire input sentence (e.g., \"How are you?\") and compresses its full meaning into a single vector (a \"context vector\").</li> <li>Decoder: A \"decoder\" (another RNN) takes that one vector and \"decodes\" it into the output sentence (e.g., \"\u00bfC\u00f3mo est\u00e1s?\").</li> <li>The Problem: This single context vector is a bottleneck. It's hard to cram the entire meaning of a 50-word sentence into one vector.</li> </ol> <p></p>"},{"location":"day3/day3/#b-the-breakthrough-the-attention-mechanism","title":"b) The Breakthrough: The Attention Mechanism","text":"<p>Attention solved the bottleneck. Instead of forcing the decoder to rely on one vector, it allows the decoder to \"look back\" at all the encoder's outputs from the entire input sentence at every step.</p> <p>It learns to \"pay attention\" to the specific input words that are most relevant for generating the current output word. This was a massive leap in performance.</p> <ul> <li>Advantage: It's highly parallelizable (much faster to train) and can capture extremely long-range dependencies, making it the new state-of-the-art.</li> </ul> <p></p>"},{"location":"day3/day3/#iv-modern-models-bert-gpt","title":"iv. Modern Models: BERT &amp; GPT","text":"<p>These are the two most famous models built on the Transformer architecture.</p>"},{"location":"day3/day3/#a-bert-bidirectional-encoder-representations-from-transformers","title":"a) BERT (Bidirectional Encoder Representations from Transformers)","text":"<ul> <li>What it is: An Encoder-only Transformer.</li> <li>How it Learns: It's trained by taking a sentence, \"masking\" (hiding) 15% of the words, and then trying to predict those hidden words.</li> <li>Key Feature: It's bidirectional. To predict a masked word, it looks at both the words that come before it and the words that come after it.</li> <li>Best For: Understanding tasks. It builds a deep understanding of context, making it perfect for sentiment analysis, question answering, and text classification.</li> </ul>"},{"location":"day3/day3/#b-gpt-generative-pre-trained-transformer","title":"b) GPT (Generative Pre-trained Transformer)","text":"<ul> <li>What it is: A Decoder-only Transformer.</li> <li>How it Learns: It's trained as a \"language model,\" meaning it simply tries to predict the very next word in a sentence, given all the words that came before it.</li> <li>Key Feature: It's auto-regressive (one-way). It only looks backward (at the words that came before).</li> <li>Best For: Generation tasks. Because it's trained to \"predict the next word,\" it is exceptional at writing essays, holding conversations, summarizing text, and generating creative content.</li> </ul>"},{"location":"day3/day3/#54-step-4-model-training-the-learning-step","title":"5.4. Step 4: Model Training (The \"Learning\" Step)","text":"<p>This step is the process that \"teaches\" the model architectures from Step 3.</p> <p>This is where the model \"learns\" by looking for patterns and relationships within the data.</p> <ol> <li>Feed Data: The model (e.g., BERT) is fed the numerical data from Step 2.</li> <li>Make Prediction: It makes a prediction (e.g., \"I think this movie review is positive\").</li> <li>Check Answer: It checks its prediction against the right answer (the \"label\").</li> <li>Measure Error: It measures how \"wrong\" it was (this is called the \"loss\").</li> <li>Adjust: It slightly adjusts its internal parameters (weights) to be \"less wrong\" next time.</li> </ol> <p>This process is repeated millions or even billions of times. Once \"trained,\" this model can be saved and used in Step 3 to make predictions on new, unseen data.</p>"},{"location":"day3/day3/#6-why-is-nlp-so-hard","title":"6. Why is NLP So Hard?","text":"<p>Human language is incredibly complex and messy. Even the best NLP models struggle with the same things humans do. These \"ambiguities\" are the biggest challenge.</p> <ul> <li>Biased Training Data: If the data used to train a model is biased (e.g., pulled from biased parts of the web), the model's answers will also be biased. This is a major risk, especially in sensitive fields like healthcare or HR.</li> <li>Misinterpretation (\"Garbage In, Garbage Out\"): A model can easily get confused by messy, real-world language, including:<ul> <li>Slang, idioms, or fragments</li> <li>Mumbled words or strong dialects</li> <li>Bad grammar or misspellings</li> <li>Homonyms (e.g., \"bear\" the animal vs. \"bear\" the burden)</li> </ul> </li> <li>Tone of Voice &amp; Sarcasm: The way something is said can change its meaning completely. Models struggle to detect sarcasm or exaggeration, as they often only \"read\" the words, not the intent.</li> <li>New and Evolving Language: New words are invented all the time (\"rizz,\" \"skibidi\"), and grammar rules evolve. Models can't keep up unless they are constantly retrained.</li> </ul>"},{"location":"day3/day3/#7-where-is-nlp-used","title":"7. Where is NLP Used?","text":"<p>You can find NLP applications in almost every major industry.</p> <ul> <li>Finance: NLP models instantly read financial reports, news articles, and social media to help make split-second trading decisions.</li> <li>Healthcare: NLP analyzes millions of medical records and research papers at once, helping doctors detect diseases earlier or find new insights.</li> <li>Insurance: Models analyze insurance claims to spot patterns (like potential fraud) and help automate the claims process.</li> <li>Legal: Instead of lawyers manually reading millions of documents for a case, NLP can automate \"legal discovery\" by scanning and finding all relevant information.</li> </ul> <p>A computer can't just \"read\" a sentence. To get from raw human language to a useful insight, it follows a strict \"assembly line\" process.</p>"},{"location":"day3/day3/#8-practical-implementation-next-word-prediction-using-pre-trained-model","title":"8. Practical Implementation: Next-Word Prediction using Pre-Trained Model","text":""},{"location":"day3/day3/#fine-tuning-bert-on-harry-potter-corpus","title":"Fine-Tuning BERT on Harry Potter Corpus","text":"<p>Open the Colab Link, Make a Copy and Upload the dataset on Colab</p> <p>\ud83d\udcd3 Colab Notebook:</p> <p>Open in Google Colab </p> <p>\ud83d\udcca Dataset: </p> <p>harry_potter_corpus.txt</p>"},{"location":"day3/day3/#9-summary","title":"9. Summary","text":"<ul> <li>Covered Deep Learning basics, including artificial neurons, neural networks, and how models learn using forward pass, loss, backpropagation, and gradient descent.</li> <li>Explored why deep learning outperforms traditional ML, handling unstructured data and learning features automatically.</li> <li>Introduced NLP, its evolution from rules-based to statistical to deep learning approaches.</li> <li>Learned text preprocessing, feature extraction, and vectorization methods: BoW, TF-IDF, One-Hot, Word2Vec, GloVe, fastText.</li> <li>Studied sequence models: RNNs, LSTMs, and the Transformer architecture with attention mechanism.</li> <li>Covered modern NLP models: BERT for understanding and GPT for text generation, and their real-world applications.</li> <li>Discussed challenges in NLP, like ambiguity, sarcasm, bias, evolving language, and applications in finance, healthcare, insurance, legal, and more.</li> </ul>"},{"location":"day3/day3/#see-you-next-week","title":"See you next week! \ud83d\ude80","text":""},{"location":"day4/day4/","title":"DL with CNN and Image Detection","text":""},{"location":"day4/day4/#welcome-to-day-4-of-the-bootcamp","title":"Welcome to Day 4 of the Bootcamp!!!","text":"<p>Today, we'll be learning how image recognition happens, and how Deep Learning (DL) works! CNN_Model_Training colab link: CNN_Model_Training</p>"},{"location":"day4/day4/#1-recap","title":"1. Recap","text":""},{"location":"day4/day4/#11-what-are-anns","title":"1.1 What are ANN's?","text":"<p>An Artificial Neural Network (ANN) is a type of model in Deep Learning that tries to work like the human brain \u2014 it learns by finding patterns in data.</p> <ul> <li>Just like our brain has neurons that send signals to each other, an ANN has artificial neurons (nodes) connected in layers that pass information forward and adjust themselves to learn.</li> <li>ANN is made up of layers of neurons:</li> <li>Input Layer: Receives the data.</li> <li>Hidden Layers: Where the actual learning happens \u2014 the model adjusts its weights and biases for each neuron during training.</li> <li>Output Layer: Produces the final prediction.</li> </ul> <p></p> <pre><code># Define the model\nmodel = Sequential([\n    Input(shape=(4,)),            # Input layer (4 features)\n    Dense(4, activation='relu'),  # Hidden layer 1\n    Dense(4, activation='relu'),  # Hidden layer 2\n    Dense(3, activation='sigmoid') # Output layer (multi-class classification)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"day4/day4/#12-key-terms","title":"1.2 Key Terms","text":"<ul> <li>Activation Function: Adds non-linearity (mathematically) for a neuron, so the model can learn complex patterns instead of simple linear ones.</li> <li> <p>ReLU (Rectified Linear Unit): The most commonly used activation function in deep learning. It outputs the input directly if it is positive; otherwise, it outputs zero. Formula: <code>f(x) = max(0, x)</code> Advantages:  </p> <ul> <li>Computationally efficient.  </li> <li>Helps mitigate the vanishing gradient problem (After a point of time, the network stops learning ). Disadvantages:  </li> <li>Can suffer from the \"dying ReLU\" problem, where neurons output zero for all inputs. </li> </ul> </li> <li> <p>Sigmoid: Maps input values to a range between 0 and 1, making it useful for binary classification tasks. Formula: <code>f(x) = 1 / (1 + e^(-x))</code> Advantages:  </p> <ul> <li>Outputs can be interpreted as probabilities. Disadvantages:  </li> <li>Can cause vanishing gradients for very large or small input values. </li> </ul> </li> <li> <p>Tanh (Hyperbolic Tangent): Similar to Sigmoid but maps inputs to a range between -1 and 1. Formula: <code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code> Advantages:  </p> <ul> <li>Helps with optimization. Disadvantages:  </li> <li>Can still suffer from vanishing gradients. </li> </ul> </li> <li> <p>Softmax: Converts raw scores (logits) into probabilities for multi-class classification tasks. Formula: <code>f(x\u1d62) = e^(x\u1d62) / \u03a3(e^(x\u2c7c))</code> (for all <code>j</code>) Advantages:  </p> <ul> <li>Ensures the output probabilities sum to 1. Disadvantages:  </li> <li>Not suitable for hidden layers, only used in the output layer for multi-class classification. </li> </ul> </li> <li> <p>Optimizers: Algorithms that improve model learning by adjusting weights and biases during training to reduce error.</p> </li> <li>Loss Function: Measures how wrong the model\u2019s predictions are \u2014 the model tries to minimize this loss while learning.</li> </ul>"},{"location":"day4/day4/#2-major-applications-of-cnns","title":"2. Major Applications of CNNs","text":"<p>CNNs are widely used in various fields due to their ability to process grid-like data such as images. Here are some key applications:</p> <ul> <li>Image Classification: Assigning a label to an entire image (e.g., identifying cats, dogs, or cars).</li> <li>Image Segmentation: Dividing an image into regions and labeling each pixel (e.g., tumor segmentation in medical imaging).</li> <li>Object Detection: Identifying and localizing multiple objects in an image (e.g., detecting pedestrians in autonomous vehicles).</li> <li>Facial Recognition: Identifying or verifying individuals based on facial features.</li> <li>Medical Imaging: Diagnosing diseases from X-rays, MRIs, or CT scans.</li> <li>Autonomous Vehicles: Recognizing traffic signs, lanes, and obstacles.</li> <li>Satellite Imagery: Land cover classification, disaster monitoring, and urban planning.</li> <li>Video Analysis: Action recognition, surveillance, and video classification.</li> </ul>"},{"location":"day4/day4/#21-differences-in-applications-anns-vs-cnns","title":"2.1 Differences in Applications: ANNs vs. CNNs","text":"<p>Here are the key differences between ANN and CNN applications:</p> <ol> <li> <p>Image Classification ANN: Can be used for simple image classification tasks but requires manual feature extraction. CNN: Automatically extracts features like edges, textures, and shapes, making it ideal for complex image classification tasks.</p> </li> <li> <p>Image Segmentation ANN: Not suitable for pixel-level tasks like segmentation due to its fully connected structure. CNN: Perfect for tasks like semantic and instance segmentation, as it processes spatial relationships in images.</p> </li> <li> <p>Object Detection ANN: Struggles with detecting and localizing multiple objects in an image. CNN: Excels at object detection tasks, using architectures like YOLO and Faster R-CNN.</p> </li> <li> <p>Tabular Data ANN: Works well for structured data like spreadsheets, where relationships between features are not spatial. CNN: Not typically used for tabular data, as it is designed for grid-like data such as images.</p> </li> <li> <p>Video Analysis ANN: Limited in handling sequential frames in videos. CNN: Can process video data by extending its architecture to 3D CNNs for spatiotemporal analysis.</p> </li> </ol>"},{"location":"day4/day4/#3-problems-solved-by-cnns","title":"3. Problems Solved by CNNs","text":"<p>CNNs are the backbone of modern computer vision, solving problems like image classification, segmentation, and object detection. Below are the details of these problems and popular models used for each:</p>"},{"location":"day4/day4/#31-image-classification","title":"3.1 Image Classification","text":"<p>Image classification involves assigning a single label to an entire image. For example, determining whether an image contains a cat, dog, or car.</p>"},{"location":"day4/day4/#popular-models","title":"Popular Models","text":"<ul> <li>AlexNet: Introduced ReLU activation and dropout, revolutionizing deep learning in 2012.</li> <li>VGGNet: Known for its simplicity and use of small (3\u00d73) convolution filters.</li> <li>ResNet: Introduced residual connections, enabling the training of very deep networks.</li> <li>EfficientNet: Balances model size, accuracy, and computational efficiency.</li> </ul>"},{"location":"day4/day4/#_1","title":"DL with CNN and Image Detection","text":""},{"location":"day4/day4/#32-image-segmentation","title":"3.2 Image Segmentation","text":"<p>Image segmentation involves dividing an image into regions, where each pixel is assigned a label. It is more detailed than classification.</p>"},{"location":"day4/day4/#popular-models_1","title":"Popular Models","text":"<ul> <li>U-Net: Designed for biomedical image segmentation, uses an encoder-decoder structure.</li> <li>Mask R-CNN: Extends Faster R-CNN for instance segmentation.</li> </ul>"},{"location":"day4/day4/#_2","title":"DL with CNN and Image Detection","text":""},{"location":"day4/day4/#33-object-detection","title":"3.3 Object Detection","text":"<p>Object detection identifies and localizes multiple objects in an image by drawing bounding boxes around them.</p>"},{"location":"day4/day4/#popular-models_2","title":"Popular Models","text":"<ul> <li>YOLO (You Only Look Once): Real-time object detection with high speed and accuracy.</li> <li>SSD (Single Shot MultiBox Detector): Balances speed and accuracy for real-time applications.</li> <li>Faster R-CNN: Combines region proposal networks with CNNs for high accuracy.</li> </ul>"},{"location":"day4/day4/#_3","title":"DL with CNN and Image Detection","text":""},{"location":"day4/day4/#4-convolutional-neural-networks-cnns","title":"4. Convolutional Neural Networks (CNNs)","text":""},{"location":"day4/day4/#41-conversion-of-images-to-pixel-values-for-both-grayscale-and-rgb","title":"4.1 Conversion of Images to Pixel Values for Both Grayscale and RGB","text":"<p>Images are essentially grids of pixels, where each pixel represents a small part of the image. These pixels are stored as numerical values that describe the intensity of light or color at that point. Computers process these numerical values to analyze and understand images.</p>"},{"location":"day4/day4/#grayscale-images","title":"Grayscale Images","text":"<p>A grayscale image contains only shades of gray, ranging from black to white. Each pixel in a grayscale image is represented by a single value, typically between 0 and 255: - 0 represents black (no light). - 255 represents white (maximum light). - Values in between (e.g., 128) represent shades of gray.</p> <p>Example: A 3\u00d73 grayscale image might look like this:</p> Pixel Value Black 0 Dark Gray 50 Light Gray 200 <p>Matrix Representation:</p> <pre><code>[[  0, 128, 255 ],\n [ 64, 200,  50 ],\n [128, 255,   0 ]]\n</code></pre>"},{"location":"day4/day4/#rgb-images","title":"RGB Images","text":"<p>An RGB image contains three color channels: Red, Green, and Blue. Each pixel is represented by three values, one for each channel, typically between 0 and 255: - (255, 0, 0) represents pure red.</p> <ul> <li> <p>(0, 255, 0) represents pure green.</p> </li> <li> <p>(0, 0, 255) represents pure blue.</p> </li> <li> <p>(255, 255, 255) represents white (all channels at maximum intensity).</p> </li> <li> <p>(0, 0, 0) represents black (all channels at zero intensity).</p> </li> </ul> <p>Example: A 2\u00d72 RGB image might look like this:</p> Pixel RGB Value Red (255, 0, 0) Green (0, 255, 0) Blue (0, 0, 255) Yellow (255, 255, 0) <p>Matrix Representation:</p> <pre><code>Red Channel:   [[255,   0],\n                [  0, 255]]\n\nGreen Channel: [[  0, 255],\n                [  0, 255]]\n\nBlue Channel:  [[  0,   0],\n                [255,   0]]\n</code></pre>"},{"location":"day4/day4/#how-images-are-stored-and-processed","title":"How Images Are Stored and Processed","text":"<ol> <li>Grayscale Images: Stored as a 2D matrix where each value represents the intensity of light at that pixel.</li> <li>RGB Images: Stored as a 3D matrix (Height \u00d7 Width \u00d7 3), where the third dimension represents the three color channels (Red, Green, Blue).</li> </ol>"},{"location":"day4/day4/#example-in-python","title":"Example in Python","text":"<pre><code>import cv2\nimport numpy as np\n\n# Load a grayscale image\ngrayscale_image = cv2.imread('example_grayscale.jpg', cv2.IMREAD_GRAYSCALE)\nprint(\"Grayscale Image Shape:\", grayscale_image.shape)\nprint(\"Grayscale Pixel Values:\\n\", grayscale_image)\n\n# Load an RGB image\nrgb_image = cv2.imread('example_rgb.jpg', cv2.IMREAD_COLOR)\nrgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\nprint(\"RGB Image Shape:\", rgb_image.shape)\nprint(\"RGB Pixel Values:\\n\", rgb_image)\n</code></pre>"},{"location":"day4/day4/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Grayscale images are simpler, with one value per pixel, making them easier to process.</li> <li>RGB images are more complex, with three values per pixel, allowing for full-color representation.</li> <li>Both types of images are represented as numerical matrices, which can be processed by machine learning models.</li> </ol>"},{"location":"day4/day4/#41-what-are-cnns","title":"4.1 What are CNN's?","text":"<p>A Convolutional Neural Network (CNN) is a Deep Learning model designed for grid-like data such as images, where a grayscale image is a single H\u00d7W (Height x Width) pixel matrix and an RGB image is three H\u00d7W (Height x Width) matrices stacked together, and the learnable parameters are the weights and biases.</p> <ul> <li>Instead of looking at the whole image at once, CNNs look at small local regions (patches) to learn patterns like edges, textures, shapes, and then combine them to recognize higher-level features.</li> </ul>"},{"location":"day4/day4/#42-how-cnns-work","title":"4.2 How CNNs Work?","text":"<p>A CNN has three main types of layers:</p>"},{"location":"day4/day4/#421-convolution-layer","title":"4.2.1 Convolution Layer","text":"<p>The convolution layer is the core building block of CNNs. It applies filters (kernels) to the input image to extract features like edges, textures, and patterns. Below, we break down the key concepts of stride, kernel, and padding, using the same matrix for all examples.</p>"},{"location":"day4/day4/#example-matrix","title":"Example Matrix","text":"<p>We will use the following 5\u00d75 input matrix for all examples:</p> <pre><code>Input Matrix:\n[ [ 1   2   3   4   5 ]\n  [ 6   7   8   9  10 ]\n  [ 11  12  13  14  15 ]\n  [ 16  17  18  19  20 ]\n  [ 21  22  23  24  25 ] ]\n</code></pre>"},{"location":"day4/day4/#stride","title":"Stride","text":"<ul> <li>Definition: Stride determines how the filter (kernel) moves across the input matrix.  </li> <li>How it works: A stride of 1 means the filter moves 1 step at a time, while a stride of 2 skips 1 step between positions.  </li> <li>Effect: Larger strides reduce the size of the output matrix, while smaller strides preserve more detail.</li> </ul> <p>Example: Using a 3\u00d73 kernel with a stride of 1:</p> <pre><code>Kernel:\n[ [ 1   0   1 ]\n  [ 0   1   0 ]\n  [ 1   0   1 ] ]\n</code></pre> <p>Step-by-Step Calculation: Place the kernel on the top-left corner of the input matrix. Perform element-wise multiplication and sum the results. Move the kernel by the stride (1 step in this case) and repeat.</p> <p>Output Matrix (Stride = 1):</p> <pre><code>[ [ 50   60   70 ]\n  [ 110  120  130 ]\n  [ 170  180  190 ] ]\n</code></pre> <p>With a stride of 2, the kernel skips every other step:</p> <p>Output Matrix (Stride = 2):</p> <pre><code>[ [ 50   70 ]\n  [ 170  190 ] ]\n</code></pre>"},{"location":"day4/day4/#kernel-filter","title":"Kernel (Filter)","text":"<ul> <li>Definition: A kernel (or filter) is a small matrix (e.g., 3\u00d73) that slides over the input matrix to detect patterns.  </li> <li>How it works: Each kernel performs element-wise multiplication with the input matrix and sums the results to produce a single value in the output matrix.  </li> <li>Purpose: Different kernels detect different features, such as edges, corners, or textures.</li> </ul> <p>Example: Using the same 3\u00d73 kernel as above:</p> <pre><code>Kernel:\n[ [ 1   0   1 ]\n  [ 0   1   0 ]\n  [ 1   0   1 ] ]\n</code></pre> <p>Step-by-Step Calculation: Place the kernel on the top-left corner of the input matrix. Multiply each element of the kernel with the corresponding element of the input matrix. Sum the results to get the output value for that position. Slide the kernel to the next position (based on the stride) and repeat.</p> <p>Output Matrix (Stride = 1):</p> <pre><code>[ [ 50   60   70 ]\n  [ 110  120  130 ]\n  [ 170  180  190 ] ]\n</code></pre>"},{"location":"day4/day4/#padding","title":"Padding","text":"<ul> <li>Definition: Padding adds extra pixels (usually zeros) around the edges of the input matrix.  </li> <li>Why it's needed: Without padding, the size of the output matrix decreases after each convolution. Padding helps preserve the spatial dimensions of the input.  </li> <li>Types of Padding:  </li> <li>Same Padding: Adds enough padding to ensure the output size matches the input size.  </li> <li>Valid Padding: No padding is added, so the output size is smaller than the input.</li> </ul> <p>Example: Using the same 5\u00d75 input matrix and 3\u00d73 kernel:</p> <p>Input Matrix (No Padding):</p> <pre><code>[ [ 1   2   3   4   5 ]\n  [ 6   7   8   9  10 ]\n  [ 11  12  13  14  15 ]\n  [ 16  17  18  19  20 ]\n  [ 21  22  23  24  25 ] ]\n</code></pre> <p>Input Matrix (With Padding):</p> <pre><code>[ [ 0   0   0   0   0   0   0 ]\n  [ 0   1   2   3   4   5   0 ]\n  [ 0   6   7   8   9  10   0 ]\n  [ 0  11  12  13  14  15   0 ]\n  [ 0  16  17  18  19  20   0 ]\n  [ 0  21  22  23  24  25   0 ]\n  [ 0   0   0   0   0   0   0 ] ]\n</code></pre> <p>With padding, the output matrix size remains the same as the input matrix.</p>"},{"location":"day4/day4/#422-pooling-layer","title":"4.2.2 Pooling Layer","text":"<ul> <li>Definition: Pooling reduces the size of the feature map by summarizing regions of the input matrix.  </li> <li>Types:  </li> <li>Max Pooling: Takes the maximum value in each region.  </li> <li>Average Pooling: Takes the average value in each region.</li> </ul> <p>Example: Using the same 5\u00d75 input matrix:</p> <pre><code>Input Matrix:\n[ [ 1   2   3   4   5 ]\n  [ 6   7   8   9  10 ]\n  [ 11  12  13  14  15 ]\n  [ 16  17  18  19  20 ]\n  [ 21  22  23  24  25 ] ]\n</code></pre> <p>Max Pooling (2\u00d72, Stride = 2):</p> <pre><code>[ [ 7   9 ]\n  [ 17  19 ] ]\n</code></pre> <p>Average Pooling (2\u00d72, Stride = 2):</p> <pre><code>[ [ 6.5   8.5 ]\n  [ 16.5  18.5 ] ]\n</code></pre> <p>Pooling reduces the size of the matrix while retaining the most important information.</p>"},{"location":"day4/day4/#423-flattening-layers","title":"4.2.3 Flattening Layers","text":"<ul> <li>Definition: Flattening is the process of converting a multi-dimensional feature map (e.g., 2D or 3D) into a 1D vector.</li> <li>Why it's needed: Fully connected (dense) layers require a 1D input, so the output of convolution and pooling layers must be flattened before being passed to dense layers.</li> <li>How it works: The spatial dimensions (height, width, channels) of the feature map are unrolled into a single vector.</li> </ul> <p>Example: If the output of the pooling layer is a 3D tensor of shape <code>(4, 4, 8)</code> (Height \u00d7 Width \u00d7 Channels), flattening converts it into a 1D vector of size <code>4 \u00d7 4 \u00d7 8 = 128</code>.</p>"},{"location":"day4/day4/#424-fully-connected-layer","title":"4.2.4 Fully Connected Layer","text":"<ul> <li>Predicts the outcome based on the extracted features passed to it.</li> </ul>"},{"location":"day4/day4/#5-intuition-behind-finding-edges-and-textures-using-cnn-using-mnist-dataset-for-1-layer","title":"5. Intuition behind finding edges and textures using CNN using MNIST Dataset for 1 layer","text":""},{"location":"day4/day4/#51-input-image","title":"5.1 Input Image","text":"<p>The MNIST input is a 28\u00d728 grayscale image containing a handwritten digit. This raw image serves as the starting point for feature extraction. Even before convolution, the CNN receives a pixel matrix where:</p> <p>Dark areas represent the background.</p> <p>Bright areas represent the handwritten stroke. </p>"},{"location":"day4/day4/#52-first-convolution-layer-conv1-detecting-simple-edges","title":"5.2 First Convolution Layer (Conv1) \u2013 Detecting Simple Edges","text":"<p>The first convolution layer applies a set of filters to the input image to detect simple features such as edges and gradients. Each filter produces a feature map, which highlights specific patterns in the image. </p> <ul> <li>Faint vertical edge getting represented in the first feature map.</li> </ul> <p></p> <ul> <li>Vertical and Horizontal edges together, with the corners, are getting represented in the second feature map, representing 7 as an image.</li> </ul> <p></p> <ul> <li>The background gets represented in the third feature map.</li> </ul> <p></p> <ul> <li>The top vertical edge, and the rightmost horizontal edge gets represented, completing the shape in this particular feature map.</li> </ul> <p></p> <ul> <li>The rightmost curve, gets represented in this particular feature map.</li> </ul> <p></p> <ul> <li>In this feature map, the topmost horizontal curve, and the leftmost vertical curve faintly gets represented.</li> </ul> <p></p> <p>Each feature map is the result of applying a specific filter (kernel) to the input image, followed by the ReLU activation function, which ensures that only positive values are retained. These feature maps are then passed to the next layer for further processing.</p>"},{"location":"day4/day4/#6-tensors-parallelism-and-training-in-cnns","title":"6. Tensors, Parallelism, and Training in CNNs","text":"<p>In this section, we will explore the fundamental building blocks of deep learning \u2014 tensors, the importance of GPUs/TPUs for parallelism, and how the training process works in CNNs.</p>"},{"location":"day4/day4/#61-what-are-tensors","title":"6.1 What Are Tensors?","text":"<p>Tensors are the core data structure used in deep learning. They are generalizations of matrices to higher dimensions and are used to represent data in a structured way.</p> <ul> <li>Scalars (0D): A single number (e.g., <code>5</code>).</li> <li>Vectors (1D): A list of numbers (e.g., <code>[1, 2, 3]</code>).</li> <li>Matrices (2D): A grid of numbers (e.g., a 2\u00d72 matrix):   <code>plaintext   [ [1, 2],     [3, 4] ]</code></li> <li>Tensors (nD): Generalized n-dimensional arrays. For example:</li> <li>A 3D tensor could represent an RGB image (Height \u00d7 Width \u00d7 Channels):     <code>plaintext     [ [ [R1, G1, B1], [R2, G2, B2] ],       [ [R3, G3, B3], [R4, G4, B4] ] ]</code></li> <li>A 4D tensor could represent a batch of images (Batch \u00d7 Height \u00d7 Width \u00d7 Channels).</li> </ul> <p>Why Tensors? - Tensors allow us to represent complex data like images, videos, and text in a structured way. - They are optimized for mathematical operations like matrix multiplication, which is the backbone of deep learning.</p>"},{"location":"day4/day4/#62-why-gpus-and-tpus-are-essential","title":"6.2 Why GPUs and TPUs Are Essential","text":"<p>Images are essentially large matrices, and processing them requires a lot of computational power. GPUs (Graphics Processing Units) and TPUs (Tensor Processing Units) are specialized hardware designed to handle these computations efficiently.</p>"},{"location":"day4/day4/#621-the-role-of-gpus","title":"6.2.1 The Role of GPUs","text":"<ul> <li>GPUs are designed for parallel processing, making them ideal for matrix operations.</li> <li>In CNNs, operations like convolution and backpropagation involve millions of matrix multiplications, which GPUs can handle simultaneously.</li> <li>Example: A single image of size 224\u00d7224\u00d73 (Height \u00d7 Width \u00d7 Channels) has over 150,000 pixels. Processing a batch of 32 images involves over 4.8 million pixels, which GPUs can process in parallel.</li> </ul>"},{"location":"day4/day4/#622-the-role-of-tpus","title":"6.2.2 The Role of TPUs","text":"<ul> <li>TPUs are specialized hardware designed by Google for deep learning tasks.</li> <li>They are optimized for tensor operations and are faster than GPUs for certain workloads.</li> <li>TPUs are commonly used for large-scale training tasks, such as training models on massive datasets like ImageNet.</li> </ul>"},{"location":"day4/day4/#623-why-parallelism-matters","title":"6.2.3 Why Parallelism Matters","text":"<ul> <li>Matrix Operations: Convolutions, pooling, and fully connected layers involve matrix multiplications, which are computationally expensive.</li> <li>Batch Processing: Training on batches of data requires simultaneous processing of multiple images, which GPUs/TPUs excel at.</li> <li>Speed: Without GPUs/TPUs, training deep learning models would take days or weeks instead of hours.</li> </ul>"},{"location":"day4/day4/#63-the-training-cycle-in-cnns","title":"6.3 The Training Cycle in CNNs","text":"<p>Training a CNN involves multiple steps, which are repeated for several epochs until the model converges. Here's how the training process works:</p>"},{"location":"day4/day4/#631-forward-propagation","title":"6.3.1 Forward Propagation","text":"<ul> <li>The input image is passed through the network layer by layer.</li> <li>Each layer performs operations like convolution, activation, and pooling to extract features.</li> <li>The final layer produces predictions (e.g., class probabilities for classification tasks).</li> </ul>"},{"location":"day4/day4/#632-loss-calculation","title":"6.3.2 Loss Calculation","text":"<ul> <li>The model's predictions are compared to the true labels using a loss function (e.g., cross-entropy loss for classification).</li> <li>The loss quantifies how far the predictions are from the actual labels.</li> </ul>"},{"location":"day4/day4/#633-backpropagation","title":"6.3.3 Backpropagation","text":"<ul> <li>Backpropagation computes the gradients of the loss with respect to the model's parameters (weights and biases).</li> <li>Gradients are calculated using the chain rule of calculus, starting from the output layer and propagating backward through the network.</li> </ul>"},{"location":"day4/day4/#634-weight-updates","title":"6.3.4 Weight Updates","text":"<ul> <li>The gradients are used to update the model's parameters using an optimizer (e.g., SGD, Adam).</li> <li>The learning rate controls the size of the updates.</li> </ul>"},{"location":"day4/day4/#635-epochs-and-iterations","title":"6.3.5 Epochs and Iterations","text":"<ul> <li>Epoch: One complete pass through the entire training dataset.</li> <li>Iteration: One update of the model's parameters, typically after processing a single batch of data.</li> <li>Training involves multiple epochs, with the model improving its predictions over time.</li> </ul>"},{"location":"day4/day4/#64-how-cnn-training-differs-from-rnn-training","title":"6.4 How CNN Training Differs from RNN Training","text":"<p>While the core concepts of training (forward propagation, backpropagation, and weight updates) are the same for CNNs and RNNs, there are key differences in how they process data:</p>"},{"location":"day4/day4/#641-data-structure","title":"6.4.1 Data Structure","text":"<ul> <li>CNNs: Process grid-like data (e.g., images) where spatial relationships are important.</li> <li>RNNs: Process sequential data (e.g., text, time series) where temporal relationships are important.</li> </ul>"},{"location":"day4/day4/#642-backpropagation","title":"6.4.2 Backpropagation","text":"<ul> <li>CNNs: Use standard backpropagation to compute gradients.</li> <li>RNNs: Use backpropagation through time (BPTT) to handle sequential dependencies.</li> </ul>"},{"location":"day4/day4/#643-epochs","title":"6.4.3 Epochs","text":"<ul> <li>In CNNs, each epoch processes the entire dataset of images.</li> <li>In RNNs, each epoch processes sequences, which may involve additional steps to handle variable sequence lengths.</li> </ul>"},{"location":"day4/day4/#644-parallelism","title":"6.4.4 Parallelism","text":"<ul> <li>CNNs benefit more from GPU/TPU parallelism because images can be processed independently in batches.</li> <li>RNNs are less parallelizable due to their sequential nature, where each step depends on the previous one.</li> </ul>"},{"location":"day4/day4/#65-summary","title":"6.5 Summary","text":"<ul> <li>Tensors: The fundamental data structure in deep learning, representing data as n-dimensional arrays.</li> <li>GPUs/TPUs: Essential for efficient training of CNNs due to their ability to handle parallel computations.</li> <li>Training Cycle: Involves forward propagation, loss calculation, backpropagation, and weight updates, repeated over multiple epochs.</li> <li>CNN vs. RNN Training: While the core concepts are similar, CNNs process grid-like data in parallel, whereas RNNs process sequential data step by step.</li> </ul> <p>Understanding these concepts is crucial for building and optimizing deep learning models, especially when working with large datasets and complex architectures.</p>"},{"location":"day4/day4/#7-advantages-and-disadvantages-of-cnn","title":"7. Advantages and Disadvantages of CNN","text":""},{"location":"day4/day4/#71-advantages","title":"7.1 Advantages","text":"<ul> <li>Learns hierarchical spatial features (edges \u2192 textures \u2192 objects).  </li> <li>Parameter sharing &amp; local connectivity \u2192 fewer parameters and efficient learning for images.  </li> <li>State-of-the-art for vision tasks and has efficient variants for edge/mobile.  </li> <li>Robust to translation, scaling, and rotation due to convolution and pooling layers.  </li> <li>Feature extraction is automatic \u2014 no need for manual feature engineering.  </li> <li>Versatility \u2014 can be applied to images, videos, and even sequential data like audio or text (with modifications).  </li> </ul>"},{"location":"day4/day4/#711-cnns-for-videos","title":"7.1.1 CNNs for Videos","text":"<ul> <li>Videos are sequences of frames, where each frame is treated as an image. CNNs extract spatial features from each frame.</li> <li>Temporal relationships across frames can be modeled using architectures like RNNs, LSTMs, or 3D CNNs.</li> <li>Applications: Action recognition, video classification, and object tracking.</li> </ul>"},{"location":"day4/day4/#72-disadvantages","title":"7.2 Disadvantages","text":"<ul> <li>Data hungry \u2014 needs large labeled datasets for best performance.  </li> <li>Compute and memory intensive (training often requires GPUs/TPUs).  </li> <li>Can overfit on small datasets.  </li> <li>Lack of interpretability \u2014 CNNs are often considered \"black boxes\" due to their complexity.  </li> <li>Requires significant hyperparameter tuning (e.g., kernel size, stride, number of filters) for optimal performance.  </li> </ul> <p>CNN_Model_Training colab link: CNN_Model_Training</p>"},{"location":"files/day3/BERT_%28Harry_Potter%29/","title":"BERT (Harry Potter)","text":"<p>\ud83e\udde9 Step 1: Install dependencies</p> In\u00a0[38]: Copied! <pre>!pip install transformers datasets torch accelerate\n</pre> !pip install transformers datasets torch accelerate <pre>Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers&lt;=0.23.0,&gt;=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess&lt;0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec&lt;=2025.3.0,&gt;=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (2025.3.0)\nRequirement already satisfied: typing-extensions&gt;=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (3.13.2)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers) (1.2.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (3.11)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;transformers) (2025.10.5)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch) (3.0.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas-&gt;datasets) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.4.0)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (25.4.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.8.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (6.7.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (0.4.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets) (1.22.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)\nERROR: Operation cancelled by user\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n    status = run_func(*args)\n             ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n    return func(self, options, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n    conflicts = self._determine_conflicts(to_install)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n    return check_install_conflicts(to_install)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n    package_set, _ = create_package_set_from_installed()\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n    dependencies = list(dist.iter_dependencies())\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 222, in iter_dependencies\n    req = Requirement(req_string.strip())\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n    parsed = _parse_requirement(requirement_string)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 80, in _parse_requirement\n    url, specifier, marker = _parse_requirement_details(tokenizer)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 124, in _parse_requirement_details\n    marker = _parse_requirement_marker(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 151, in _parse_requirement_marker\n    marker = _parse_marker(tokenizer)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 266, in _parse_marker\n    expression = [_parse_marker_atom(tokenizer)]\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_parser.py\", line 289, in _parse_marker_atom\n    tokenizer.consume(\"WS\")\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 112, in consume\n    if self.check(name):\n       ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 129, in check\n    match = expression.match(self.source, self.position)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip3\", line 10, in &lt;module&gt;\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n    return command.main(cmd_args)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n    return self._main(args)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n    return run(options, args)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n    logger.critical(\"Operation cancelled by user\")\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n    self._log(CRITICAL, msg, args, **kwargs)\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n    self.handle(record)\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n    self.callHandlers(record)\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n    hdlr.handle(record)\n  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n    self.emit(record)\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n    with self:\n         ^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 865, in __exit__\n    self._exit_buffer()\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 823, in _exit_buffer\n    self._check_buffer()\n  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 2060, in _check_buffer\n    self.file.write(text)\nKeyboardInterrupt\n^C\n</pre> <p>\ud83e\udde9 Step 2: Import Libraries</p> In\u00a0[39]: Copied! <pre>from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nimport torch\n</pre> from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling from datasets import load_dataset import torch <p>\ud83e\udde9 Step 3: Prepare the Dataset</p> <p>If you have your corpus (say harry_potter_corpus.txt), upload it to Colab first.</p> In\u00a0[\u00a0]: Copied! <pre>from google.colab import files\nuploaded = files.upload()  # upload harry_potter_corpus.txt\n</pre> from google.colab import files uploaded = files.upload()  # upload harry_potter_corpus.txt        Upload widget is only available when the cell has been executed in the       current browser session. Please rerun this cell to enable.        <p>Then create a HuggingFace dataset from it:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = load_dataset(\"text\", data_files={\"train\": \"harry_potter_corpus.txt\"})\nprint(dataset)\n</pre> dataset = load_dataset(\"text\", data_files={\"train\": \"harry_potter_corpus.txt\"}) print(dataset) <p>\ud83e\udde9 Step 4: Tokenize the text</p> <p>We\u2019ll use BERT\u2019s tokenizer to split text into tokens.</p> In\u00a0[\u00a0]: Copied! <pre>tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n</pre> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  def tokenize_function(examples):     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)  tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"]) <p>\ud83e\udde9 Step 5: Create Data Collator</p> <p>This helps dynamically mask random words during training for the MLM (Masked Language Modeling) task.</p> In\u00a0[\u00a0]: Copied! <pre>data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)\n</pre> data_collator = DataCollatorForLanguageModeling(     tokenizer=tokenizer,     mlm=True,     mlm_probability=0.15 ) <p>\ud83e\udde9 Step 6: Load Pretrained BERT</p> In\u00a0[\u00a0]: Copied! <pre>model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n</pre> model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") <p>\ud83e\udde9 Step 7: Define Training Arguments</p> <p>We\u2019ll fine-tune for a few epochs (keep it light for Colab).</p> In\u00a0[\u00a0]: Copied! <pre>training_args = TrainingArguments(\n    output_dir=\"./bert-harrypotter\",\n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    save_steps=500,\n    save_total_limit=2,\n    prediction_loss_only=True,\n    logging_steps=100\n)\n</pre> training_args = TrainingArguments(     output_dir=\"./bert-harrypotter\",     overwrite_output_dir=True,     num_train_epochs=2,     per_device_train_batch_size=8,     save_steps=500,     save_total_limit=2,     prediction_loss_only=True,     logging_steps=100 ) <p>\ud83e\udde9 Step 8: Create Trainer</p> In\u00a0[\u00a0]: Copied! <pre>trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"]\n)\n</pre> trainer = Trainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=tokenized_datasets[\"train\"] ) <p>\ud83e\udde9 Step 9: Fine-tune the model</p> In\u00a0[\u00a0]: Copied! <pre>trainer.train()\n</pre> trainer.train() <p>This step will:</p> <p>Randomly mask 15% of tokens (like replacing \u201cmagic\u201d \u2192 \u201c[MASK]\u201d)</p> <p>Train BERT to predict them</p> <p>Adapt the model to Harry Potter\u2019s vocabulary</p> <p>\ud83e\udde9 Step 10: Save your fine-tuned model</p> In\u00a0[\u00a0]: Copied! <pre>trainer.save_model(\"./bert-harrypotter-finetuned\")\ntokenizer.save_pretrained(\"./bert-harrypotter-finetuned\")\n</pre> trainer.save_model(\"./bert-harrypotter-finetuned\") tokenizer.save_pretrained(\"./bert-harrypotter-finetuned\") <p>\ud83e\uddea 3. Use the Fine-tuned Model</p> <p>Now you can load it again and use it for masked word predictions.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\n\nfill_mask = pipeline(\"fill-mask\", model=\"./bert-harrypotter-finetuned\", tokenizer=\"./bert-harrypotter-finetuned\")\n\nprompt = \"Harry looked at Ron and said it was a [MASK] day at Hogwarts.\"\nfor pred in fill_mask(prompt):\n    print(f\"{pred['token_str']}: {pred['score']:.4f}\")\n</pre> from transformers import pipeline  fill_mask = pipeline(\"fill-mask\", model=\"./bert-harrypotter-finetuned\", tokenizer=\"./bert-harrypotter-finetuned\")  prompt = \"Harry looked at Ron and said it was a [MASK] day at Hogwarts.\" for pred in fill_mask(prompt):     print(f\"{pred['token_str']}: {pred['score']:.4f}\") <p>Example Output:</p> <p>magical: 0.4231</p> <p>beautiful: 0.2122</p> <p>strange: 0.1048</p> <p>cold: 0.0873</p> <p>wonderful: 0.0657</p> <p>Now your model has learned the Harry Potter tone! \ud83e\ude84</p> <p>\ud83e\udde9 4.Generate Harry Potter-style text</p> <p>You can combine your BERT fine-tuned model with a small GPT-2 generator to make creative completions.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\nprompt = \"At Hogwarts, Hermione discovered a hidden chamber where\"\nprint(generator(prompt, max_length=40, temperature=0.8)[0]['generated_text'])\n</pre> from transformers import pipeline  generator = pipeline(\"text-generation\", model=\"gpt2\") prompt = \"At Hogwarts, Hermione discovered a hidden chamber where\" print(generator(prompt, max_length=40, temperature=0.8)[0]['generated_text']) <p>\ud83e\udde9 5. Explanation to Teach</p> Concept What Students Learn Pretrained Model BERT already knows English Fine-Tuning We adapt it to new domain (Harry Potter) Masked Language Modeling Predict missing words Tokenization Converts words \u2192 numbers Data Collator Randomly masks words for training Trainer API Handles training loops, checkpoints Output Model now \u201ctalks\u201d like the Harry Potter universe <p>\ud83e\ude84 Recap Workflow</p> <p>Upload Corpus \u2192 Harry Potter books or fan dataset</p> <p>Tokenize Text \u2192 Convert to BERT-friendly format</p> <p>Fine-Tune \u2192 Train BERT for a few epochs</p> <p>Save Model \u2192 bert-harrypotter-finetuned</p> <p>Use It! \u2192 Masked word prediction or sentiment analysis</p>"},{"location":"files/day4/code_which/","title":"Code which","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\n\n# \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# \u2705 Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\n# \u2705 Datasets &amp; DataLoader\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# \u2705 Model setup\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n\n# \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# \u2705 Training loop with progress display\nnum_epochs = \ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models  # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # \u2705 Data transforms transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  # \u2705 Datasets &amp; DataLoader train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)  # \u2705 Model setup model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device)  # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # \u2705 Training loop with progress display num_epochs =  total_steps = len(train_loader) total_images = len(train_dataset)  for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\n# Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt import torch  # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"},{"location":"files/day4/simple_cnn/","title":"Simple cnn","text":"<p>First, we start off with importing all the models required, for this particular workshop, we will be using pytorch</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models import matplotlib.pyplot as plt <p>The above cell is used to detect if there's CUDA functionality, basically if you have an Nvidia GPU, you can use that for model training, instead of using CPU power completely.</p> In\u00a0[7]: Copied! <pre># \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>Now, the model we will be focusing on is called Resnet-50. For resnet-50, the input required is 224 x 224 x 3, but then the image size of the dataset is 28 x 28 x 1. So, for resnet-50, we will need to transform to 224 x 224, and now, its only 1 channel present in the image, but then the input taken by the model is 3 channels, so we use the greyscale function to convert to 3 channels, and then we convert images to tensor values, and we normalize it.</p> In\u00a0[10]: Copied! <pre>transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n</pre> transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  <p>This is to load the MNIST dataset (which is a digit classification dataset, has multiple hand written images from 0-9).</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n</pre> train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) <p>This is to load the resnet-18 model, and the resnet-18 model initially outputs 1000 classes, but then we only need 10 (0-9), so we mention 10 as well to output only 10 classes.</p> In\u00a0[\u00a0]: Copied! <pre>model = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n</pre> model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device) <p>This initializes the loss and optimizer required.</p> In\u00a0[\u00a0]: Copied! <pre># \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</pre> # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) <p>Initializes the number of epochs ( the number of times the model trains ), and total images</p> In\u00a0[\u00a0]: Copied! <pre>num_epochs = 2\ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n</pre> num_epochs = 2 total_steps = len(train_loader) total_images = len(train_dataset)  <p>Training with log information as well</p> In\u00a0[\u00a0]: Copied! <pre>for epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") <p>To visualize 15 predictions, using matplotlib</p> In\u00a0[\u00a0]: Copied! <pre># Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"}]}